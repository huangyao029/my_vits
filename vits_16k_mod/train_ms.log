nohup: ignoring input
rank =  0
rank =  1
/mnt/disk3/huangyao/tts/vits/mel_processing.py:57: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/mnt/disk3/huangyao/tts/vits/mel_processing.py:75: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.
Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/mnt/disk3/huangyao/tts/vits/mel_processing.py:57: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/mnt/disk3/huangyao/tts/vits/mel_processing.py:75: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.
Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 9, 96], strides() = [16992, 96, 1]
bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 9, 96], strides() = [16992, 96, 1]
bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Train Epoch: 1 [0.00%] G-Loss: 196.6089 D-Loss: 6.0588 Loss-g-fm: 0.1172 Loss-g-mel: 69.4213 Loss-g-dur: 2.0830 Loss-g-kl: 118.9291 lr: 0.0002 grad_norm_g: nan grad_norm_d: inf
Saving model and optimizer state at iteration 1 to /ZFS4T/tts/data/VITS/model_saved/G_0.pth
Saving model and optimizer state at iteration 1 to /ZFS4T/tts/data/VITS/model_saved/D_0.pth
Train Epoch: 1 [96.15%] G-Loss: 59.1630 D-Loss: 3.0089 Loss-g-fm: 0.2402 Loss-g-mel: 51.5850 Loss-g-dur: 2.2081 Loss-g-kl: 3.5538 lr: 0.0002 grad_norm_g: 45.7623 grad_norm_d: 0.7845
======> Epoch: 1
Train Epoch: 2 [92.31%] G-Loss: 54.7258 D-Loss: 2.9801 Loss-g-fm: 0.6426 Loss-g-mel: 46.5483 Loss-g-dur: 2.1853 Loss-g-kl: 3.4610 lr: 0.0002 grad_norm_g: 245.0291 grad_norm_d: 13.9310
======> Epoch: 2
Train Epoch: 3 [88.46%] G-Loss: 49.9985 D-Loss: 2.4959 Loss-g-fm: 2.6388 Loss-g-mel: 39.6214 Loss-g-dur: 2.4446 Loss-g-kl: 2.3924 lr: 0.0002 grad_norm_g: 62.4905 grad_norm_d: 27.7731
======> Epoch: 3
Train Epoch: 4 [84.62%] G-Loss: 47.4603 D-Loss: 2.3552 Loss-g-fm: 2.5085 Loss-g-mel: 39.1275 Loss-g-dur: 2.2364 Loss-g-kl: 1.6062 lr: 0.0002 grad_norm_g: 65.7994 grad_norm_d: 7.2954
======> Epoch: 4
Train Epoch: 5 [80.77%] G-Loss: 42.4777 D-Loss: 2.6988 Loss-g-fm: 2.1304 Loss-g-mel: 34.4293 Loss-g-dur: 2.2364 Loss-g-kl: 1.5196 lr: 0.0002 grad_norm_g: 59.9837 grad_norm_d: 32.7370
======> Epoch: 5
Train Epoch: 6 [76.92%] G-Loss: 42.5231 D-Loss: 2.8616 Loss-g-fm: 1.0927 Loss-g-mel: 36.0306 Loss-g-dur: 2.3036 Loss-g-kl: 1.4416 lr: 0.0002 grad_norm_g: 54.8048 grad_norm_d: 11.9121
======> Epoch: 6
Train Epoch: 7 [73.08%] G-Loss: 43.5100 D-Loss: 2.7515 Loss-g-fm: 1.5901 Loss-g-mel: 36.6770 Loss-g-dur: 2.2861 Loss-g-kl: 1.3124 lr: 0.0002 grad_norm_g: 64.7092 grad_norm_d: 14.0502
======> Epoch: 7
Train Epoch: 8 [69.23%] G-Loss: 41.6592 D-Loss: 2.8860 Loss-g-fm: 0.9900 Loss-g-mel: 34.9807 Loss-g-dur: 2.2775 Loss-g-kl: 1.5504 lr: 0.0002 grad_norm_g: 61.4869 grad_norm_d: 7.8378
======> Epoch: 8
Train Epoch: 9 [65.38%] G-Loss: 43.8634 D-Loss: 2.7236 Loss-g-fm: 1.5242 Loss-g-mel: 36.6951 Loss-g-dur: 2.3576 Loss-g-kl: 1.1928 lr: 0.0002 grad_norm_g: 39.8303 grad_norm_d: 8.1630
======> Epoch: 9
Train Epoch: 10 [61.54%] G-Loss: 41.7780 D-Loss: 3.1049 Loss-g-fm: 1.2243 Loss-g-mel: 35.5164 Loss-g-dur: 2.2931 Loss-g-kl: 1.0361 lr: 0.0002 grad_norm_g: 47.2105 grad_norm_d: 18.6181
======> Epoch: 10
Train Epoch: 11 [57.69%] G-Loss: 41.2360 D-Loss: 2.6722 Loss-g-fm: 1.4145 Loss-g-mel: 34.6073 Loss-g-dur: 2.3336 Loss-g-kl: 1.1268 lr: 0.0002 grad_norm_g: 30.9138 grad_norm_d: 7.0919
======> Epoch: 11
Train Epoch: 12 [53.85%] G-Loss: 40.9945 D-Loss: 2.7729 Loss-g-fm: 1.2326 Loss-g-mel: 33.7853 Loss-g-dur: 2.3152 Loss-g-kl: 1.4271 lr: 0.0002 grad_norm_g: 45.7089 grad_norm_d: 14.2829
======> Epoch: 12
Train Epoch: 13 [50.00%] G-Loss: 40.7286 D-Loss: 2.8873 Loss-g-fm: 2.0202 Loss-g-mel: 31.8241 Loss-g-dur: 2.7354 Loss-g-kl: 1.9443 lr: 0.0002 grad_norm_g: 50.6536 grad_norm_d: 12.0141
======> Epoch: 13
Train Epoch: 14 [46.15%] G-Loss: 37.2504 D-Loss: 2.9115 Loss-g-fm: 1.3818 Loss-g-mel: 30.5641 Loss-g-dur: 2.3386 Loss-g-kl: 1.3086 lr: 0.0002 grad_norm_g: 35.0963 grad_norm_d: 5.8104
======> Epoch: 14
Train Epoch: 15 [42.31%] G-Loss: 37.8500 D-Loss: 2.7648 Loss-g-fm: 1.5393 Loss-g-mel: 30.2713 Loss-g-dur: 2.5189 Loss-g-kl: 1.4563 lr: 0.0002 grad_norm_g: 61.5018 grad_norm_d: 7.0761
======> Epoch: 15
Train Epoch: 16 [38.46%] G-Loss: 36.7541 D-Loss: 2.7228 Loss-g-fm: 1.3253 Loss-g-mel: 30.0922 Loss-g-dur: 2.3024 Loss-g-kl: 1.2288 lr: 0.0002 grad_norm_g: 64.6099 grad_norm_d: 6.6360
======> Epoch: 16
Train Epoch: 17 [34.62%] G-Loss: 36.6111 D-Loss: 2.6331 Loss-g-fm: 1.3978 Loss-g-mel: 29.9819 Loss-g-dur: 2.3816 Loss-g-kl: 1.4155 lr: 0.0002 grad_norm_g: 69.4735 grad_norm_d: 7.2972
======> Epoch: 17
Train Epoch: 18 [30.77%] G-Loss: 37.1690 D-Loss: 2.6990 Loss-g-fm: 1.7785 Loss-g-mel: 29.5118 Loss-g-dur: 2.3418 Loss-g-kl: 1.5872 lr: 0.0002 grad_norm_g: 51.3817 grad_norm_d: 7.8838
======> Epoch: 18
Train Epoch: 19 [26.92%] G-Loss: 36.6110 D-Loss: 2.5938 Loss-g-fm: 1.8486 Loss-g-mel: 28.9870 Loss-g-dur: 2.3093 Loss-g-kl: 1.3566 lr: 0.0002 grad_norm_g: 39.0265 grad_norm_d: 6.0489
======> Epoch: 19
Train Epoch: 20 [23.08%] G-Loss: 34.5896 D-Loss: 2.6553 Loss-g-fm: 1.5017 Loss-g-mel: 27.5759 Loss-g-dur: 2.2751 Loss-g-kl: 1.3476 lr: 0.0002 grad_norm_g: 27.0244 grad_norm_d: 8.6160
======> Epoch: 20
Train Epoch: 21 [19.23%] G-Loss: 33.4346 D-Loss: 2.7473 Loss-g-fm: 1.5033 Loss-g-mel: 26.4619 Loss-g-dur: 2.2857 Loss-g-kl: 1.2745 lr: 0.0002 grad_norm_g: 43.2398 grad_norm_d: 4.5721
======> Epoch: 21
Train Epoch: 22 [15.38%] G-Loss: 34.9902 D-Loss: 2.7798 Loss-g-fm: 1.6011 Loss-g-mel: 27.8257 Loss-g-dur: 2.5433 Loss-g-kl: 1.2084 lr: 0.0002 grad_norm_g: 38.2954 grad_norm_d: 11.6583
======> Epoch: 22
Train Epoch: 23 [11.54%] G-Loss: 35.3299 D-Loss: 2.5893 Loss-g-fm: 1.5446 Loss-g-mel: 28.3751 Loss-g-dur: 2.1948 Loss-g-kl: 1.1318 lr: 0.0002 grad_norm_g: 37.3851 grad_norm_d: 2.9030
======> Epoch: 23
Train Epoch: 24 [7.69%] G-Loss: 36.8431 D-Loss: 2.8858 Loss-g-fm: 1.8622 Loss-g-mel: 29.5054 Loss-g-dur: 2.3140 Loss-g-kl: 1.3459 lr: 0.0002 grad_norm_g: 47.4925 grad_norm_d: 16.3191
======> Epoch: 24
Train Epoch: 25 [3.85%] G-Loss: 34.8616 D-Loss: 2.7332 Loss-g-fm: 1.4562 Loss-g-mel: 27.9053 Loss-g-dur: 2.2804 Loss-g-kl: 1.3363 lr: 0.0002 grad_norm_g: 58.4294 grad_norm_d: 4.4161
======> Epoch: 25
Train Epoch: 26 [0.00%] G-Loss: 34.3682 D-Loss: 2.8367 Loss-g-fm: 1.3618 Loss-g-mel: 27.7241 Loss-g-dur: 2.2718 Loss-g-kl: 1.4364 lr: 0.0002 grad_norm_g: 89.5210 grad_norm_d: 11.9693
Train Epoch: 26 [96.15%] G-Loss: 35.3038 D-Loss: 2.6561 Loss-g-fm: 1.7239 Loss-g-mel: 28.1578 Loss-g-dur: 2.3058 Loss-g-kl: 1.3888 lr: 0.0002 grad_norm_g: 75.1720 grad_norm_d: 11.0104
======> Epoch: 26
Train Epoch: 27 [92.31%] G-Loss: 36.1712 D-Loss: 2.6220 Loss-g-fm: 2.0327 Loss-g-mel: 28.1358 Loss-g-dur: 2.6459 Loss-g-kl: 1.5361 lr: 0.0002 grad_norm_g: 80.2506 grad_norm_d: 20.4088
======> Epoch: 27
Train Epoch: 28 [88.46%] G-Loss: 34.6338 D-Loss: 2.9091 Loss-g-fm: 1.5267 Loss-g-mel: 27.1195 Loss-g-dur: 2.4106 Loss-g-kl: 1.4465 lr: 0.0002 grad_norm_g: 100.2195 grad_norm_d: 35.1295
======> Epoch: 28
Train Epoch: 29 [84.62%] G-Loss: 34.0636 D-Loss: 2.7563 Loss-g-fm: 1.4044 Loss-g-mel: 27.3779 Loss-g-dur: 2.2897 Loss-g-kl: 1.2780 lr: 0.0002 grad_norm_g: 87.9528 grad_norm_d: 25.7596
======> Epoch: 29
Train Epoch: 30 [80.77%] G-Loss: 33.0753 D-Loss: 2.8096 Loss-g-fm: 1.2163 Loss-g-mel: 26.5984 Loss-g-dur: 2.2641 Loss-g-kl: 1.2125 lr: 0.0002 grad_norm_g: 65.0586 grad_norm_d: 18.1536
======> Epoch: 30
Train Epoch: 31 [76.92%] G-Loss: 34.9518 D-Loss: 2.7841 Loss-g-fm: 1.2032 Loss-g-mel: 28.1277 Loss-g-dur: 2.2246 Loss-g-kl: 1.2192 lr: 0.0002 grad_norm_g: 37.4078 grad_norm_d: 22.9980
======> Epoch: 31
Train Epoch: 32 [73.08%] G-Loss: 32.9813 D-Loss: 2.7359 Loss-g-fm: 1.3910 Loss-g-mel: 26.3394 Loss-g-dur: 2.3033 Loss-g-kl: 1.1960 lr: 0.0002 grad_norm_g: 83.2899 grad_norm_d: 16.8633
======> Epoch: 32
Train Epoch: 33 [69.23%] G-Loss: 32.6181 D-Loss: 2.7106 Loss-g-fm: 1.4830 Loss-g-mel: 25.9330 Loss-g-dur: 2.2818 Loss-g-kl: 1.0941 lr: 0.0002 grad_norm_g: 38.9345 grad_norm_d: 26.5554
======> Epoch: 33
Train Epoch: 34 [65.38%] G-Loss: 32.2213 D-Loss: 2.8190 Loss-g-fm: 1.2195 Loss-g-mel: 25.5878 Loss-g-dur: 2.2359 Loss-g-kl: 1.1874 lr: 0.0002 grad_norm_g: 80.0047 grad_norm_d: 31.0778
======> Epoch: 34
Train Epoch: 35 [61.54%] G-Loss: 32.9477 D-Loss: 2.6800 Loss-g-fm: 1.5665 Loss-g-mel: 26.0142 Loss-g-dur: 2.3048 Loss-g-kl: 1.0833 lr: 0.0002 grad_norm_g: 185.6728 grad_norm_d: 34.9059
======> Epoch: 35
Train Epoch: 36 [57.69%] G-Loss: 33.7028 D-Loss: 2.8123 Loss-g-fm: 1.0266 Loss-g-mel: 27.1118 Loss-g-dur: 2.2626 Loss-g-kl: 1.2469 lr: 0.0002 grad_norm_g: 169.3490 grad_norm_d: 42.8966
======> Epoch: 36
Train Epoch: 37 [53.85%] G-Loss: 33.0646 D-Loss: 2.7590 Loss-g-fm: 1.1938 Loss-g-mel: 26.6340 Loss-g-dur: 2.2918 Loss-g-kl: 1.1562 lr: 0.0002 grad_norm_g: 169.5120 grad_norm_d: 35.6965
======> Epoch: 37
Train Epoch: 38 [50.00%] G-Loss: 34.1141 D-Loss: 2.6688 Loss-g-fm: 1.9666 Loss-g-mel: 26.4559 Loss-g-dur: 2.3645 Loss-g-kl: 1.2546 lr: 0.0002 grad_norm_g: 182.0725 grad_norm_d: 54.5679
======> Epoch: 38
Train Epoch: 39 [46.15%] G-Loss: 35.9793 D-Loss: 2.7215 Loss-g-fm: 1.4231 Loss-g-mel: 28.7872 Loss-g-dur: 2.3877 Loss-g-kl: 1.2347 lr: 0.0002 grad_norm_g: 85.9041 grad_norm_d: 9.5818
======> Epoch: 39
Train Epoch: 40 [42.31%] G-Loss: 34.6070 D-Loss: 2.7095 Loss-g-fm: 1.7256 Loss-g-mel: 27.1008 Loss-g-dur: 2.3841 Loss-g-kl: 1.4418 lr: 0.0002 grad_norm_g: 83.5236 grad_norm_d: 20.1612
======> Epoch: 40
Train Epoch: 41 [38.46%] G-Loss: 33.8806 D-Loss: 2.7504 Loss-g-fm: 1.4949 Loss-g-mel: 27.1161 Loss-g-dur: 2.3131 Loss-g-kl: 1.3385 lr: 0.0002 grad_norm_g: 255.7336 grad_norm_d: 51.2497
======> Epoch: 41
Train Epoch: 42 [34.62%] G-Loss: 35.0622 D-Loss: 2.7312 Loss-g-fm: 1.7071 Loss-g-mel: 27.9722 Loss-g-dur: 2.3218 Loss-g-kl: 1.1423 lr: 0.0002 grad_norm_g: 326.0867 grad_norm_d: 47.3344
======> Epoch: 42
Train Epoch: 43 [30.77%] G-Loss: 32.3406 D-Loss: 2.8311 Loss-g-fm: 1.0727 Loss-g-mel: 25.8684 Loss-g-dur: 2.2962 Loss-g-kl: 1.1853 lr: 0.0002 grad_norm_g: 51.6430 grad_norm_d: 10.4327
======> Epoch: 43
Train Epoch: 44 [26.92%] G-Loss: 34.4403 D-Loss: 2.8418 Loss-g-fm: 1.7599 Loss-g-mel: 27.0980 Loss-g-dur: 2.4266 Loss-g-kl: 1.3748 lr: 0.0002 grad_norm_g: 185.3986 grad_norm_d: 56.1481
======> Epoch: 44
Train Epoch: 45 [23.08%] G-Loss: 31.3833 D-Loss: 2.8025 Loss-g-fm: 1.3705 Loss-g-mel: 24.8250 Loss-g-dur: 2.2667 Loss-g-kl: 1.1353 lr: 0.0002 grad_norm_g: 246.6931 grad_norm_d: 59.6749
======> Epoch: 45
Train Epoch: 46 [19.23%] G-Loss: 34.0807 D-Loss: 2.7444 Loss-g-fm: 1.4466 Loss-g-mel: 26.9759 Loss-g-dur: 2.3842 Loss-g-kl: 1.2887 lr: 0.0002 grad_norm_g: 153.1296 grad_norm_d: 38.6901
======> Epoch: 46
Train Epoch: 47 [15.38%] G-Loss: 33.3083 D-Loss: 2.8143 Loss-g-fm: 1.3480 Loss-g-mel: 26.5821 Loss-g-dur: 2.3027 Loss-g-kl: 1.1383 lr: 0.0002 grad_norm_g: 211.3079 grad_norm_d: 21.1292
======> Epoch: 47
Train Epoch: 48 [11.54%] G-Loss: 32.9858 D-Loss: 2.5973 Loss-g-fm: 2.1097 Loss-g-mel: 24.8215 Loss-g-dur: 2.6632 Loss-g-kl: 1.5306 lr: 0.0002 grad_norm_g: 199.6806 grad_norm_d: 31.5492
======> Epoch: 48
Train Epoch: 49 [7.69%] G-Loss: 33.6389 D-Loss: 2.6896 Loss-g-fm: 1.6499 Loss-g-mel: 26.6702 Loss-g-dur: 2.3126 Loss-g-kl: 1.0896 lr: 0.0002 grad_norm_g: 290.5984 grad_norm_d: 50.3714
======> Epoch: 49
Train Epoch: 50 [3.85%] G-Loss: 32.5022 D-Loss: 2.8839 Loss-g-fm: 1.6778 Loss-g-mel: 25.5610 Loss-g-dur: 2.3194 Loss-g-kl: 1.0230 lr: 0.0002 grad_norm_g: 380.2867 grad_norm_d: 98.1334
======> Epoch: 50
Train Epoch: 51 [0.00%] G-Loss: 32.2729 D-Loss: 2.7398 Loss-g-fm: 1.6837 Loss-g-mel: 25.0537 Loss-g-dur: 2.3346 Loss-g-kl: 1.2855 lr: 0.0002 grad_norm_g: 317.1039 grad_norm_d: 78.8474
Train Epoch: 51 [96.15%] G-Loss: 32.9657 D-Loss: 2.7990 Loss-g-fm: 1.6807 Loss-g-mel: 25.8193 Loss-g-dur: 2.3402 Loss-g-kl: 1.2218 lr: 0.0002 grad_norm_g: 196.4548 grad_norm_d: 36.7395
======> Epoch: 51
Train Epoch: 52 [92.31%] G-Loss: 34.2875 D-Loss: 2.7181 Loss-g-fm: 1.6430 Loss-g-mel: 27.2344 Loss-g-dur: 2.3528 Loss-g-kl: 1.2275 lr: 0.0002 grad_norm_g: 316.3580 grad_norm_d: 53.1224
======> Epoch: 52
Train Epoch: 53 [88.46%] G-Loss: 32.2108 D-Loss: 2.7797 Loss-g-fm: 1.3780 Loss-g-mel: 25.4382 Loss-g-dur: 2.3009 Loss-g-kl: 1.1073 lr: 0.0002 grad_norm_g: 167.7462 grad_norm_d: 36.7535
======> Epoch: 53
Train Epoch: 54 [84.62%] G-Loss: 33.3478 D-Loss: 2.7709 Loss-g-fm: 1.4243 Loss-g-mel: 26.2155 Loss-g-dur: 2.2722 Loss-g-kl: 1.2355 lr: 0.0002 grad_norm_g: 344.1730 grad_norm_d: 43.3945
======> Epoch: 54
Train Epoch: 55 [80.77%] G-Loss: 31.4071 D-Loss: 2.7427 Loss-g-fm: 1.3523 Loss-g-mel: 24.6550 Loss-g-dur: 2.3076 Loss-g-kl: 1.1963 lr: 0.0002 grad_norm_g: 362.8238 grad_norm_d: 85.8146
======> Epoch: 55
Train Epoch: 56 [76.92%] G-Loss: 32.9783 D-Loss: 2.5482 Loss-g-fm: 2.1430 Loss-g-mel: 25.3297 Loss-g-dur: 2.3254 Loss-g-kl: 1.1760 lr: 0.0002 grad_norm_g: 511.7227 grad_norm_d: 66.6764
======> Epoch: 56
Train Epoch: 57 [73.08%] G-Loss: 31.9720 D-Loss: 2.7677 Loss-g-fm: 1.6783 Loss-g-mel: 25.0404 Loss-g-dur: 2.3024 Loss-g-kl: 1.0723 lr: 0.0002 grad_norm_g: 432.4788 grad_norm_d: 114.0475
======> Epoch: 57
Train Epoch: 58 [69.23%] G-Loss: 32.9548 D-Loss: 2.7072 Loss-g-fm: 1.7032 Loss-g-mel: 26.0075 Loss-g-dur: 2.3208 Loss-g-kl: 1.0978 lr: 0.0002 grad_norm_g: 296.5456 grad_norm_d: 64.3988
======> Epoch: 58
Train Epoch: 59 [65.38%] G-Loss: 31.7838 D-Loss: 2.7220 Loss-g-fm: 1.6206 Loss-g-mel: 24.8002 Loss-g-dur: 2.3572 Loss-g-kl: 1.0874 lr: 0.0002 grad_norm_g: 183.3074 grad_norm_d: 75.6541
======> Epoch: 59
Train Epoch: 60 [61.54%] G-Loss: 33.4494 D-Loss: 2.8063 Loss-g-fm: 1.7486 Loss-g-mel: 26.3263 Loss-g-dur: 2.3259 Loss-g-kl: 1.0406 lr: 0.0002 grad_norm_g: 244.8211 grad_norm_d: 69.5401
======> Epoch: 60
Train Epoch: 61 [57.69%] G-Loss: 32.8830 D-Loss: 2.7716 Loss-g-fm: 2.1944 Loss-g-mel: 24.6514 Loss-g-dur: 2.6618 Loss-g-kl: 1.2937 lr: 0.0002 grad_norm_g: 196.0592 grad_norm_d: 47.4638
======> Epoch: 61
Train Epoch: 62 [53.85%] G-Loss: 31.2826 D-Loss: 2.5241 Loss-g-fm: 2.3986 Loss-g-mel: 22.5629 Loss-g-dur: 2.7103 Loss-g-kl: 1.3346 lr: 0.0002 grad_norm_g: 213.0215 grad_norm_d: 49.7004
======> Epoch: 62
Train Epoch: 63 [50.00%] G-Loss: 32.5531 D-Loss: 2.6851 Loss-g-fm: 2.1062 Loss-g-mel: 24.8684 Loss-g-dur: 2.3676 Loss-g-kl: 1.3186 lr: 0.0002 grad_norm_g: 431.9917 grad_norm_d: 81.2344
======> Epoch: 63
Train Epoch: 64 [46.15%] G-Loss: 32.8002 D-Loss: 2.6918 Loss-g-fm: 2.1520 Loss-g-mel: 25.2456 Loss-g-dur: 2.3107 Loss-g-kl: 1.1940 lr: 0.0002 grad_norm_g: 481.7028 grad_norm_d: 88.9652
======> Epoch: 64
Train Epoch: 65 [42.31%] G-Loss: 31.4619 D-Loss: 2.6832 Loss-g-fm: 1.6867 Loss-g-mel: 24.6446 Loss-g-dur: 2.1733 Loss-g-kl: 1.1459 lr: 0.0002 grad_norm_g: 307.5487 grad_norm_d: 43.4315
======> Epoch: 65
Train Epoch: 66 [38.46%] G-Loss: 29.4398 D-Loss: 2.8227 Loss-g-fm: 1.3378 Loss-g-mel: 23.1064 Loss-g-dur: 2.2169 Loss-g-kl: 0.9410 lr: 0.0002 grad_norm_g: 111.5155 grad_norm_d: 46.4761
======> Epoch: 66
Train Epoch: 67 [34.62%] G-Loss: 32.4510 D-Loss: 2.6733 Loss-g-fm: 1.8195 Loss-g-mel: 25.5046 Loss-g-dur: 2.1714 Loss-g-kl: 0.9197 lr: 0.0002 grad_norm_g: 61.3327 grad_norm_d: 11.8829
======> Epoch: 67
Train Epoch: 68 [30.77%] G-Loss: 32.1048 D-Loss: 2.6577 Loss-g-fm: 1.9720 Loss-g-mel: 24.6921 Loss-g-dur: 2.2257 Loss-g-kl: 1.1226 lr: 0.0002 grad_norm_g: 312.8516 grad_norm_d: 23.2270
======> Epoch: 68
Train Epoch: 69 [26.92%] G-Loss: 31.3899 D-Loss: 2.7390 Loss-g-fm: 1.7636 Loss-g-mel: 24.2055 Loss-g-dur: 2.1917 Loss-g-kl: 1.2514 lr: 0.0002 grad_norm_g: 302.2893 grad_norm_d: 60.3949
======> Epoch: 69
Train Epoch: 70 [23.08%] G-Loss: 30.0860 D-Loss: 2.6185 Loss-g-fm: 2.0297 Loss-g-mel: 22.9789 Loss-g-dur: 2.1886 Loss-g-kl: 1.0612 lr: 0.0002 grad_norm_g: 469.5211 grad_norm_d: 75.8849
======> Epoch: 70
Train Epoch: 71 [19.23%] G-Loss: 30.7072 D-Loss: 2.6172 Loss-g-fm: 2.3847 Loss-g-mel: 22.1869 Loss-g-dur: 2.5956 Loss-g-kl: 1.2589 lr: 0.0002 grad_norm_g: 227.7856 grad_norm_d: 71.1621
======> Epoch: 71
Train Epoch: 72 [15.38%] G-Loss: 32.2417 D-Loss: 2.5778 Loss-g-fm: 2.0760 Loss-g-mel: 24.8259 Loss-g-dur: 2.2137 Loss-g-kl: 0.9353 lr: 0.0002 grad_norm_g: 364.9704 grad_norm_d: 54.1919
======> Epoch: 72
Train Epoch: 73 [11.54%] G-Loss: 28.3714 D-Loss: 2.5726 Loss-g-fm: 1.8858 Loss-g-mel: 21.3156 Loss-g-dur: 2.1030 Loss-g-kl: 1.0471 lr: 0.0002 grad_norm_g: 432.0943 grad_norm_d: 86.7931
======> Epoch: 73
Train Epoch: 74 [7.69%] G-Loss: 31.3924 D-Loss: 2.7130 Loss-g-fm: 2.0749 Loss-g-mel: 23.9858 Loss-g-dur: 2.1286 Loss-g-kl: 1.0389 lr: 0.0002 grad_norm_g: 358.8739 grad_norm_d: 44.0739
======> Epoch: 74
Train Epoch: 75 [3.85%] G-Loss: 30.9138 D-Loss: 2.5788 Loss-g-fm: 2.3108 Loss-g-mel: 23.3364 Loss-g-dur: 2.0867 Loss-g-kl: 1.0898 lr: 0.0002 grad_norm_g: 629.3089 grad_norm_d: 81.8305
======> Epoch: 75
Train Epoch: 76 [0.00%] G-Loss: 33.6157 D-Loss: 2.5191 Loss-g-fm: 2.4731 Loss-g-mel: 25.8956 Loss-g-dur: 2.1602 Loss-g-kl: 1.1318 lr: 0.0002 grad_norm_g: 468.2634 grad_norm_d: 82.1067
Train Epoch: 76 [96.15%] G-Loss: 30.4940 D-Loss: 2.6670 Loss-g-fm: 1.8514 Loss-g-mel: 23.4995 Loss-g-dur: 2.1128 Loss-g-kl: 1.1002 lr: 0.0002 grad_norm_g: 214.5035 grad_norm_d: 60.8754
======> Epoch: 76
Train Epoch: 77 [92.31%] G-Loss: 30.6115 D-Loss: 2.5476 Loss-g-fm: 2.3942 Loss-g-mel: 22.7464 Loss-g-dur: 2.0983 Loss-g-kl: 1.1999 lr: 0.0002 grad_norm_g: 539.0993 grad_norm_d: 88.8642
terminate called without an active exception
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2b37a4820>
Traceback (most recent call last):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2402926) is killed by signal: Aborted. 
Saving model and optimizer state at iteration 77 to /ZFS4T/tts/data/VITS/model_saved/G_2000.pth
Saving model and optimizer state at iteration 77 to /ZFS4T/tts/data/VITS/model_saved/D_2000.pth
======> Epoch: 77
Train Epoch: 78 [88.46%] G-Loss: 30.1311 D-Loss: 2.6318 Loss-g-fm: 2.0216 Loss-g-mel: 22.7822 Loss-g-dur: 2.1355 Loss-g-kl: 1.2541 lr: 0.0002 grad_norm_g: 414.5584 grad_norm_d: 68.0666
======> Epoch: 78
Train Epoch: 79 [84.62%] G-Loss: 31.4501 D-Loss: 2.7848 Loss-g-fm: 1.8741 Loss-g-mel: 24.1520 Loss-g-dur: 2.1491 Loss-g-kl: 1.2302 lr: 0.0002 grad_norm_g: 252.2021 grad_norm_d: 64.4668
======> Epoch: 79
Train Epoch: 80 [80.77%] G-Loss: 31.2718 D-Loss: 2.5659 Loss-g-fm: 2.1686 Loss-g-mel: 23.8311 Loss-g-dur: 2.1464 Loss-g-kl: 1.1416 lr: 0.0002 grad_norm_g: 489.0321 grad_norm_d: 49.0000
======> Epoch: 80
Train Epoch: 81 [76.92%] G-Loss: 30.1260 D-Loss: 2.6076 Loss-g-fm: 2.0138 Loss-g-mel: 22.9871 Loss-g-dur: 2.1049 Loss-g-kl: 1.1668 lr: 0.0002 grad_norm_g: 75.7682 grad_norm_d: 51.0620
======> Epoch: 81
Train Epoch: 82 [73.08%] G-Loss: 31.5610 D-Loss: 2.5810 Loss-g-fm: 2.4421 Loss-g-mel: 23.8925 Loss-g-dur: 2.1526 Loss-g-kl: 1.0271 lr: 0.0002 grad_norm_g: 506.9234 grad_norm_d: 77.5551
======> Epoch: 82
Train Epoch: 83 [69.23%] G-Loss: 30.3020 D-Loss: 2.6702 Loss-g-fm: 1.9501 Loss-g-mel: 23.1831 Loss-g-dur: 2.1060 Loss-g-kl: 1.1323 lr: 0.0002 grad_norm_g: 472.4973 grad_norm_d: 105.6894
======> Epoch: 83
Train Epoch: 84 [65.38%] G-Loss: 34.0925 D-Loss: 2.7359 Loss-g-fm: 2.4235 Loss-g-mel: 25.9172 Loss-g-dur: 2.2095 Loss-g-kl: 1.1393 lr: 0.0002 grad_norm_g: 395.4172 grad_norm_d: 85.1748
======> Epoch: 84
Train Epoch: 85 [61.54%] G-Loss: 33.1797 D-Loss: 2.6695 Loss-g-fm: 2.4781 Loss-g-mel: 25.5674 Loss-g-dur: 2.1796 Loss-g-kl: 1.1579 lr: 0.0002 grad_norm_g: 236.7516 grad_norm_d: 47.1573
======> Epoch: 85
Train Epoch: 86 [57.69%] G-Loss: 32.2559 D-Loss: 2.6245 Loss-g-fm: 2.2445 Loss-g-mel: 24.5360 Loss-g-dur: 2.1845 Loss-g-kl: 1.3391 lr: 0.0002 grad_norm_g: 180.0278 grad_norm_d: 26.8704
======> Epoch: 86
Train Epoch: 87 [53.85%] G-Loss: 31.7642 D-Loss: 2.5169 Loss-g-fm: 2.7489 Loss-g-mel: 23.7182 Loss-g-dur: 2.1105 Loss-g-kl: 1.0090 lr: 0.0002 grad_norm_g: 630.4684 grad_norm_d: 95.5161
======> Epoch: 87
Train Epoch: 88 [50.00%] G-Loss: 29.8443 D-Loss: 2.6306 Loss-g-fm: 2.1067 Loss-g-mel: 22.2773 Loss-g-dur: 2.0662 Loss-g-kl: 1.0740 lr: 0.0002 grad_norm_g: 475.0055 grad_norm_d: 88.4853
======> Epoch: 88
Train Epoch: 89 [46.15%] G-Loss: 33.1636 D-Loss: 2.4711 Loss-g-fm: 3.0533 Loss-g-mel: 24.6936 Loss-g-dur: 2.2096 Loss-g-kl: 1.0058 lr: 0.0002 grad_norm_g: 628.9753 grad_norm_d: 124.6662
======> Epoch: 89
Train Epoch: 90 [42.31%] G-Loss: 31.8557 D-Loss: 2.5116 Loss-g-fm: 2.7885 Loss-g-mel: 23.9314 Loss-g-dur: 2.0627 Loss-g-kl: 1.0646 lr: 0.0002 grad_norm_g: 694.4118 grad_norm_d: 115.9056
======> Epoch: 90
Train Epoch: 91 [38.46%] G-Loss: 33.1647 D-Loss: 2.4245 Loss-g-fm: 3.0305 Loss-g-mel: 24.5933 Loss-g-dur: 2.1775 Loss-g-kl: 1.3011 lr: 0.0002 grad_norm_g: 469.3006 grad_norm_d: 61.0996
======> Epoch: 91
Train Epoch: 92 [34.62%] G-Loss: 29.7530 D-Loss: 2.5775 Loss-g-fm: 2.3229 Loss-g-mel: 21.8321 Loss-g-dur: 2.1106 Loss-g-kl: 1.2076 lr: 0.0002 grad_norm_g: 646.1293 grad_norm_d: 109.8300
======> Epoch: 92
Train Epoch: 93 [30.77%] G-Loss: 30.1314 D-Loss: 2.5958 Loss-g-fm: 2.6282 Loss-g-mel: 22.1535 Loss-g-dur: 2.1280 Loss-g-kl: 1.1590 lr: 0.0002 grad_norm_g: 651.0297 grad_norm_d: 99.3929
======> Epoch: 93
Train Epoch: 94 [26.92%] G-Loss: 32.1966 D-Loss: 2.8256 Loss-g-fm: 2.1845 Loss-g-mel: 24.8336 Loss-g-dur: 2.1834 Loss-g-kl: 1.1147 lr: 0.0002 grad_norm_g: 309.4816 grad_norm_d: 52.5563
======> Epoch: 94
Train Epoch: 95 [23.08%] G-Loss: 32.3461 D-Loss: 2.5524 Loss-g-fm: 2.5668 Loss-g-mel: 24.1198 Loss-g-dur: 2.1719 Loss-g-kl: 1.2096 lr: 0.0002 grad_norm_g: 605.0008 grad_norm_d: 109.9437
======> Epoch: 95
Train Epoch: 96 [19.23%] G-Loss: 32.5599 D-Loss: 2.5373 Loss-g-fm: 2.7379 Loss-g-mel: 24.2376 Loss-g-dur: 2.1117 Loss-g-kl: 1.3676 lr: 0.0002 grad_norm_g: 623.9864 grad_norm_d: 98.5373
======> Epoch: 96
Train Epoch: 97 [15.38%] G-Loss: 32.7167 D-Loss: 2.6555 Loss-g-fm: 2.6142 Loss-g-mel: 24.8177 Loss-g-dur: 2.1795 Loss-g-kl: 1.1846 lr: 0.0002 grad_norm_g: 455.7172 grad_norm_d: 90.6027
======> Epoch: 97
Train Epoch: 98 [11.54%] G-Loss: 30.7829 D-Loss: 2.5642 Loss-g-fm: 2.4618 Loss-g-mel: 22.9357 Loss-g-dur: 2.1029 Loss-g-kl: 1.1498 lr: 0.0002 grad_norm_g: 383.6069 grad_norm_d: 53.5971
======> Epoch: 98
Train Epoch: 99 [7.69%] G-Loss: 32.5104 D-Loss: 2.5424 Loss-g-fm: 3.0086 Loss-g-mel: 23.6220 Loss-g-dur: 2.2570 Loss-g-kl: 1.4010 lr: 0.0002 grad_norm_g: 749.7984 grad_norm_d: 66.7766
======> Epoch: 99
Train Epoch: 100 [3.85%] G-Loss: 33.4397 D-Loss: 2.5167 Loss-g-fm: 3.2153 Loss-g-mel: 24.7463 Loss-g-dur: 2.0475 Loss-g-kl: 1.0670 lr: 0.0002 grad_norm_g: 837.6865 grad_norm_d: 103.4962
======> Epoch: 100
Train Epoch: 101 [0.00%] G-Loss: 31.4240 D-Loss: 2.6005 Loss-g-fm: 3.0201 Loss-g-mel: 23.1013 Loss-g-dur: 2.0936 Loss-g-kl: 0.9956 lr: 0.0002 grad_norm_g: 856.6709 grad_norm_d: 123.3807
Train Epoch: 101 [96.15%] G-Loss: 33.5932 D-Loss: 2.4232 Loss-g-fm: 3.2101 Loss-g-mel: 24.7336 Loss-g-dur: 2.1209 Loss-g-kl: 1.2767 lr: 0.0002 grad_norm_g: 683.6701 grad_norm_d: 92.8887
======> Epoch: 101
Train Epoch: 102 [92.31%] G-Loss: 31.3317 D-Loss: 2.7055 Loss-g-fm: 2.7366 Loss-g-mel: 22.6399 Loss-g-dur: 2.0613 Loss-g-kl: 1.2075 lr: 0.0002 grad_norm_g: 814.9878 grad_norm_d: 120.0320
======> Epoch: 102
Train Epoch: 103 [88.46%] G-Loss: 31.6912 D-Loss: 2.4326 Loss-g-fm: 3.0124 Loss-g-mel: 23.3202 Loss-g-dur: 2.0618 Loss-g-kl: 0.9209 lr: 0.0002 grad_norm_g: 762.2746 grad_norm_d: 109.0353
======> Epoch: 103
Train Epoch: 104 [84.62%] G-Loss: 31.1754 D-Loss: 2.6611 Loss-g-fm: 2.5388 Loss-g-mel: 23.5131 Loss-g-dur: 2.0643 Loss-g-kl: 1.1112 lr: 0.0002 grad_norm_g: 361.2482 grad_norm_d: 134.6124
======> Epoch: 104
Train Epoch: 105 [80.77%] G-Loss: 30.7357 D-Loss: 2.5657 Loss-g-fm: 2.5694 Loss-g-mel: 22.6582 Loss-g-dur: 2.0225 Loss-g-kl: 1.2465 lr: 0.0002 grad_norm_g: 664.1630 grad_norm_d: 107.8972
======> Epoch: 105
Train Epoch: 106 [76.92%] G-Loss: 28.5406 D-Loss: 2.5575 Loss-g-fm: 2.6131 Loss-g-mel: 20.8149 Loss-g-dur: 2.0335 Loss-g-kl: 1.0102 lr: 0.0002 grad_norm_g: 708.2266 grad_norm_d: 117.3655
======> Epoch: 106
Train Epoch: 107 [73.08%] G-Loss: 30.9750 D-Loss: 2.6773 Loss-g-fm: 2.3869 Loss-g-mel: 23.2517 Loss-g-dur: 2.0899 Loss-g-kl: 1.3125 lr: 0.0002 grad_norm_g: 216.7323 grad_norm_d: 105.4241
======> Epoch: 107
Train Epoch: 108 [69.23%] G-Loss: 33.8956 D-Loss: 2.6585 Loss-g-fm: 3.0146 Loss-g-mel: 24.9653 Loss-g-dur: 2.0446 Loss-g-kl: 1.1334 lr: 0.0002 grad_norm_g: 807.6328 grad_norm_d: 130.9337
======> Epoch: 108
Train Epoch: 109 [65.38%] G-Loss: 31.2889 D-Loss: 2.5200 Loss-g-fm: 3.4577 Loss-g-mel: 22.3278 Loss-g-dur: 2.0767 Loss-g-kl: 0.9671 lr: 0.0002 grad_norm_g: 876.2291 grad_norm_d: 120.2817
======> Epoch: 109
Train Epoch: 110 [61.54%] G-Loss: 31.5067 D-Loss: 2.6670 Loss-g-fm: 2.6061 Loss-g-mel: 23.3172 Loss-g-dur: 2.0435 Loss-g-kl: 1.1370 lr: 0.0002 grad_norm_g: 503.6820 grad_norm_d: 153.0043
======> Epoch: 110
Train Epoch: 111 [57.69%] G-Loss: 31.0377 D-Loss: 2.5890 Loss-g-fm: 2.3268 Loss-g-mel: 23.4929 Loss-g-dur: 2.0870 Loss-g-kl: 1.1634 lr: 0.0002 grad_norm_g: 164.5226 grad_norm_d: 89.8797
======> Epoch: 111
Train Epoch: 112 [53.85%] G-Loss: 31.5570 D-Loss: 2.5844 Loss-g-fm: 2.8049 Loss-g-mel: 23.1139 Loss-g-dur: 2.1139 Loss-g-kl: 1.2920 lr: 0.0002 grad_norm_g: 775.4319 grad_norm_d: 118.5153
======> Epoch: 112
Train Epoch: 113 [50.00%] G-Loss: 31.9544 D-Loss: 2.4373 Loss-g-fm: 3.1071 Loss-g-mel: 23.4325 Loss-g-dur: 2.0858 Loss-g-kl: 1.0100 lr: 0.0002 grad_norm_g: 751.3896 grad_norm_d: 51.4001
======> Epoch: 113
Train Epoch: 114 [46.15%] G-Loss: 31.9041 D-Loss: 2.6719 Loss-g-fm: 3.1065 Loss-g-mel: 23.3471 Loss-g-dur: 2.1492 Loss-g-kl: 1.1920 lr: 0.0002 grad_norm_g: 733.2831 grad_norm_d: 104.5006
======> Epoch: 114
Train Epoch: 115 [42.31%] G-Loss: 32.8903 D-Loss: 2.3753 Loss-g-fm: 3.5091 Loss-g-mel: 23.5121 Loss-g-dur: 2.1605 Loss-g-kl: 1.2607 lr: 0.0002 grad_norm_g: 749.6750 grad_norm_d: 85.8915
======> Epoch: 115
Train Epoch: 116 [38.46%] G-Loss: 30.7365 D-Loss: 2.5235 Loss-g-fm: 4.1272 Loss-g-mel: 20.4909 Loss-g-dur: 2.1105 Loss-g-kl: 1.1255 lr: 0.0002 grad_norm_g: 746.3235 grad_norm_d: 67.9892
======> Epoch: 116
Train Epoch: 117 [34.62%] G-Loss: 33.3386 D-Loss: 2.4348 Loss-g-fm: 3.0104 Loss-g-mel: 24.4645 Loss-g-dur: 2.1627 Loss-g-kl: 1.4360 lr: 0.0002 grad_norm_g: 568.8999 grad_norm_d: 143.8733
======> Epoch: 117
Train Epoch: 118 [30.77%] G-Loss: 31.8683 D-Loss: 2.7934 Loss-g-fm: 3.0273 Loss-g-mel: 23.5234 Loss-g-dur: 2.1290 Loss-g-kl: 1.1882 lr: 0.0002 grad_norm_g: 348.3415 grad_norm_d: 58.9969
======> Epoch: 118
Train Epoch: 119 [26.92%] G-Loss: 30.2695 D-Loss: 2.3959 Loss-g-fm: 3.7747 Loss-g-mel: 20.9284 Loss-g-dur: 2.0266 Loss-g-kl: 1.1430 lr: 0.0002 grad_norm_g: 768.5467 grad_norm_d: 113.3587
======> Epoch: 119
Train Epoch: 120 [23.08%] G-Loss: 30.5061 D-Loss: 2.4855 Loss-g-fm: 3.3011 Loss-g-mel: 21.5979 Loss-g-dur: 2.2577 Loss-g-kl: 1.0390 lr: 0.0002 grad_norm_g: 857.7777 grad_norm_d: 172.8322
======> Epoch: 120
Train Epoch: 121 [19.23%] G-Loss: 30.8474 D-Loss: 2.4871 Loss-g-fm: 3.1170 Loss-g-mel: 22.0859 Loss-g-dur: 2.1559 Loss-g-kl: 1.1393 lr: 0.0002 grad_norm_g: 697.3165 grad_norm_d: 84.0881
======> Epoch: 121
Train Epoch: 122 [15.38%] G-Loss: 32.8290 D-Loss: 2.3430 Loss-g-fm: 3.8502 Loss-g-mel: 22.9388 Loss-g-dur: 2.2160 Loss-g-kl: 1.2686 lr: 0.0002 grad_norm_g: 937.4951 grad_norm_d: 105.0290
======> Epoch: 122
Train Epoch: 123 [11.54%] G-Loss: 33.8885 D-Loss: 2.3042 Loss-g-fm: 3.7233 Loss-g-mel: 24.3306 Loss-g-dur: 2.0327 Loss-g-kl: 1.1725 lr: 0.0002 grad_norm_g: 779.3879 grad_norm_d: 111.8176
======> Epoch: 123
Train Epoch: 124 [7.69%] G-Loss: 31.5679 D-Loss: 2.5364 Loss-g-fm: 2.9622 Loss-g-mel: 23.2798 Loss-g-dur: 2.0642 Loss-g-kl: 1.1042 lr: 0.0002 grad_norm_g: 611.3652 grad_norm_d: 93.7305
======> Epoch: 124
Train Epoch: 125 [3.85%] G-Loss: 28.8385 D-Loss: 2.6364 Loss-g-fm: 2.2730 Loss-g-mel: 21.1115 Loss-g-dur: 2.0201 Loss-g-kl: 1.1626 lr: 0.0002 grad_norm_g: 425.9342 grad_norm_d: 19.1304
======> Epoch: 125
Train Epoch: 126 [0.00%] G-Loss: 31.1861 D-Loss: 2.5513 Loss-g-fm: 2.9783 Loss-g-mel: 22.5941 Loss-g-dur: 2.2007 Loss-g-kl: 1.1262 lr: 0.0002 grad_norm_g: 270.4946 grad_norm_d: 41.9078
Train Epoch: 126 [96.15%] G-Loss: 32.2688 D-Loss: 2.5346 Loss-g-fm: 3.2544 Loss-g-mel: 23.3001 Loss-g-dur: 2.1310 Loss-g-kl: 1.3393 lr: 0.0002 grad_norm_g: 656.7604 grad_norm_d: 106.8199
======> Epoch: 126
Train Epoch: 127 [92.31%] G-Loss: 31.1163 D-Loss: 2.4702 Loss-g-fm: 3.1413 Loss-g-mel: 22.8574 Loss-g-dur: 2.0356 Loss-g-kl: 1.0578 lr: 0.0002 grad_norm_g: 590.3588 grad_norm_d: 99.8829
======> Epoch: 127
Train Epoch: 128 [88.46%] G-Loss: 32.1793 D-Loss: 2.5233 Loss-g-fm: 3.3685 Loss-g-mel: 23.3567 Loss-g-dur: 2.0895 Loss-g-kl: 1.1144 lr: 0.0002 grad_norm_g: 607.3536 grad_norm_d: 105.3434
======> Epoch: 128
Train Epoch: 129 [84.62%] G-Loss: 31.8105 D-Loss: 2.6437 Loss-g-fm: 3.2031 Loss-g-mel: 23.0925 Loss-g-dur: 2.0236 Loss-g-kl: 1.0698 lr: 0.0002 grad_norm_g: 699.4127 grad_norm_d: 139.0679
======> Epoch: 129
Train Epoch: 130 [80.77%] G-Loss: 34.1388 D-Loss: 2.4254 Loss-g-fm: 3.8741 Loss-g-mel: 24.8165 Loss-g-dur: 2.0589 Loss-g-kl: 1.1741 lr: 0.0002 grad_norm_g: 729.0176 grad_norm_d: 125.8300
======> Epoch: 130
Train Epoch: 131 [76.92%] G-Loss: 31.9824 D-Loss: 2.3313 Loss-g-fm: 3.7402 Loss-g-mel: 22.5586 Loss-g-dur: 2.0687 Loss-g-kl: 1.2315 lr: 0.0002 grad_norm_g: 786.4834 grad_norm_d: 111.7520
======> Epoch: 131
Train Epoch: 132 [73.08%] G-Loss: 34.3053 D-Loss: 2.4465 Loss-g-fm: 4.0633 Loss-g-mel: 24.6627 Loss-g-dur: 2.0859 Loss-g-kl: 1.2167 lr: 0.0002 grad_norm_g: 856.4188 grad_norm_d: 140.2309
======> Epoch: 132
Train Epoch: 133 [69.23%] G-Loss: 32.7177 D-Loss: 2.3993 Loss-g-fm: 3.2819 Loss-g-mel: 23.5073 Loss-g-dur: 2.1249 Loss-g-kl: 1.3670 lr: 0.0002 grad_norm_g: 689.7181 grad_norm_d: 85.7466
======> Epoch: 133
Train Epoch: 134 [65.38%] G-Loss: 31.6888 D-Loss: 2.5421 Loss-g-fm: 2.7642 Loss-g-mel: 23.4868 Loss-g-dur: 1.9888 Loss-g-kl: 1.1429 lr: 0.0002 grad_norm_g: 423.9501 grad_norm_d: 134.7939
======> Epoch: 134
Train Epoch: 135 [61.54%] G-Loss: 29.4376 D-Loss: 2.4120 Loss-g-fm: 3.6842 Loss-g-mel: 20.4902 Loss-g-dur: 1.9633 Loss-g-kl: 1.0929 lr: 0.0002 grad_norm_g: 306.2529 grad_norm_d: 107.8559
======> Epoch: 135
Train Epoch: 136 [57.69%] G-Loss: 32.5280 D-Loss: 2.5346 Loss-g-fm: 3.2085 Loss-g-mel: 23.9091 Loss-g-dur: 2.0760 Loss-g-kl: 1.1998 lr: 0.0002 grad_norm_g: 574.6237 grad_norm_d: 80.5306
======> Epoch: 136
Train Epoch: 137 [53.85%] G-Loss: 30.4949 D-Loss: 2.5851 Loss-g-fm: 2.9119 Loss-g-mel: 22.4855 Loss-g-dur: 2.0094 Loss-g-kl: 1.0030 lr: 0.0002 grad_norm_g: 597.4575 grad_norm_d: 117.6463
======> Epoch: 137
Train Epoch: 138 [50.00%] G-Loss: 31.8082 D-Loss: 2.4384 Loss-g-fm: 3.0209 Loss-g-mel: 23.2289 Loss-g-dur: 2.1098 Loss-g-kl: 1.2366 lr: 0.0002 grad_norm_g: 488.0455 grad_norm_d: 130.4730
======> Epoch: 138
Train Epoch: 139 [46.15%] G-Loss: 31.9048 D-Loss: 2.5208 Loss-g-fm: 3.5193 Loss-g-mel: 23.0803 Loss-g-dur: 1.9950 Loss-g-kl: 0.9499 lr: 0.0002 grad_norm_g: 725.9956 grad_norm_d: 84.7840
======> Epoch: 139
Train Epoch: 140 [42.31%] G-Loss: 30.8829 D-Loss: 2.6499 Loss-g-fm: 2.9099 Loss-g-mel: 22.2257 Loss-g-dur: 1.9977 Loss-g-kl: 1.2253 lr: 0.0002 grad_norm_g: 610.8595 grad_norm_d: 85.3184
======> Epoch: 140
Train Epoch: 141 [38.46%] G-Loss: 30.4556 D-Loss: 2.6195 Loss-g-fm: 2.6502 Loss-g-mel: 22.5464 Loss-g-dur: 2.0150 Loss-g-kl: 1.1334 lr: 0.0002 grad_norm_g: 84.6351 grad_norm_d: 55.0714
======> Epoch: 141
Train Epoch: 142 [34.62%] G-Loss: 31.7933 D-Loss: 2.4455 Loss-g-fm: 3.1985 Loss-g-mel: 23.1789 Loss-g-dur: 2.0244 Loss-g-kl: 1.1914 lr: 0.0002 grad_norm_g: 151.4985 grad_norm_d: 85.5676
======> Epoch: 142
Train Epoch: 143 [30.77%] G-Loss: 30.8115 D-Loss: 2.4404 Loss-g-fm: 3.2284 Loss-g-mel: 22.1388 Loss-g-dur: 2.0148 Loss-g-kl: 1.1692 lr: 0.0002 grad_norm_g: 755.4018 grad_norm_d: 93.4709
======> Epoch: 143
Train Epoch: 144 [26.92%] G-Loss: 30.6843 D-Loss: 2.5901 Loss-g-fm: 3.3162 Loss-g-mel: 21.6570 Loss-g-dur: 2.0868 Loss-g-kl: 1.1463 lr: 0.0002 grad_norm_g: 705.7219 grad_norm_d: 151.5618
======> Epoch: 144
Train Epoch: 145 [23.08%] G-Loss: 30.1775 D-Loss: 2.5352 Loss-g-fm: 3.1473 Loss-g-mel: 21.6157 Loss-g-dur: 1.9993 Loss-g-kl: 0.8761 lr: 0.0002 grad_norm_g: 804.2197 grad_norm_d: 120.8489
======> Epoch: 145
Train Epoch: 146 [19.23%] G-Loss: 30.3522 D-Loss: 2.4824 Loss-g-fm: 3.5904 Loss-g-mel: 21.4735 Loss-g-dur: 2.1049 Loss-g-kl: 0.9946 lr: 0.0002 grad_norm_g: 697.9400 grad_norm_d: 117.4676
======> Epoch: 146
Train Epoch: 147 [15.38%] G-Loss: 32.3664 D-Loss: 2.5110 Loss-g-fm: 3.3502 Loss-g-mel: 23.4738 Loss-g-dur: 2.1975 Loss-g-kl: 1.3629 lr: 0.0002 grad_norm_g: 581.3422 grad_norm_d: 95.5456
======> Epoch: 147
Train Epoch: 148 [11.54%] G-Loss: 30.1561 D-Loss: 2.5282 Loss-g-fm: 3.0057 Loss-g-mel: 21.9079 Loss-g-dur: 2.0070 Loss-g-kl: 1.0780 lr: 0.0002 grad_norm_g: 685.3968 grad_norm_d: 89.3646
======> Epoch: 148
Train Epoch: 149 [7.69%] G-Loss: 29.6714 D-Loss: 2.5928 Loss-g-fm: 2.4901 Loss-g-mel: 22.1513 Loss-g-dur: 1.9513 Loss-g-kl: 1.1345 lr: 0.0002 grad_norm_g: 397.2434 grad_norm_d: 176.3711
======> Epoch: 149
Train Epoch: 150 [3.85%] G-Loss: 30.7355 D-Loss: 2.5112 Loss-g-fm: 3.3276 Loss-g-mel: 22.0524 Loss-g-dur: 1.9896 Loss-g-kl: 1.1568 lr: 0.0002 grad_norm_g: 770.1702 grad_norm_d: 148.2035
======> Epoch: 150
Train Epoch: 151 [0.00%] G-Loss: 32.8675 D-Loss: 2.4037 Loss-g-fm: 4.0063 Loss-g-mel: 23.0610 Loss-g-dur: 2.0656 Loss-g-kl: 1.3568 lr: 0.0002 grad_norm_g: 584.9656 grad_norm_d: 122.9837
Train Epoch: 151 [96.15%] G-Loss: 31.3230 D-Loss: 2.5361 Loss-g-fm: 3.4025 Loss-g-mel: 22.6610 Loss-g-dur: 2.0222 Loss-g-kl: 1.1291 lr: 0.0002 grad_norm_g: 461.5859 grad_norm_d: 71.8742
======> Epoch: 151
Train Epoch: 152 [92.31%] G-Loss: 30.9114 D-Loss: 2.6133 Loss-g-fm: 3.6498 Loss-g-mel: 21.8129 Loss-g-dur: 1.9977 Loss-g-kl: 1.1848 lr: 0.0002 grad_norm_g: 807.7248 grad_norm_d: 186.6271
======> Epoch: 152
Train Epoch: 153 [88.46%] G-Loss: 32.4887 D-Loss: 2.5285 Loss-g-fm: 3.1068 Loss-g-mel: 23.8897 Loss-g-dur: 2.1188 Loss-g-kl: 1.2344 lr: 0.0002 grad_norm_g: 76.1758 grad_norm_d: 85.2188
======> Epoch: 153
Train Epoch: 154 [84.62%] G-Loss: 31.3247 D-Loss: 2.3645 Loss-g-fm: 3.3556 Loss-g-mel: 22.5935 Loss-g-dur: 2.0018 Loss-g-kl: 1.1689 lr: 0.0002 grad_norm_g: 462.5080 grad_norm_d: 102.6110
Saving model and optimizer state at iteration 154 to /ZFS4T/tts/data/VITS/model_saved/G_4000.pth
Saving model and optimizer state at iteration 154 to /ZFS4T/tts/data/VITS/model_saved/D_4000.pth
======> Epoch: 154
Train Epoch: 155 [80.77%] G-Loss: 29.4173 D-Loss: 2.4592 Loss-g-fm: 3.1070 Loss-g-mel: 20.8751 Loss-g-dur: 1.9700 Loss-g-kl: 1.1302 lr: 0.0002 grad_norm_g: 719.2783 grad_norm_d: 128.3486
======> Epoch: 155
Train Epoch: 156 [76.92%] G-Loss: 30.0248 D-Loss: 2.5650 Loss-g-fm: 2.8988 Loss-g-mel: 21.8380 Loss-g-dur: 2.0041 Loss-g-kl: 1.2678 lr: 0.0002 grad_norm_g: 327.9467 grad_norm_d: 161.7859
======> Epoch: 156
Train Epoch: 157 [73.08%] G-Loss: 34.5059 D-Loss: 2.4788 Loss-g-fm: 4.6476 Loss-g-mel: 23.7844 Loss-g-dur: 2.0794 Loss-g-kl: 1.2650 lr: 0.0002 grad_norm_g: 694.9729 grad_norm_d: 104.5194
======> Epoch: 157
Train Epoch: 158 [69.23%] G-Loss: 31.4386 D-Loss: 2.5695 Loss-g-fm: 3.2231 Loss-g-mel: 22.7012 Loss-g-dur: 2.0487 Loss-g-kl: 1.1928 lr: 0.0002 grad_norm_g: 249.3349 grad_norm_d: 82.2093
======> Epoch: 158
Train Epoch: 159 [65.38%] G-Loss: 31.2526 D-Loss: 2.4826 Loss-g-fm: 3.2689 Loss-g-mel: 22.6522 Loss-g-dur: 1.9775 Loss-g-kl: 1.1818 lr: 0.0002 grad_norm_g: 557.5747 grad_norm_d: 67.3627
======> Epoch: 159
Train Epoch: 160 [61.54%] G-Loss: 30.3016 D-Loss: 2.3470 Loss-g-fm: 3.6767 Loss-g-mel: 21.3194 Loss-g-dur: 1.9751 Loss-g-kl: 0.9678 lr: 0.0002 grad_norm_g: 674.1303 grad_norm_d: 110.1973
======> Epoch: 160
Train Epoch: 161 [57.69%] G-Loss: 30.5447 D-Loss: 2.5425 Loss-g-fm: 3.0590 Loss-g-mel: 22.1666 Loss-g-dur: 2.0633 Loss-g-kl: 1.1263 lr: 0.0002 grad_norm_g: 494.0408 grad_norm_d: 98.6105
======> Epoch: 161
Train Epoch: 162 [53.85%] G-Loss: 32.1442 D-Loss: 2.3994 Loss-g-fm: 3.6019 Loss-g-mel: 22.9350 Loss-g-dur: 2.1747 Loss-g-kl: 1.2109 lr: 0.0002 grad_norm_g: 484.5173 grad_norm_d: 141.8125
======> Epoch: 162
Train Epoch: 163 [50.00%] G-Loss: 31.5817 D-Loss: 2.5251 Loss-g-fm: 3.0343 Loss-g-mel: 22.9527 Loss-g-dur: 2.1673 Loss-g-kl: 1.4569 lr: 0.0002 grad_norm_g: 195.1566 grad_norm_d: 25.7430
======> Epoch: 163
Train Epoch: 164 [46.15%] G-Loss: 31.4542 D-Loss: 2.5960 Loss-g-fm: 3.3065 Loss-g-mel: 22.7328 Loss-g-dur: 2.0539 Loss-g-kl: 1.1637 lr: 0.0002 grad_norm_g: 526.2222 grad_norm_d: 92.1303
======> Epoch: 164
Train Epoch: 165 [42.31%] G-Loss: 29.9720 D-Loss: 2.5393 Loss-g-fm: 3.2209 Loss-g-mel: 21.4791 Loss-g-dur: 1.9954 Loss-g-kl: 1.2559 lr: 0.0002 grad_norm_g: 550.8444 grad_norm_d: 100.6818
======> Epoch: 165
Train Epoch: 166 [38.46%] G-Loss: 30.6319 D-Loss: 2.5399 Loss-g-fm: 3.0401 Loss-g-mel: 22.1811 Loss-g-dur: 1.9351 Loss-g-kl: 1.3094 lr: 0.0002 grad_norm_g: 285.4889 grad_norm_d: 122.5126
======> Epoch: 166
Train Epoch: 167 [34.62%] G-Loss: 32.5149 D-Loss: 2.3737 Loss-g-fm: 3.9555 Loss-g-mel: 23.3270 Loss-g-dur: 1.9753 Loss-g-kl: 1.1460 lr: 0.0002 grad_norm_g: 605.6516 grad_norm_d: 103.3915
======> Epoch: 167
Train Epoch: 168 [30.77%] G-Loss: 31.3428 D-Loss: 2.5573 Loss-g-fm: 3.2970 Loss-g-mel: 22.2341 Loss-g-dur: 2.1660 Loss-g-kl: 1.3223 lr: 0.0002 grad_norm_g: 431.3073 grad_norm_d: 103.9714
======> Epoch: 168
Train Epoch: 169 [26.92%] G-Loss: 27.6303 D-Loss: 2.6298 Loss-g-fm: 3.4325 Loss-g-mel: 19.0834 Loss-g-dur: 1.8190 Loss-g-kl: 1.0577 lr: 0.0002 grad_norm_g: 180.8725 grad_norm_d: 49.6701
======> Epoch: 169
Train Epoch: 170 [23.08%] G-Loss: 31.5076 D-Loss: 2.4600 Loss-g-fm: 3.6667 Loss-g-mel: 21.9596 Loss-g-dur: 2.0769 Loss-g-kl: 1.3568 lr: 0.0002 grad_norm_g: 754.0894 grad_norm_d: 107.1484
======> Epoch: 170
Train Epoch: 171 [19.23%] G-Loss: 30.7513 D-Loss: 2.5404 Loss-g-fm: 3.3364 Loss-g-mel: 22.1547 Loss-g-dur: 1.9315 Loss-g-kl: 0.9887 lr: 0.0002 grad_norm_g: 647.9688 grad_norm_d: 172.1677
======> Epoch: 171
Train Epoch: 172 [15.38%] G-Loss: 32.5098 D-Loss: 2.4394 Loss-g-fm: 3.5358 Loss-g-mel: 23.4438 Loss-g-dur: 2.0542 Loss-g-kl: 1.3470 lr: 0.0002 grad_norm_g: 407.9329 grad_norm_d: 56.9480
======> Epoch: 172
Train Epoch: 173 [11.54%] G-Loss: 32.2106 D-Loss: 2.4771 Loss-g-fm: 3.2918 Loss-g-mel: 23.5119 Loss-g-dur: 2.1136 Loss-g-kl: 1.3627 lr: 0.0002 grad_norm_g: 447.6545 grad_norm_d: 46.9207
======> Epoch: 173
Train Epoch: 174 [7.69%] G-Loss: 31.9727 D-Loss: 2.7672 Loss-g-fm: 3.5283 Loss-g-mel: 22.4516 Loss-g-dur: 2.0629 Loss-g-kl: 1.2438 lr: 0.0002 grad_norm_g: 639.1896 grad_norm_d: 152.7481
======> Epoch: 174
Train Epoch: 175 [3.85%] G-Loss: 30.0330 D-Loss: 2.6117 Loss-g-fm: 2.9572 Loss-g-mel: 22.1024 Loss-g-dur: 1.9506 Loss-g-kl: 1.2525 lr: 0.0002 grad_norm_g: 124.7725 grad_norm_d: 23.2914
======> Epoch: 175
Train Epoch: 176 [0.00%] G-Loss: 29.6125 D-Loss: 2.4257 Loss-g-fm: 3.2307 Loss-g-mel: 21.1509 Loss-g-dur: 2.1000 Loss-g-kl: 1.0164 lr: 0.0002 grad_norm_g: 126.6519 grad_norm_d: 23.5220
Train Epoch: 176 [96.15%] G-Loss: 30.1808 D-Loss: 2.4718 Loss-g-fm: 3.3295 Loss-g-mel: 21.4180 Loss-g-dur: 1.9447 Loss-g-kl: 1.2248 lr: 0.0002 grad_norm_g: 569.3569 grad_norm_d: 109.9875
======> Epoch: 176
Train Epoch: 177 [92.31%] G-Loss: 30.8609 D-Loss: 2.4871 Loss-g-fm: 3.4947 Loss-g-mel: 22.0246 Loss-g-dur: 2.0669 Loss-g-kl: 1.1825 lr: 0.0002 grad_norm_g: 384.3640 grad_norm_d: 140.3649
======> Epoch: 177
Train Epoch: 178 [88.46%] G-Loss: 32.1739 D-Loss: 2.3802 Loss-g-fm: 3.9175 Loss-g-mel: 22.5928 Loss-g-dur: 2.0358 Loss-g-kl: 1.0490 lr: 0.0002 grad_norm_g: 736.1466 grad_norm_d: 111.8335
======> Epoch: 178
Train Epoch: 179 [84.62%] G-Loss: 29.8011 D-Loss: 2.5214 Loss-g-fm: 3.4105 Loss-g-mel: 20.7497 Loss-g-dur: 1.9494 Loss-g-kl: 1.3022 lr: 0.0002 grad_norm_g: 645.4381 grad_norm_d: 139.5245
======> Epoch: 179
Train Epoch: 180 [80.77%] G-Loss: 32.1200 D-Loss: 2.5242 Loss-g-fm: 3.7573 Loss-g-mel: 22.7874 Loss-g-dur: 1.9799 Loss-g-kl: 0.9492 lr: 0.0002 grad_norm_g: 781.1046 grad_norm_d: 151.5426
======> Epoch: 180
Train Epoch: 181 [76.92%] G-Loss: 34.0848 D-Loss: 2.4021 Loss-g-fm: 4.1189 Loss-g-mel: 24.1441 Loss-g-dur: 2.0470 Loss-g-kl: 1.2054 lr: 0.0002 grad_norm_g: 679.1337 grad_norm_d: 146.2266
======> Epoch: 181
Train Epoch: 182 [73.08%] G-Loss: 29.2190 D-Loss: 2.8016 Loss-g-fm: 3.1206 Loss-g-mel: 21.0884 Loss-g-dur: 1.9923 Loss-g-kl: 1.0962 lr: 0.0002 grad_norm_g: 150.5128 grad_norm_d: 46.2776
======> Epoch: 182
Train Epoch: 183 [69.23%] G-Loss: 30.3558 D-Loss: 2.6236 Loss-g-fm: 2.9182 Loss-g-mel: 22.1160 Loss-g-dur: 2.0860 Loss-g-kl: 1.0635 lr: 0.0002 grad_norm_g: 202.9932 grad_norm_d: 61.9031
======> Epoch: 183
Train Epoch: 184 [65.38%] G-Loss: 28.6254 D-Loss: 2.4055 Loss-g-fm: 4.1928 Loss-g-mel: 19.2152 Loss-g-dur: 1.6903 Loss-g-kl: 1.1101 lr: 0.0002 grad_norm_g: 367.2210 grad_norm_d: 31.5559
======> Epoch: 184
Train Epoch: 185 [61.54%] G-Loss: 32.7373 D-Loss: 2.3927 Loss-g-fm: 4.0229 Loss-g-mel: 23.3669 Loss-g-dur: 1.9339 Loss-g-kl: 0.9619 lr: 0.0002 grad_norm_g: 334.8761 grad_norm_d: 93.4743
======> Epoch: 185
Train Epoch: 186 [57.69%] G-Loss: 32.9482 D-Loss: 2.5169 Loss-g-fm: 4.0297 Loss-g-mel: 23.3774 Loss-g-dur: 2.0535 Loss-g-kl: 1.1960 lr: 0.0002 grad_norm_g: 467.6813 grad_norm_d: 159.4563
======> Epoch: 186
Train Epoch: 187 [53.85%] G-Loss: 33.3402 D-Loss: 2.4060 Loss-g-fm: 3.8251 Loss-g-mel: 23.9026 Loss-g-dur: 2.1105 Loss-g-kl: 1.2420 lr: 0.0002 grad_norm_g: 409.9459 grad_norm_d: 44.9081
======> Epoch: 187
Train Epoch: 188 [50.00%] G-Loss: 31.6360 D-Loss: 2.4703 Loss-g-fm: 4.2557 Loss-g-mel: 21.5349 Loss-g-dur: 2.0628 Loss-g-kl: 1.1819 lr: 0.0002 grad_norm_g: 595.9525 grad_norm_d: 132.6134
======> Epoch: 188
Train Epoch: 189 [46.15%] G-Loss: 31.8053 D-Loss: 2.3971 Loss-g-fm: 3.5743 Loss-g-mel: 22.8048 Loss-g-dur: 2.0426 Loss-g-kl: 1.1479 lr: 0.0002 grad_norm_g: 399.6722 grad_norm_d: 73.9675
======> Epoch: 189
Train Epoch: 190 [42.31%] G-Loss: 30.7775 D-Loss: 2.5830 Loss-g-fm: 3.7814 Loss-g-mel: 21.5570 Loss-g-dur: 1.9981 Loss-g-kl: 1.0696 lr: 0.0002 grad_norm_g: 471.6873 grad_norm_d: 113.2515
======> Epoch: 190
Train Epoch: 191 [38.46%] G-Loss: 31.9926 D-Loss: 2.6134 Loss-g-fm: 3.5077 Loss-g-mel: 23.3185 Loss-g-dur: 1.9381 Loss-g-kl: 0.9541 lr: 0.0002 grad_norm_g: 522.5567 grad_norm_d: 105.4479
======> Epoch: 191
Train Epoch: 192 [34.62%] G-Loss: 29.2673 D-Loss: 2.5789 Loss-g-fm: 3.0134 Loss-g-mel: 21.0891 Loss-g-dur: 2.0181 Loss-g-kl: 1.1598 lr: 0.0002 grad_norm_g: 125.7843 grad_norm_d: 9.7745
======> Epoch: 192
Train Epoch: 193 [30.77%] G-Loss: 30.4609 D-Loss: 2.5299 Loss-g-fm: 3.3022 Loss-g-mel: 22.0414 Loss-g-dur: 1.9603 Loss-g-kl: 1.0589 lr: 0.0002 grad_norm_g: 452.0691 grad_norm_d: 142.8495
======> Epoch: 193
Train Epoch: 194 [26.92%] G-Loss: 29.6094 D-Loss: 2.5065 Loss-g-fm: 3.2246 Loss-g-mel: 20.8064 Loss-g-dur: 1.9575 Loss-g-kl: 1.2575 lr: 0.0002 grad_norm_g: 557.0618 grad_norm_d: 109.9909
======> Epoch: 194
Train Epoch: 195 [23.08%] G-Loss: 31.9683 D-Loss: 2.4738 Loss-g-fm: 4.0893 Loss-g-mel: 22.0953 Loss-g-dur: 2.0139 Loss-g-kl: 1.4029 lr: 0.0002 grad_norm_g: 442.4398 grad_norm_d: 147.5653
======> Epoch: 195
Train Epoch: 196 [19.23%] G-Loss: 30.2963 D-Loss: 2.5749 Loss-g-fm: 3.4068 Loss-g-mel: 21.3673 Loss-g-dur: 1.9511 Loss-g-kl: 1.2759 lr: 0.0002 grad_norm_g: 593.3129 grad_norm_d: 195.9687
======> Epoch: 196
Train Epoch: 197 [15.38%] G-Loss: 31.9826 D-Loss: 2.5126 Loss-g-fm: 3.9265 Loss-g-mel: 22.3270 Loss-g-dur: 1.9372 Loss-g-kl: 1.1870 lr: 0.0002 grad_norm_g: 582.6166 grad_norm_d: 174.4720
======> Epoch: 197
Train Epoch: 198 [11.54%] G-Loss: 30.8944 D-Loss: 2.6169 Loss-g-fm: 3.4672 Loss-g-mel: 21.7399 Loss-g-dur: 2.0533 Loss-g-kl: 1.3405 lr: 0.0002 grad_norm_g: 531.7031 grad_norm_d: 142.1833
======> Epoch: 198
Train Epoch: 199 [7.69%] G-Loss: 31.6343 D-Loss: 2.3820 Loss-g-fm: 3.8707 Loss-g-mel: 22.1848 Loss-g-dur: 2.0789 Loss-g-kl: 1.3241 lr: 0.0002 grad_norm_g: 578.6466 grad_norm_d: 117.6599
======> Epoch: 199
Train Epoch: 200 [3.85%] G-Loss: 28.3326 D-Loss: 2.4452 Loss-g-fm: 4.2600 Loss-g-mel: 19.0908 Loss-g-dur: 1.5522 Loss-g-kl: 1.0728 lr: 0.0002 grad_norm_g: 442.8368 grad_norm_d: 34.2968
======> Epoch: 200
Train Epoch: 201 [0.00%] G-Loss: 30.7815 D-Loss: 2.5508 Loss-g-fm: 3.2558 Loss-g-mel: 22.4621 Loss-g-dur: 2.0059 Loss-g-kl: 1.0261 lr: 0.0002 grad_norm_g: 453.9991 grad_norm_d: 74.4814
Train Epoch: 201 [96.15%] G-Loss: 32.8888 D-Loss: 2.5688 Loss-g-fm: 3.6751 Loss-g-mel: 23.2266 Loss-g-dur: 1.9619 Loss-g-kl: 1.2168 lr: 0.0002 grad_norm_g: 579.1135 grad_norm_d: 129.8015
======> Epoch: 201
Train Epoch: 202 [92.31%] G-Loss: 31.7007 D-Loss: 2.5104 Loss-g-fm: 3.8536 Loss-g-mel: 22.3849 Loss-g-dur: 2.0318 Loss-g-kl: 1.1992 lr: 0.0002 grad_norm_g: 547.7605 grad_norm_d: 84.5413
======> Epoch: 202
Train Epoch: 203 [88.46%] G-Loss: 31.0569 D-Loss: 2.5945 Loss-g-fm: 3.3733 Loss-g-mel: 22.1999 Loss-g-dur: 2.0419 Loss-g-kl: 1.1166 lr: 0.0002 grad_norm_g: 339.6366 grad_norm_d: 182.8260
======> Epoch: 203
Train Epoch: 204 [84.62%] G-Loss: 30.7162 D-Loss: 2.6034 Loss-g-fm: 3.6759 Loss-g-mel: 21.3112 Loss-g-dur: 2.0325 Loss-g-kl: 1.1712 lr: 0.0002 grad_norm_g: 519.0127 grad_norm_d: 108.9481
======> Epoch: 204
Train Epoch: 205 [80.77%] G-Loss: 28.6565 D-Loss: 2.2020 Loss-g-fm: 4.4677 Loss-g-mel: 19.4580 Loss-g-dur: 1.5723 Loss-g-kl: 1.1128 lr: 0.0002 grad_norm_g: 193.3304 grad_norm_d: 42.2283
======> Epoch: 205
Train Epoch: 206 [76.92%] G-Loss: 30.6037 D-Loss: 2.4700 Loss-g-fm: 3.6901 Loss-g-mel: 21.8779 Loss-g-dur: 1.9936 Loss-g-kl: 1.1366 lr: 0.0002 grad_norm_g: 419.4184 grad_norm_d: 69.6767
======> Epoch: 206
Train Epoch: 207 [73.08%] G-Loss: 31.6679 D-Loss: 2.4136 Loss-g-fm: 3.6492 Loss-g-mel: 22.5582 Loss-g-dur: 1.9305 Loss-g-kl: 0.9579 lr: 0.0002 grad_norm_g: 524.3977 grad_norm_d: 99.0146
======> Epoch: 207
Train Epoch: 208 [69.23%] G-Loss: 31.0446 D-Loss: 2.6244 Loss-g-fm: 3.4415 Loss-g-mel: 21.9799 Loss-g-dur: 1.9816 Loss-g-kl: 1.1548 lr: 0.0002 grad_norm_g: 122.0660 grad_norm_d: 93.2494
======> Epoch: 208
Traceback (most recent call last):
  File "/mnt/disk3/huangyao/tts/vits/train_ms.py", line 217, in <module>
    main()        
  File "/mnt/disk3/huangyao/tts/vits/train_ms.py", line 34, in main
    mp.spawn(run, nprocs = n_gpus, args = (n_gpus, hps,))
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with signal SIGSEGV
ERROR: Unexpected segmentation fault encountered in worker.
 /home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 25 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '

Command 'nophup' not found, did you mean:

  command 'nohup' from deb coreutils (8.30-3ubuntu2)

Try: sudo apt install <deb name>

nohup: ignoring input
  File "/mnt/disk3/huangyao/tts/vits/train_ms.log", line 1
    nohup: ignoring input
                    ^
SyntaxError: invalid syntax
nohup: ignoring input
nohup: ignoring input
rank =  1
**************************************** /ZFS4T/tts/data/VITS/model_saved/G_4000.pth ****************************************
rank =  0
**************************************** /ZFS4T/tts/data/VITS/model_saved/G_4000.pth ****************************************
/mnt/disk3/huangyao/tts/vits/mel_processing.py:57: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/mnt/disk3/huangyao/tts/vits/mel_processing.py:75: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.
Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/mnt/disk3/huangyao/tts/vits/mel_processing.py:57: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/mnt/disk3/huangyao/tts/vits/mel_processing.py:75: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.
Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 9, 96], strides() = [16992, 96, 1]
bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 9, 96], strides() = [16992, 96, 1]
bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Train Epoch: 1 [0.00%] G-Loss: 31.6168 D-Loss: 6.0588 Loss-g-fm: 0.1344 Loss-g-mel: 22.3524 Loss-g-dur: 1.9418 Loss-g-kl: 1.1300 lr: 0.0002 grad_norm_g: nan grad_norm_d: inf
terminate called without an active exception
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f44c239b820>
Traceback (most recent call last):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 12860) is killed by signal: Aborted. 
Saving model and optimizer state at iteration 1 to /ZFS4T/tts/data/VITS/model_saved/G_0.pth
Saving model and optimizer state at iteration 1 to /ZFS4T/tts/data/VITS/model_saved/D_0.pth
Train Epoch: 1 [96.15%] G-Loss: 26.1851 D-Loss: 3.0489 Loss-g-fm: 0.1543 Loss-g-mel: 21.5265 Loss-g-dur: 1.9868 Loss-g-kl: 0.8998 lr: 0.0002 grad_norm_g: 27.6799 grad_norm_d: 0.5740
======> Epoch: 1
Train Epoch: 2 [92.31%] G-Loss: 25.8716 D-Loss: 2.9925 Loss-g-fm: 0.2469 Loss-g-mel: 20.9488 Loss-g-dur: 2.0442 Loss-g-kl: 1.0496 lr: 0.0002 grad_norm_g: 22.6288 grad_norm_d: 2.6321
======> Epoch: 2
Train Epoch: 3 [88.46%] G-Loss: 23.7098 D-Loss: 2.9770 Loss-g-fm: 0.5006 Loss-g-mel: 18.4361 Loss-g-dur: 1.8124 Loss-g-kl: 1.0075 lr: 0.0002 grad_norm_g: 79.0740 grad_norm_d: 3.5361
======> Epoch: 3
Train Epoch: 4 [84.62%] G-Loss: 27.9086 D-Loss: 2.9703 Loss-g-fm: 0.4744 Loss-g-mel: 22.6839 Loss-g-dur: 2.0783 Loss-g-kl: 1.1653 lr: 0.0002 grad_norm_g: 36.1407 grad_norm_d: 1.9146
======> Epoch: 4
Train Epoch: 5 [80.77%] G-Loss: 24.9217 D-Loss: 2.9873 Loss-g-fm: 0.2649 Loss-g-mel: 20.0218 Loss-g-dur: 1.9306 Loss-g-kl: 1.1287 lr: 0.0002 grad_norm_g: 39.4556 grad_norm_d: 2.6571
======> Epoch: 5
Train Epoch: 6 [76.92%] G-Loss: 26.3543 D-Loss: 3.0902 Loss-g-fm: 0.2724 Loss-g-mel: 21.6638 Loss-g-dur: 2.0005 Loss-g-kl: 1.1499 lr: 0.0002 grad_norm_g: 19.7695 grad_norm_d: 3.3164
======> Epoch: 6
Train Epoch: 7 [73.08%] G-Loss: 26.3452 D-Loss: 2.9672 Loss-g-fm: 0.3935 Loss-g-mel: 21.6111 Loss-g-dur: 2.0096 Loss-g-kl: 1.1460 lr: 0.0002 grad_norm_g: 21.2563 grad_norm_d: 1.5862
======> Epoch: 7
Train Epoch: 8 [69.23%] G-Loss: 26.4004 D-Loss: 2.9710 Loss-g-fm: 0.3165 Loss-g-mel: 21.2877 Loss-g-dur: 2.0269 Loss-g-kl: 1.4066 lr: 0.0002 grad_norm_g: 31.0211 grad_norm_d: 1.6497
======> Epoch: 8
Train Epoch: 9 [65.38%] G-Loss: 28.2418 D-Loss: 2.9536 Loss-g-fm: 0.4220 Loss-g-mel: 22.7843 Loss-g-dur: 2.1381 Loss-g-kl: 1.2794 lr: 0.0002 grad_norm_g: 36.5112 grad_norm_d: 1.2113
======> Epoch: 9
Train Epoch: 10 [61.54%] G-Loss: 26.7228 D-Loss: 2.9770 Loss-g-fm: 0.3722 Loss-g-mel: 21.7644 Loss-g-dur: 1.9943 Loss-g-kl: 1.0530 lr: 0.0002 grad_norm_g: 27.1450 grad_norm_d: 2.0480
======> Epoch: 10
Train Epoch: 11 [57.69%] G-Loss: 27.4969 D-Loss: 2.9871 Loss-g-fm: 0.4101 Loss-g-mel: 22.2959 Loss-g-dur: 2.0833 Loss-g-kl: 1.1759 lr: 0.0002 grad_norm_g: 19.6464 grad_norm_d: 4.0940
======> Epoch: 11
Train Epoch: 12 [53.85%] G-Loss: 27.4637 D-Loss: 2.9441 Loss-g-fm: 0.4398 Loss-g-mel: 22.2995 Loss-g-dur: 2.0421 Loss-g-kl: 1.1837 lr: 0.0002 grad_norm_g: 43.0623 grad_norm_d: 1.3384
======> Epoch: 12
Train Epoch: 13 [50.00%] G-Loss: 23.2265 D-Loss: 2.9355 Loss-g-fm: 0.7254 Loss-g-mel: 17.9561 Loss-g-dur: 1.7119 Loss-g-kl: 1.1406 lr: 0.0002 grad_norm_g: 27.8964 grad_norm_d: 2.0976
======> Epoch: 13
Train Epoch: 14 [46.15%] G-Loss: 26.6275 D-Loss: 2.9326 Loss-g-fm: 0.6045 Loss-g-mel: 21.3483 Loss-g-dur: 2.0541 Loss-g-kl: 1.0926 lr: 0.0002 grad_norm_g: 39.7941 grad_norm_d: 2.3351
======> Epoch: 14
Train Epoch: 15 [42.31%] G-Loss: 24.8624 D-Loss: 2.9346 Loss-g-fm: 0.5072 Loss-g-mel: 19.7402 Loss-g-dur: 2.0512 Loss-g-kl: 1.0740 lr: 0.0002 grad_norm_g: 31.6044 grad_norm_d: 1.0447
======> Epoch: 15
Train Epoch: 16 [38.46%] G-Loss: 27.1905 D-Loss: 2.9436 Loss-g-fm: 0.4127 Loss-g-mel: 22.0400 Loss-g-dur: 2.0019 Loss-g-kl: 1.0468 lr: 0.0002 grad_norm_g: 23.8204 grad_norm_d: 1.5031
======> Epoch: 16
Train Epoch: 17 [34.62%] G-Loss: 26.1691 D-Loss: 2.9713 Loss-g-fm: 0.4554 Loss-g-mel: 20.8733 Loss-g-dur: 2.1208 Loss-g-kl: 1.3490 lr: 0.0002 grad_norm_g: 60.5988 grad_norm_d: 1.7368
======> Epoch: 17
Train Epoch: 18 [30.77%] G-Loss: 27.1943 D-Loss: 2.9603 Loss-g-fm: 0.5603 Loss-g-mel: 21.7237 Loss-g-dur: 2.0302 Loss-g-kl: 1.3778 lr: 0.0002 grad_norm_g: 20.8329 grad_norm_d: 0.6616
======> Epoch: 18
Train Epoch: 19 [26.92%] G-Loss: 26.4126 D-Loss: 2.9717 Loss-g-fm: 0.4583 Loss-g-mel: 21.0401 Loss-g-dur: 2.0623 Loss-g-kl: 1.2567 lr: 0.0002 grad_norm_g: 27.9521 grad_norm_d: 1.1397
======> Epoch: 19
Train Epoch: 20 [23.08%] G-Loss: 26.0295 D-Loss: 2.9709 Loss-g-fm: 0.4175 Loss-g-mel: 20.8761 Loss-g-dur: 1.9433 Loss-g-kl: 1.2179 lr: 0.0002 grad_norm_g: 37.3980 grad_norm_d: 1.0824
======> Epoch: 20
Train Epoch: 21 [19.23%] G-Loss: 25.0458 D-Loss: 2.9688 Loss-g-fm: 0.4516 Loss-g-mel: 19.9940 Loss-g-dur: 1.9620 Loss-g-kl: 1.1303 lr: 0.0002 grad_norm_g: 22.6162 grad_norm_d: 1.2690
======> Epoch: 21
Train Epoch: 22 [15.38%] G-Loss: 24.2409 D-Loss: 2.9536 Loss-g-fm: 0.4742 Loss-g-mel: 18.9646 Loss-g-dur: 2.0514 Loss-g-kl: 0.9983 lr: 0.0002 grad_norm_g: 32.7586 grad_norm_d: 1.4773
======> Epoch: 22
Train Epoch: 23 [11.54%] G-Loss: 26.2335 D-Loss: 2.9360 Loss-g-fm: 0.4507 Loss-g-mel: 21.3163 Loss-g-dur: 1.9278 Loss-g-kl: 0.9886 lr: 0.0002 grad_norm_g: 30.8195 grad_norm_d: 1.4442
======> Epoch: 23
Train Epoch: 24 [7.69%] G-Loss: 27.6469 D-Loss: 2.9097 Loss-g-fm: 0.6802 Loss-g-mel: 22.1567 Loss-g-dur: 2.0343 Loss-g-kl: 1.2506 lr: 0.0002 grad_norm_g: 43.3214 grad_norm_d: 1.9436
======> Epoch: 24
Train Epoch: 25 [3.85%] G-Loss: 26.2491 D-Loss: 2.9534 Loss-g-fm: 0.5145 Loss-g-mel: 20.8950 Loss-g-dur: 2.0578 Loss-g-kl: 1.2747 lr: 0.0002 grad_norm_g: 28.8506 grad_norm_d: 1.6061
======> Epoch: 25
Train Epoch: 26 [0.00%] G-Loss: 26.7567 D-Loss: 2.9338 Loss-g-fm: 0.6390 Loss-g-mel: 21.2567 Loss-g-dur: 2.0174 Loss-g-kl: 1.3711 lr: 0.0002 grad_norm_g: 28.3725 grad_norm_d: 1.9635
Train Epoch: 26 [96.15%] G-Loss: 26.5712 D-Loss: 2.9142 Loss-g-fm: 0.6678 Loss-g-mel: 21.0710 Loss-g-dur: 2.0744 Loss-g-kl: 1.3909 lr: 0.0002 grad_norm_g: 48.0243 grad_norm_d: 3.2964
======> Epoch: 26
Train Epoch: 27 [92.31%] G-Loss: 23.2302 D-Loss: 2.9598 Loss-g-fm: 0.7588 Loss-g-mel: 17.9624 Loss-g-dur: 1.6758 Loss-g-kl: 1.0951 lr: 0.0002 grad_norm_g: 53.5051 grad_norm_d: 2.4254
======> Epoch: 27
Train Epoch: 28 [88.46%] G-Loss: 27.4209 D-Loss: 2.9442 Loss-g-fm: 0.7020 Loss-g-mel: 21.6008 Loss-g-dur: 2.1728 Loss-g-kl: 1.4344 lr: 0.0002 grad_norm_g: 37.2910 grad_norm_d: 2.5141
======> Epoch: 28
Train Epoch: 29 [84.62%] G-Loss: 26.7164 D-Loss: 2.9642 Loss-g-fm: 0.5665 Loss-g-mel: 21.5478 Loss-g-dur: 1.9908 Loss-g-kl: 1.1607 lr: 0.0002 grad_norm_g: 16.7503 grad_norm_d: 2.4317
======> Epoch: 29
Train Epoch: 30 [80.77%] G-Loss: 25.6587 D-Loss: 2.9432 Loss-g-fm: 0.5249 Loss-g-mel: 20.6158 Loss-g-dur: 1.9537 Loss-g-kl: 1.1249 lr: 0.0002 grad_norm_g: 23.9678 grad_norm_d: 2.5705
======> Epoch: 30
Train Epoch: 31 [76.92%] G-Loss: 27.3267 D-Loss: 2.9281 Loss-g-fm: 0.5799 Loss-g-mel: 21.7660 Loss-g-dur: 1.9976 Loss-g-kl: 1.1681 lr: 0.0002 grad_norm_g: 36.9270 grad_norm_d: 2.6475
======> Epoch: 31
Train Epoch: 32 [73.08%] G-Loss: 25.3357 D-Loss: 2.9399 Loss-g-fm: 0.4956 Loss-g-mel: 20.1030 Loss-g-dur: 2.0096 Loss-g-kl: 1.1287 lr: 0.0002 grad_norm_g: 26.1666 grad_norm_d: 2.5773
======> Epoch: 32
Train Epoch: 33 [69.23%] G-Loss: 26.0791 D-Loss: 2.9104 Loss-g-fm: 0.6146 Loss-g-mel: 20.8888 Loss-g-dur: 1.9762 Loss-g-kl: 1.0078 lr: 0.0002 grad_norm_g: 25.4543 grad_norm_d: 1.8425
======> Epoch: 33
Train Epoch: 34 [65.38%] G-Loss: 25.3991 D-Loss: 2.9534 Loss-g-fm: 0.4575 Loss-g-mel: 20.2434 Loss-g-dur: 1.9156 Loss-g-kl: 1.1164 lr: 0.0002 grad_norm_g: 23.1697 grad_norm_d: 1.7865
======> Epoch: 34
Train Epoch: 35 [61.54%] G-Loss: 25.0903 D-Loss: 2.9583 Loss-g-fm: 0.5270 Loss-g-mel: 20.0540 Loss-g-dur: 1.9732 Loss-g-kl: 0.9955 lr: 0.0002 grad_norm_g: 21.2448 grad_norm_d: 1.3251
======> Epoch: 35
Train Epoch: 36 [57.69%] G-Loss: 26.0190 D-Loss: 2.9655 Loss-g-fm: 0.4581 Loss-g-mel: 20.5154 Loss-g-dur: 1.9473 Loss-g-kl: 1.1983 lr: 0.0002 grad_norm_g: 26.1351 grad_norm_d: 5.4163
======> Epoch: 36
Train Epoch: 37 [53.85%] G-Loss: 26.0514 D-Loss: 2.9636 Loss-g-fm: 0.4728 Loss-g-mel: 21.0049 Loss-g-dur: 1.9643 Loss-g-kl: 1.1378 lr: 0.0002 grad_norm_g: 37.4548 grad_norm_d: 1.5455
======> Epoch: 37
Train Epoch: 38 [50.00%] G-Loss: 27.3399 D-Loss: 2.9124 Loss-g-fm: 0.7786 Loss-g-mel: 21.6785 Loss-g-dur: 2.0164 Loss-g-kl: 1.2606 lr: 0.0002 grad_norm_g: 20.8224 grad_norm_d: 3.2248
======> Epoch: 38
Train Epoch: 39 [46.15%] G-Loss: 27.4347 D-Loss: 2.9073 Loss-g-fm: 0.7204 Loss-g-mel: 21.7346 Loss-g-dur: 2.0746 Loss-g-kl: 1.2761 lr: 0.0002 grad_norm_g: 55.7797 grad_norm_d: 2.1981
======> Epoch: 39
Train Epoch: 40 [42.31%] G-Loss: 27.2279 D-Loss: 2.8994 Loss-g-fm: 0.7093 Loss-g-mel: 21.3755 Loss-g-dur: 2.0428 Loss-g-kl: 1.4831 lr: 0.0002 grad_norm_g: 19.9343 grad_norm_d: 2.5941
======> Epoch: 40
Train Epoch: 41 [38.46%] G-Loss: 27.1262 D-Loss: 2.9694 Loss-g-fm: 0.6438 Loss-g-mel: 21.6052 Loss-g-dur: 2.0580 Loss-g-kl: 1.4905 lr: 0.0002 grad_norm_g: 27.1762 grad_norm_d: 5.1160
======> Epoch: 41
Train Epoch: 42 [34.62%] G-Loss: 27.3726 D-Loss: 2.9079 Loss-g-fm: 0.7187 Loss-g-mel: 21.9461 Loss-g-dur: 1.9912 Loss-g-kl: 1.1395 lr: 0.0002 grad_norm_g: 44.6173 grad_norm_d: 1.8760
======> Epoch: 42
Train Epoch: 43 [30.77%] G-Loss: 26.0113 D-Loss: 2.9648 Loss-g-fm: 0.5864 Loss-g-mel: 20.6653 Loss-g-dur: 1.9329 Loss-g-kl: 1.1418 lr: 0.0002 grad_norm_g: 20.0353 grad_norm_d: 3.4895
======> Epoch: 43
Train Epoch: 44 [26.92%] G-Loss: 27.7491 D-Loss: 2.8867 Loss-g-fm: 0.8482 Loss-g-mel: 21.6902 Loss-g-dur: 2.0986 Loss-g-kl: 1.4284 lr: 0.0002 grad_norm_g: 36.8934 grad_norm_d: 4.5864
======> Epoch: 44
Train Epoch: 45 [23.08%] G-Loss: 25.5539 D-Loss: 2.9429 Loss-g-fm: 0.6680 Loss-g-mel: 20.2336 Loss-g-dur: 1.9184 Loss-g-kl: 1.1408 lr: 0.0002 grad_norm_g: 19.4039 grad_norm_d: 3.9227
======> Epoch: 45
Train Epoch: 46 [19.23%] G-Loss: 27.2510 D-Loss: 2.9352 Loss-g-fm: 0.7422 Loss-g-mel: 21.4722 Loss-g-dur: 2.0769 Loss-g-kl: 1.3309 lr: 0.0002 grad_norm_g: 22.8248 grad_norm_d: 2.0699
======> Epoch: 46
Train Epoch: 47 [15.38%] G-Loss: 26.7143 D-Loss: 2.9342 Loss-g-fm: 0.6754 Loss-g-mel: 21.2915 Loss-g-dur: 1.9586 Loss-g-kl: 1.1677 lr: 0.0002 grad_norm_g: 31.7827 grad_norm_d: 6.1229
======> Epoch: 47
Train Epoch: 48 [11.54%] G-Loss: 23.0592 D-Loss: 2.9011 Loss-g-fm: 1.0141 Loss-g-mel: 17.3298 Loss-g-dur: 1.6994 Loss-g-kl: 1.2075 lr: 0.0002 grad_norm_g: 41.7243 grad_norm_d: 9.4423
======> Epoch: 48
Train Epoch: 49 [7.69%] G-Loss: 26.5354 D-Loss: 2.9072 Loss-g-fm: 0.7753 Loss-g-mel: 21.0284 Loss-g-dur: 1.9399 Loss-g-kl: 1.0645 lr: 0.0002 grad_norm_g: 19.9206 grad_norm_d: 5.6866
======> Epoch: 49
Train Epoch: 50 [3.85%] G-Loss: 26.3366 D-Loss: 2.9282 Loss-g-fm: 0.7488 Loss-g-mel: 21.1091 Loss-g-dur: 1.9425 Loss-g-kl: 1.0231 lr: 0.0002 grad_norm_g: 20.2886 grad_norm_d: 2.5034
======> Epoch: 50
Train Epoch: 51 [0.00%] G-Loss: 25.7758 D-Loss: 2.9294 Loss-g-fm: 0.6967 Loss-g-mel: 20.1648 Loss-g-dur: 1.9734 Loss-g-kl: 1.2715 lr: 0.0002 grad_norm_g: 24.4520 grad_norm_d: 2.2764
Train Epoch: 51 [96.15%] G-Loss: 26.8812 D-Loss: 2.9900 Loss-g-fm: 0.8806 Loss-g-mel: 21.0078 Loss-g-dur: 2.0303 Loss-g-kl: 1.2506 lr: 0.0002 grad_norm_g: 52.1981 grad_norm_d: 5.2166
======> Epoch: 51
Train Epoch: 52 [92.31%] G-Loss: 28.2098 D-Loss: 2.8879 Loss-g-fm: 0.8701 Loss-g-mel: 22.5063 Loss-g-dur: 2.0900 Loss-g-kl: 1.3113 lr: 0.0002 grad_norm_g: 22.5283 grad_norm_d: 7.5971
======> Epoch: 52
Train Epoch: 53 [88.46%] G-Loss: 26.2256 D-Loss: 2.8436 Loss-g-fm: 0.8674 Loss-g-mel: 20.7145 Loss-g-dur: 1.9328 Loss-g-kl: 1.1098 lr: 0.0002 grad_norm_g: 17.5447 grad_norm_d: 3.7605
======> Epoch: 53
Train Epoch: 54 [84.62%] G-Loss: 27.0523 D-Loss: 2.9773 Loss-g-fm: 0.8685 Loss-g-mel: 21.4118 Loss-g-dur: 1.9297 Loss-g-kl: 1.2433 lr: 0.0002 grad_norm_g: 25.7926 grad_norm_d: 23.5437
======> Epoch: 54
Train Epoch: 55 [80.77%] G-Loss: 24.9205 D-Loss: 2.8915 Loss-g-fm: 0.6360 Loss-g-mel: 19.5920 Loss-g-dur: 1.8891 Loss-g-kl: 1.2133 lr: 0.0002 grad_norm_g: 29.3453 grad_norm_d: 3.0416
======> Epoch: 55
Train Epoch: 56 [76.92%] G-Loss: 25.8992 D-Loss: 2.8540 Loss-g-fm: 0.9089 Loss-g-mel: 20.2338 Loss-g-dur: 2.0004 Loss-g-kl: 1.1969 lr: 0.0002 grad_norm_g: 28.4807 grad_norm_d: 2.3888
======> Epoch: 56
Train Epoch: 57 [73.08%] G-Loss: 25.8511 D-Loss: 2.8951 Loss-g-fm: 0.7866 Loss-g-mel: 20.2713 Loss-g-dur: 1.9685 Loss-g-kl: 1.1611 lr: 0.0002 grad_norm_g: 24.2581 grad_norm_d: 2.7297
======> Epoch: 57
Train Epoch: 58 [69.23%] G-Loss: 27.0862 D-Loss: 2.8756 Loss-g-fm: 0.8611 Loss-g-mel: 21.4487 Loss-g-dur: 1.9465 Loss-g-kl: 1.1776 lr: 0.0002 grad_norm_g: 17.5110 grad_norm_d: 3.2723
======> Epoch: 58
Train Epoch: 59 [65.38%] G-Loss: 26.3112 D-Loss: 2.8830 Loss-g-fm: 0.8534 Loss-g-mel: 20.5373 Loss-g-dur: 2.0232 Loss-g-kl: 1.2334 lr: 0.0002 grad_norm_g: 42.2376 grad_norm_d: 9.2200
======> Epoch: 59
Train Epoch: 60 [61.54%] G-Loss: 26.9631 D-Loss: 2.8994 Loss-g-fm: 0.8808 Loss-g-mel: 21.3034 Loss-g-dur: 2.0431 Loss-g-kl: 1.0975 lr: 0.0002 grad_norm_g: 19.9073 grad_norm_d: 4.6041
======> Epoch: 60
Train Epoch: 61 [57.69%] G-Loss: 22.4893 D-Loss: 2.8849 Loss-g-fm: 1.2175 Loss-g-mel: 16.7093 Loss-g-dur: 1.5912 Loss-g-kl: 1.1449 lr: 0.0002 grad_norm_g: 57.5408 grad_norm_d: 7.4355
======> Epoch: 61
Train Epoch: 62 [53.85%] G-Loss: 22.6566 D-Loss: 2.7999 Loss-g-fm: 1.2673 Loss-g-mel: 16.7453 Loss-g-dur: 1.5841 Loss-g-kl: 1.1784 lr: 0.0002 grad_norm_g: 50.4632 grad_norm_d: 7.0068
======> Epoch: 62
Train Epoch: 63 [50.00%] G-Loss: 26.4548 D-Loss: 2.8551 Loss-g-fm: 1.0333 Loss-g-mel: 20.1938 Loss-g-dur: 2.0959 Loss-g-kl: 1.4562 lr: 0.0002 grad_norm_g: 25.4195 grad_norm_d: 4.8744
======> Epoch: 63
Train Epoch: 64 [46.15%] G-Loss: 26.2708 D-Loss: 2.8877 Loss-g-fm: 0.9265 Loss-g-mel: 20.4257 Loss-g-dur: 2.0092 Loss-g-kl: 1.3019 lr: 0.0002 grad_norm_g: 20.1976 grad_norm_d: 1.7363
======> Epoch: 64
Train Epoch: 65 [42.31%] G-Loss: 25.7796 D-Loss: 2.8394 Loss-g-fm: 0.8250 Loss-g-mel: 20.1002 Loss-g-dur: 1.9325 Loss-g-kl: 1.2746 lr: 0.0002 grad_norm_g: 19.1986 grad_norm_d: 5.1807
======> Epoch: 65
Train Epoch: 66 [38.46%] G-Loss: 24.5583 D-Loss: 2.8736 Loss-g-fm: 0.7531 Loss-g-mel: 19.1940 Loss-g-dur: 1.9322 Loss-g-kl: 1.0310 lr: 0.0002 grad_norm_g: 26.4019 grad_norm_d: 3.8593
======> Epoch: 66
Train Epoch: 67 [34.62%] G-Loss: 26.9637 D-Loss: 2.8413 Loss-g-fm: 0.9396 Loss-g-mel: 21.4583 Loss-g-dur: 1.9078 Loss-g-kl: 1.0085 lr: 0.0002 grad_norm_g: 22.4890 grad_norm_d: 6.3870
======> Epoch: 67
Train Epoch: 68 [30.77%] G-Loss: 26.4502 D-Loss: 2.8847 Loss-g-fm: 0.9278 Loss-g-mel: 20.5981 Loss-g-dur: 1.9241 Loss-g-kl: 1.1940 lr: 0.0002 grad_norm_g: 19.7485 grad_norm_d: 2.5717
======> Epoch: 68
Train Epoch: 69 [26.92%] G-Loss: 25.6669 D-Loss: 2.9197 Loss-g-fm: 0.9203 Loss-g-mel: 19.8872 Loss-g-dur: 1.9264 Loss-g-kl: 1.2790 lr: 0.0002 grad_norm_g: 24.7184 grad_norm_d: 4.0508
======> Epoch: 69
Train Epoch: 70 [23.08%] G-Loss: 24.9057 D-Loss: 2.8374 Loss-g-fm: 0.9063 Loss-g-mel: 19.3406 Loss-g-dur: 1.9034 Loss-g-kl: 1.1036 lr: 0.0002 grad_norm_g: 45.4658 grad_norm_d: 4.6312
======> Epoch: 70
Train Epoch: 71 [19.23%] G-Loss: 22.6988 D-Loss: 2.8278 Loss-g-fm: 1.4299 Loss-g-mel: 16.5468 Loss-g-dur: 1.5737 Loss-g-kl: 1.2052 lr: 0.0002 grad_norm_g: 67.6548 grad_norm_d: 6.6242
======> Epoch: 71
Train Epoch: 72 [15.38%] G-Loss: 26.2693 D-Loss: 2.8328 Loss-g-fm: 0.9833 Loss-g-mel: 20.7750 Loss-g-dur: 1.9629 Loss-g-kl: 1.0169 lr: 0.0002 grad_norm_g: 23.4540 grad_norm_d: 5.5929
======> Epoch: 72
Train Epoch: 73 [11.54%] G-Loss: 23.4686 D-Loss: 2.8320 Loss-g-fm: 0.8423 Loss-g-mel: 17.9537 Loss-g-dur: 1.8916 Loss-g-kl: 1.1075 lr: 0.0002 grad_norm_g: 23.1404 grad_norm_d: 3.2895
======> Epoch: 73
Train Epoch: 74 [7.69%] G-Loss: 25.5872 D-Loss: 2.9224 Loss-g-fm: 0.8177 Loss-g-mel: 19.8842 Loss-g-dur: 1.9274 Loss-g-kl: 1.1099 lr: 0.0002 grad_norm_g: 25.2076 grad_norm_d: 17.1822
======> Epoch: 74
Train Epoch: 75 [3.85%] G-Loss: 25.4035 D-Loss: 2.8566 Loss-g-fm: 0.8686 Loss-g-mel: 19.7950 Loss-g-dur: 1.8803 Loss-g-kl: 1.1336 lr: 0.0002 grad_norm_g: 26.9483 grad_norm_d: 6.1244
======> Epoch: 75
Train Epoch: 76 [0.00%] G-Loss: 27.1980 D-Loss: 2.7776 Loss-g-fm: 1.1077 Loss-g-mel: 21.3148 Loss-g-dur: 1.9424 Loss-g-kl: 1.1590 lr: 0.0002 grad_norm_g: 18.6299 grad_norm_d: 4.8991
Train Epoch: 76 [96.15%] G-Loss: 25.4223 D-Loss: 2.8479 Loss-g-fm: 0.8828 Loss-g-mel: 19.7720 Loss-g-dur: 1.8896 Loss-g-kl: 1.1549 lr: 0.0002 grad_norm_g: 29.3894 grad_norm_d: 12.5891
======> Epoch: 76
Train Epoch: 77 [92.31%] G-Loss: 25.2814 D-Loss: 2.8883 Loss-g-fm: 1.0339 Loss-g-mel: 19.5788 Loss-g-dur: 1.8952 Loss-g-kl: 1.2495 lr: 0.0002 grad_norm_g: 33.2522 grad_norm_d: 6.2409
Saving model and optimizer state at iteration 77 to /ZFS4T/tts/data/VITS/model_saved/G_2000.pth
Saving model and optimizer state at iteration 77 to /ZFS4T/tts/data/VITS/model_saved/D_2000.pth
======> Epoch: 77
Train Epoch: 78 [88.46%] G-Loss: 25.1987 D-Loss: 2.9688 Loss-g-fm: 0.8836 Loss-g-mel: 19.4633 Loss-g-dur: 1.9054 Loss-g-kl: 1.2873 lr: 0.0002 grad_norm_g: 19.4615 grad_norm_d: 14.2956
======> Epoch: 78
Train Epoch: 79 [84.62%] G-Loss: 26.3876 D-Loss: 2.9284 Loss-g-fm: 0.9783 Loss-g-mel: 20.4057 Loss-g-dur: 1.9503 Loss-g-kl: 1.2742 lr: 0.0002 grad_norm_g: 34.4142 grad_norm_d: 10.5909
======> Epoch: 79
Train Epoch: 80 [80.77%] G-Loss: 26.1818 D-Loss: 2.8049 Loss-g-fm: 1.1320 Loss-g-mel: 20.2300 Loss-g-dur: 1.9345 Loss-g-kl: 1.1659 lr: 0.0002 grad_norm_g: 23.7224 grad_norm_d: 5.1536
======> Epoch: 80
Train Epoch: 81 [76.92%] G-Loss: 25.8337 D-Loss: 2.8414 Loss-g-fm: 1.0326 Loss-g-mel: 19.9581 Loss-g-dur: 1.8808 Loss-g-kl: 1.1573 lr: 0.0002 grad_norm_g: 24.9690 grad_norm_d: 6.7757
======> Epoch: 81
Train Epoch: 82 [73.08%] G-Loss: 26.0712 D-Loss: 2.8214 Loss-g-fm: 1.2229 Loss-g-mel: 20.1748 Loss-g-dur: 1.9432 Loss-g-kl: 1.0392 lr: 0.0002 grad_norm_g: 26.6735 grad_norm_d: 8.6167
======> Epoch: 82
Train Epoch: 83 [69.23%] G-Loss: 25.7993 D-Loss: 2.9201 Loss-g-fm: 1.0355 Loss-g-mel: 20.1004 Loss-g-dur: 1.9444 Loss-g-kl: 1.1506 lr: 0.0002 grad_norm_g: 28.6292 grad_norm_d: 17.1861
======> Epoch: 83
Train Epoch: 84 [65.38%] G-Loss: 28.9538 D-Loss: 2.9261 Loss-g-fm: 1.1342 Loss-g-mel: 22.6337 Loss-g-dur: 2.0133 Loss-g-kl: 1.1553 lr: 0.0002 grad_norm_g: 61.0088 grad_norm_d: 5.3618
======> Epoch: 84
Train Epoch: 85 [61.54%] G-Loss: 27.8421 D-Loss: 2.9359 Loss-g-fm: 1.2713 Loss-g-mel: 21.6830 Loss-g-dur: 1.9696 Loss-g-kl: 1.1778 lr: 0.0002 grad_norm_g: 31.9955 grad_norm_d: 16.6627
======> Epoch: 85
Train Epoch: 86 [57.69%] G-Loss: 27.6488 D-Loss: 2.7886 Loss-g-fm: 1.3321 Loss-g-mel: 21.1150 Loss-g-dur: 2.0002 Loss-g-kl: 1.4894 lr: 0.0002 grad_norm_g: 40.6164 grad_norm_d: 11.2401
======> Epoch: 86
Train Epoch: 87 [53.85%] G-Loss: 26.0590 D-Loss: 2.8542 Loss-g-fm: 1.2781 Loss-g-mel: 20.3891 Loss-g-dur: 1.8867 Loss-g-kl: 1.0613 lr: 0.0002 grad_norm_g: 35.4950 grad_norm_d: 16.7047
======> Epoch: 87
Train Epoch: 88 [50.00%] G-Loss: 24.8779 D-Loss: 2.8086 Loss-g-fm: 1.1301 Loss-g-mel: 19.0838 Loss-g-dur: 1.8653 Loss-g-kl: 1.0712 lr: 0.0002 grad_norm_g: 31.6263 grad_norm_d: 5.5068
======> Epoch: 88
Train Epoch: 89 [46.15%] G-Loss: 27.2363 D-Loss: 2.8271 Loss-g-fm: 1.3424 Loss-g-mel: 21.0194 Loss-g-dur: 2.0113 Loss-g-kl: 1.1442 lr: 0.0002 grad_norm_g: 32.6936 grad_norm_d: 11.7804
======> Epoch: 89
Train Epoch: 90 [42.31%] G-Loss: 26.1624 D-Loss: 2.7834 Loss-g-fm: 1.2840 Loss-g-mel: 20.3086 Loss-g-dur: 1.9052 Loss-g-kl: 1.1051 lr: 0.0002 grad_norm_g: 23.0001 grad_norm_d: 4.1668
======> Epoch: 90
Train Epoch: 91 [38.46%] G-Loss: 27.3654 D-Loss: 2.8082 Loss-g-fm: 1.4159 Loss-g-mel: 20.9613 Loss-g-dur: 2.0016 Loss-g-kl: 1.3123 lr: 0.0002 grad_norm_g: 29.1790 grad_norm_d: 15.0166
======> Epoch: 91
Train Epoch: 92 [34.62%] G-Loss: 24.9680 D-Loss: 2.9100 Loss-g-fm: 0.9209 Loss-g-mel: 19.0211 Loss-g-dur: 1.9055 Loss-g-kl: 1.2116 lr: 0.0002 grad_norm_g: 24.6939 grad_norm_d: 15.8279
======> Epoch: 92
Train Epoch: 93 [30.77%] G-Loss: 25.3002 D-Loss: 2.8152 Loss-g-fm: 1.1609 Loss-g-mel: 19.3916 Loss-g-dur: 1.9596 Loss-g-kl: 1.1420 lr: 0.0002 grad_norm_g: 22.4095 grad_norm_d: 4.8414
======> Epoch: 93
Train Epoch: 94 [26.92%] G-Loss: 27.7478 D-Loss: 2.7878 Loss-g-fm: 1.3397 Loss-g-mel: 21.4067 Loss-g-dur: 2.0265 Loss-g-kl: 1.2048 lr: 0.0002 grad_norm_g: 42.2777 grad_norm_d: 13.3724
======> Epoch: 94
Train Epoch: 95 [23.08%] G-Loss: 26.7630 D-Loss: 2.8771 Loss-g-fm: 1.3086 Loss-g-mel: 20.4862 Loss-g-dur: 1.9716 Loss-g-kl: 1.2685 lr: 0.0002 grad_norm_g: 32.2018 grad_norm_d: 17.1453
======> Epoch: 95
Train Epoch: 96 [19.23%] G-Loss: 27.6985 D-Loss: 2.7809 Loss-g-fm: 1.5201 Loss-g-mel: 21.1346 Loss-g-dur: 1.9389 Loss-g-kl: 1.2961 lr: 0.0002 grad_norm_g: 27.6194 grad_norm_d: 9.3642
======> Epoch: 96
Train Epoch: 97 [15.38%] G-Loss: 27.4787 D-Loss: 2.8212 Loss-g-fm: 1.3752 Loss-g-mel: 21.2959 Loss-g-dur: 1.9675 Loss-g-kl: 1.2495 lr: 0.0002 grad_norm_g: 25.8063 grad_norm_d: 5.0064
======> Epoch: 97
Train Epoch: 98 [11.54%] G-Loss: 25.6846 D-Loss: 2.7629 Loss-g-fm: 1.3303 Loss-g-mel: 19.6676 Loss-g-dur: 1.9179 Loss-g-kl: 1.1725 lr: 0.0002 grad_norm_g: 31.1856 grad_norm_d: 13.8883
======> Epoch: 98
Train Epoch: 99 [7.69%] G-Loss: 26.4270 D-Loss: 2.9577 Loss-g-fm: 1.1029 Loss-g-mel: 20.0377 Loss-g-dur: 2.1051 Loss-g-kl: 1.5185 lr: 0.0002 grad_norm_g: 25.1418 grad_norm_d: 12.6818
======> Epoch: 99
Train Epoch: 100 [3.85%] G-Loss: 27.9364 D-Loss: 2.8355 Loss-g-fm: 1.4613 Loss-g-mel: 21.8176 Loss-g-dur: 1.8855 Loss-g-kl: 1.0784 lr: 0.0002 grad_norm_g: 28.8250 grad_norm_d: 4.6867
======> Epoch: 100
Train Epoch: 101 [0.00%] G-Loss: 25.9579 D-Loss: 2.8484 Loss-g-fm: 1.2933 Loss-g-mel: 20.0746 Loss-g-dur: 1.9039 Loss-g-kl: 0.9988 lr: 0.0002 grad_norm_g: 23.3813 grad_norm_d: 9.2438
Train Epoch: 101 [96.15%] G-Loss: 28.1537 D-Loss: 2.7132 Loss-g-fm: 1.5945 Loss-g-mel: 21.4933 Loss-g-dur: 1.9465 Loss-g-kl: 1.3175 lr: 0.0002 grad_norm_g: 22.0536 grad_norm_d: 5.6729
======> Epoch: 101
Train Epoch: 102 [92.31%] G-Loss: 25.7902 D-Loss: 2.9782 Loss-g-fm: 0.9238 Loss-g-mel: 19.7849 Loss-g-dur: 1.9265 Loss-g-kl: 1.2671 lr: 0.0002 grad_norm_g: 26.0793 grad_norm_d: 31.5326
======> Epoch: 102
Train Epoch: 103 [88.46%] G-Loss: 26.2922 D-Loss: 2.7966 Loss-g-fm: 1.2566 Loss-g-mel: 20.3662 Loss-g-dur: 1.9106 Loss-g-kl: 0.9811 lr: 0.0002 grad_norm_g: 23.9026 grad_norm_d: 3.1453
======> Epoch: 103
Train Epoch: 104 [84.62%] G-Loss: 27.3994 D-Loss: 2.7631 Loss-g-fm: 1.6244 Loss-g-mel: 21.0373 Loss-g-dur: 1.8973 Loss-g-kl: 1.1876 lr: 0.0002 grad_norm_g: 37.3700 grad_norm_d: 8.0714
======> Epoch: 104
Train Epoch: 105 [80.77%] G-Loss: 25.9137 D-Loss: 2.9092 Loss-g-fm: 1.2260 Loss-g-mel: 19.8331 Loss-g-dur: 1.8241 Loss-g-kl: 1.3088 lr: 0.0002 grad_norm_g: 35.5570 grad_norm_d: 27.3750
======> Epoch: 105
Train Epoch: 106 [76.92%] G-Loss: 24.4785 D-Loss: 2.9147 Loss-g-fm: 1.2804 Loss-g-mel: 18.6503 Loss-g-dur: 1.8504 Loss-g-kl: 1.0303 lr: 0.0002 grad_norm_g: 31.7908 grad_norm_d: 8.2631
======> Epoch: 106
Train Epoch: 107 [73.08%] G-Loss: 26.6485 D-Loss: 2.8327 Loss-g-fm: 1.3273 Loss-g-mel: 20.2336 Loss-g-dur: 1.9405 Loss-g-kl: 1.3431 lr: 0.0002 grad_norm_g: 34.8948 grad_norm_d: 9.1864
======> Epoch: 107
Train Epoch: 108 [69.23%] G-Loss: 28.2598 D-Loss: 2.9365 Loss-g-fm: 1.6023 Loss-g-mel: 21.6273 Loss-g-dur: 1.8957 Loss-g-kl: 1.2257 lr: 0.0002 grad_norm_g: 40.1312 grad_norm_d: 44.9523
======> Epoch: 108
Train Epoch: 109 [65.38%] G-Loss: 25.6105 D-Loss: 2.8606 Loss-g-fm: 1.2158 Loss-g-mel: 19.7123 Loss-g-dur: 1.8999 Loss-g-kl: 0.9851 lr: 0.0002 grad_norm_g: 26.0752 grad_norm_d: 11.6399
======> Epoch: 109
Train Epoch: 110 [61.54%] G-Loss: 26.6370 D-Loss: 2.7916 Loss-g-fm: 1.3663 Loss-g-mel: 20.4062 Loss-g-dur: 1.8825 Loss-g-kl: 1.1566 lr: 0.0002 grad_norm_g: 27.4611 grad_norm_d: 12.6917
======> Epoch: 110
Train Epoch: 111 [57.69%] G-Loss: 26.4926 D-Loss: 2.8523 Loss-g-fm: 1.0418 Loss-g-mel: 20.5196 Loss-g-dur: 1.8960 Loss-g-kl: 1.2195 lr: 0.0002 grad_norm_g: 31.5950 grad_norm_d: 9.7448
======> Epoch: 111
Train Epoch: 112 [53.85%] G-Loss: 26.7690 D-Loss: 2.7802 Loss-g-fm: 1.3694 Loss-g-mel: 20.3791 Loss-g-dur: 1.9291 Loss-g-kl: 1.2864 lr: 0.0002 grad_norm_g: 34.6659 grad_norm_d: 4.6839
======> Epoch: 112
Train Epoch: 113 [50.00%] G-Loss: 27.7812 D-Loss: 2.8563 Loss-g-fm: 1.2543 Loss-g-mel: 21.6384 Loss-g-dur: 1.9037 Loss-g-kl: 1.0584 lr: 0.0002 grad_norm_g: 84.7253 grad_norm_d: 8.5719
======> Epoch: 113
Train Epoch: 114 [46.15%] G-Loss: 27.0648 D-Loss: 2.8079 Loss-g-fm: 1.4956 Loss-g-mel: 20.5499 Loss-g-dur: 1.9811 Loss-g-kl: 1.1980 lr: 0.0002 grad_norm_g: 34.8245 grad_norm_d: 11.8377
======> Epoch: 114
Train Epoch: 115 [42.31%] G-Loss: 27.7830 D-Loss: 2.7878 Loss-g-fm: 1.6734 Loss-g-mel: 20.9295 Loss-g-dur: 1.9686 Loss-g-kl: 1.3317 lr: 0.0002 grad_norm_g: 35.1093 grad_norm_d: 10.0482
======> Epoch: 115
Train Epoch: 116 [38.46%] G-Loss: 23.7078 D-Loss: 2.7789 Loss-g-fm: 2.2781 Loss-g-mel: 16.5783 Loss-g-dur: 1.3862 Loss-g-kl: 1.1761 lr: 0.0002 grad_norm_g: 74.3535 grad_norm_d: 10.9802
======> Epoch: 116
Train Epoch: 117 [34.62%] G-Loss: 27.9582 D-Loss: 2.7685 Loss-g-fm: 1.6811 Loss-g-mel: 21.0645 Loss-g-dur: 1.9222 Loss-g-kl: 1.4927 lr: 0.0002 grad_norm_g: 35.5902 grad_norm_d: 10.8332
======> Epoch: 117
Train Epoch: 118 [30.77%] G-Loss: 27.7689 D-Loss: 2.9581 Loss-g-fm: 1.8072 Loss-g-mel: 20.8406 Loss-g-dur: 1.9634 Loss-g-kl: 1.2060 lr: 0.0002 grad_norm_g: 27.7905 grad_norm_d: 15.0504
======> Epoch: 118
Train Epoch: 119 [26.92%] G-Loss: 23.5422 D-Loss: 2.6742 Loss-g-fm: 2.0129 Loss-g-mel: 16.6767 Loss-g-dur: 1.5097 Loss-g-kl: 1.2851 lr: 0.0002 grad_norm_g: 67.0260 grad_norm_d: 16.6620
======> Epoch: 119
Train Epoch: 120 [23.08%] G-Loss: 24.1502 D-Loss: 2.7690 Loss-g-fm: 1.4097 Loss-g-mel: 18.1320 Loss-g-dur: 1.7520 Loss-g-kl: 1.0677 lr: 0.0002 grad_norm_g: 38.8248 grad_norm_d: 4.0674
======> Epoch: 120
Train Epoch: 121 [19.23%] G-Loss: 26.2673 D-Loss: 2.7876 Loss-g-fm: 1.5555 Loss-g-mel: 19.7773 Loss-g-dur: 1.9714 Loss-g-kl: 1.2191 lr: 0.0002 grad_norm_g: 57.6471 grad_norm_d: 13.5012
======> Epoch: 121
Train Epoch: 122 [15.38%] G-Loss: 27.3224 D-Loss: 2.7068 Loss-g-fm: 1.5957 Loss-g-mel: 20.5275 Loss-g-dur: 2.0462 Loss-g-kl: 1.3256 lr: 0.0002 grad_norm_g: 39.1669 grad_norm_d: 10.9894
======> Epoch: 122
Train Epoch: 123 [11.54%] G-Loss: 27.8350 D-Loss: 2.7963 Loss-g-fm: 1.6099 Loss-g-mel: 21.2187 Loss-g-dur: 1.8582 Loss-g-kl: 1.2343 lr: 0.0002 grad_norm_g: 62.5024 grad_norm_d: 15.9602
======> Epoch: 123
Train Epoch: 124 [7.69%] G-Loss: 26.7420 D-Loss: 2.7057 Loss-g-fm: 1.6251 Loss-g-mel: 20.3420 Loss-g-dur: 1.8578 Loss-g-kl: 1.1337 lr: 0.0002 grad_norm_g: 43.4270 grad_norm_d: 12.6125
======> Epoch: 124
Train Epoch: 125 [3.85%] G-Loss: 24.1792 D-Loss: 2.9238 Loss-g-fm: 1.0554 Loss-g-mel: 18.1849 Loss-g-dur: 1.8391 Loss-g-kl: 1.2230 lr: 0.0002 grad_norm_g: 38.1683 grad_norm_d: 25.8604
======> Epoch: 125
Train Epoch: 126 [0.00%] G-Loss: 25.2128 D-Loss: 2.7617 Loss-g-fm: 1.6069 Loss-g-mel: 18.9819 Loss-g-dur: 1.7114 Loss-g-kl: 1.1406 lr: 0.0002 grad_norm_g: 44.2994 grad_norm_d: 9.9213
Train Epoch: 126 [96.15%] G-Loss: 27.2472 D-Loss: 2.8303 Loss-g-fm: 1.6645 Loss-g-mel: 20.3629 Loss-g-dur: 1.9709 Loss-g-kl: 1.4161 lr: 0.0002 grad_norm_g: 42.6072 grad_norm_d: 19.7807
======> Epoch: 126
Train Epoch: 127 [92.31%] G-Loss: 26.8933 D-Loss: 2.6802 Loss-g-fm: 1.7757 Loss-g-mel: 20.3027 Loss-g-dur: 1.8712 Loss-g-kl: 1.1152 lr: 0.0002 grad_norm_g: 33.3926 grad_norm_d: 6.4449
======> Epoch: 127
Train Epoch: 128 [88.46%] G-Loss: 27.5602 D-Loss: 2.8075 Loss-g-fm: 1.8156 Loss-g-mel: 20.7349 Loss-g-dur: 1.9298 Loss-g-kl: 1.2050 lr: 0.0002 grad_norm_g: 42.3112 grad_norm_d: 9.7516
======> Epoch: 128
Train Epoch: 129 [84.62%] G-Loss: 26.0404 D-Loss: 2.7680 Loss-g-fm: 1.4317 Loss-g-mel: 19.8305 Loss-g-dur: 1.8788 Loss-g-kl: 1.1322 lr: 0.0002 grad_norm_g: 37.2934 grad_norm_d: 7.6446
======> Epoch: 129
Train Epoch: 130 [80.77%] G-Loss: 28.2115 D-Loss: 2.6920 Loss-g-fm: 1.9105 Loss-g-mel: 21.5038 Loss-g-dur: 1.9025 Loss-g-kl: 1.1983 lr: 0.0002 grad_norm_g: 33.2184 grad_norm_d: 7.0484
======> Epoch: 130
Train Epoch: 131 [76.92%] G-Loss: 26.4458 D-Loss: 2.7505 Loss-g-fm: 1.6698 Loss-g-mel: 19.7665 Loss-g-dur: 1.9091 Loss-g-kl: 1.2874 lr: 0.0002 grad_norm_g: 29.3938 grad_norm_d: 7.2175
======> Epoch: 131
Train Epoch: 132 [73.08%] G-Loss: 28.6557 D-Loss: 2.8281 Loss-g-fm: 1.8758 Loss-g-mel: 21.7765 Loss-g-dur: 1.9163 Loss-g-kl: 1.2639 lr: 0.0002 grad_norm_g: 22.3100 grad_norm_d: 10.2722
======> Epoch: 132
Train Epoch: 133 [69.23%] G-Loss: 27.2884 D-Loss: 2.7560 Loss-g-fm: 1.6081 Loss-g-mel: 20.5134 Loss-g-dur: 1.9223 Loss-g-kl: 1.4336 lr: 0.0002 grad_norm_g: 37.7387 grad_norm_d: 14.7123
======> Epoch: 133
Train Epoch: 134 [65.38%] G-Loss: 26.4739 D-Loss: 2.8260 Loss-g-fm: 1.2402 Loss-g-mel: 20.3710 Loss-g-dur: 1.8384 Loss-g-kl: 1.1741 lr: 0.0002 grad_norm_g: 42.8556 grad_norm_d: 16.5473
======> Epoch: 134
Train Epoch: 135 [61.54%] G-Loss: 23.6655 D-Loss: 2.6656 Loss-g-fm: 2.1968 Loss-g-mel: 16.7566 Loss-g-dur: 1.3908 Loss-g-kl: 1.1418 lr: 0.0002 grad_norm_g: 55.1570 grad_norm_d: 16.0948
======> Epoch: 135
Train Epoch: 136 [57.69%] G-Loss: 27.2122 D-Loss: 2.7496 Loss-g-fm: 1.6696 Loss-g-mel: 20.7228 Loss-g-dur: 1.8788 Loss-g-kl: 1.2225 lr: 0.0002 grad_norm_g: 30.2440 grad_norm_d: 14.6358
======> Epoch: 136
Train Epoch: 137 [53.85%] G-Loss: 26.1224 D-Loss: 2.7653 Loss-g-fm: 1.6340 Loss-g-mel: 19.7840 Loss-g-dur: 1.8336 Loss-g-kl: 1.0326 lr: 0.0002 grad_norm_g: 29.3370 grad_norm_d: 12.0949
======> Epoch: 137
Train Epoch: 138 [50.00%] G-Loss: 27.3791 D-Loss: 2.7397 Loss-g-fm: 1.7854 Loss-g-mel: 20.7068 Loss-g-dur: 1.9305 Loss-g-kl: 1.2664 lr: 0.0002 grad_norm_g: 33.6697 grad_norm_d: 10.6606
======> Epoch: 138
Train Epoch: 139 [46.15%] G-Loss: 26.4687 D-Loss: 2.7332 Loss-g-fm: 1.7122 Loss-g-mel: 20.1249 Loss-g-dur: 1.8289 Loss-g-kl: 1.0600 lr: 0.0002 grad_norm_g: 35.0178 grad_norm_d: 5.1484
======> Epoch: 139
Train Epoch: 140 [42.31%] G-Loss: 26.2508 D-Loss: 2.8035 Loss-g-fm: 1.5586 Loss-g-mel: 19.6418 Loss-g-dur: 1.8293 Loss-g-kl: 1.2325 lr: 0.0002 grad_norm_g: 45.8144 grad_norm_d: 20.1636
======> Epoch: 140
Train Epoch: 141 [38.46%] G-Loss: 26.5931 D-Loss: 2.6860 Loss-g-fm: 1.6762 Loss-g-mel: 20.1719 Loss-g-dur: 1.8652 Loss-g-kl: 1.1773 lr: 0.0002 grad_norm_g: 46.0693 grad_norm_d: 9.0068
======> Epoch: 141
Train Epoch: 142 [34.62%] G-Loss: 27.5646 D-Loss: 2.6691 Loss-g-fm: 1.9583 Loss-g-mel: 20.6691 Loss-g-dur: 1.8447 Loss-g-kl: 1.1874 lr: 0.0002 grad_norm_g: 24.4748 grad_norm_d: 5.1834
======> Epoch: 142
Train Epoch: 143 [30.77%] G-Loss: 26.0950 D-Loss: 2.7312 Loss-g-fm: 1.7407 Loss-g-mel: 19.4172 Loss-g-dur: 1.8708 Loss-g-kl: 1.2558 lr: 0.0002 grad_norm_g: 30.6872 grad_norm_d: 6.1596
======> Epoch: 143
Train Epoch: 144 [26.92%] G-Loss: 25.5237 D-Loss: 2.9205 Loss-g-fm: 1.3621 Loss-g-mel: 19.1784 Loss-g-dur: 1.9238 Loss-g-kl: 1.1979 lr: 0.0002 grad_norm_g: 22.2127 grad_norm_d: 19.5414
======> Epoch: 144
Train Epoch: 145 [23.08%] G-Loss: 24.7362 D-Loss: 2.9715 Loss-g-fm: 1.3718 Loss-g-mel: 18.7635 Loss-g-dur: 1.8296 Loss-g-kl: 0.8868 lr: 0.0002 grad_norm_g: 37.2815 grad_norm_d: 33.8308
======> Epoch: 145
Train Epoch: 146 [19.23%] G-Loss: 24.9629 D-Loss: 2.8051 Loss-g-fm: 1.8721 Loss-g-mel: 18.4033 Loss-g-dur: 1.7150 Loss-g-kl: 1.0479 lr: 0.0002 grad_norm_g: 48.3169 grad_norm_d: 14.2970
======> Epoch: 146
Train Epoch: 147 [15.38%] G-Loss: 28.1662 D-Loss: 2.7499 Loss-g-fm: 2.0303 Loss-g-mel: 20.7296 Loss-g-dur: 2.0324 Loss-g-kl: 1.4241 lr: 0.0002 grad_norm_g: 58.8836 grad_norm_d: 9.9058
======> Epoch: 147
Train Epoch: 148 [11.54%] G-Loss: 25.8336 D-Loss: 2.6778 Loss-g-fm: 1.7000 Loss-g-mel: 19.5234 Loss-g-dur: 1.7955 Loss-g-kl: 1.0726 lr: 0.0002 grad_norm_g: 23.1706 grad_norm_d: 5.7230
======> Epoch: 148
Train Epoch: 149 [7.69%] G-Loss: 25.8369 D-Loss: 2.7071 Loss-g-fm: 1.6418 Loss-g-mel: 19.3312 Loss-g-dur: 1.7918 Loss-g-kl: 1.2059 lr: 0.0002 grad_norm_g: 52.2787 grad_norm_d: 7.8600
======> Epoch: 149
Train Epoch: 150 [3.85%] G-Loss: 25.8447 D-Loss: 2.7856 Loss-g-fm: 1.5888 Loss-g-mel: 19.5329 Loss-g-dur: 1.8044 Loss-g-kl: 1.1918 lr: 0.0002 grad_norm_g: 31.0711 grad_norm_d: 10.5898
======> Epoch: 150
Train Epoch: 151 [0.00%] G-Loss: 27.6012 D-Loss: 2.7028 Loss-g-fm: 2.0337 Loss-g-mel: 20.3884 Loss-g-dur: 1.8910 Loss-g-kl: 1.4235 lr: 0.0002 grad_norm_g: 51.5460 grad_norm_d: 6.5117
Train Epoch: 151 [96.15%] G-Loss: 27.2137 D-Loss: 2.7503 Loss-g-fm: 2.0629 Loss-g-mel: 20.1212 Loss-g-dur: 1.8454 Loss-g-kl: 1.1691 lr: 0.0002 grad_norm_g: 37.1564 grad_norm_d: 15.2043
======> Epoch: 151
Train Epoch: 152 [92.31%] G-Loss: 25.7781 D-Loss: 2.8319 Loss-g-fm: 1.5453 Loss-g-mel: 19.2877 Loss-g-dur: 1.8648 Loss-g-kl: 1.2537 lr: 0.0002 grad_norm_g: 42.6841 grad_norm_d: 20.4382
======> Epoch: 152
Train Epoch: 153 [88.46%] G-Loss: 28.4246 D-Loss: 2.7065 Loss-g-fm: 1.9972 Loss-g-mel: 21.3267 Loss-g-dur: 1.9404 Loss-g-kl: 1.3365 lr: 0.0002 grad_norm_g: 54.8490 grad_norm_d: 9.7867
======> Epoch: 153
Train Epoch: 154 [84.62%] G-Loss: 26.4112 D-Loss: 2.6935 Loss-g-fm: 1.7753 Loss-g-mel: 19.7798 Loss-g-dur: 1.8428 Loss-g-kl: 1.1873 lr: 0.0002 grad_norm_g: 25.0941 grad_norm_d: 13.0186
Saving model and optimizer state at iteration 154 to /ZFS4T/tts/data/VITS/model_saved/G_4000.pth
Saving model and optimizer state at iteration 154 to /ZFS4T/tts/data/VITS/model_saved/D_4000.pth
======> Epoch: 154
Train Epoch: 155 [80.77%] G-Loss: 25.0952 D-Loss: 2.6987 Loss-g-fm: 1.6765 Loss-g-mel: 18.6189 Loss-g-dur: 1.8148 Loss-g-kl: 1.1739 lr: 0.0002 grad_norm_g: 39.7860 grad_norm_d: 11.0116
======> Epoch: 155
Train Epoch: 156 [76.92%] G-Loss: 26.0306 D-Loss: 2.8149 Loss-g-fm: 1.7407 Loss-g-mel: 19.4126 Loss-g-dur: 1.8619 Loss-g-kl: 1.3278 lr: 0.0002 grad_norm_g: 19.9773 grad_norm_d: 8.4197
======> Epoch: 156
Train Epoch: 157 [73.08%] G-Loss: 28.8728 D-Loss: 2.6128 Loss-g-fm: 2.5116 Loss-g-mel: 21.2404 Loss-g-dur: 1.9111 Loss-g-kl: 1.4089 lr: 0.0002 grad_norm_g: 71.3655 grad_norm_d: 12.4439
======> Epoch: 157
Train Epoch: 158 [69.23%] G-Loss: 27.4682 D-Loss: 2.6261 Loss-g-fm: 2.4074 Loss-g-mel: 20.1089 Loss-g-dur: 1.9154 Loss-g-kl: 1.2697 lr: 0.0002 grad_norm_g: 50.6051 grad_norm_d: 8.7694
======> Epoch: 158
Train Epoch: 159 [65.38%] G-Loss: 26.5305 D-Loss: 2.7323 Loss-g-fm: 1.9209 Loss-g-mel: 19.7718 Loss-g-dur: 1.8152 Loss-g-kl: 1.2289 lr: 0.0002 grad_norm_g: 22.4883 grad_norm_d: 6.7684
======> Epoch: 159
Train Epoch: 160 [61.54%] G-Loss: 25.6071 D-Loss: 2.7006 Loss-g-fm: 2.0638 Loss-g-mel: 18.9621 Loss-g-dur: 1.8438 Loss-g-kl: 1.0056 lr: 0.0002 grad_norm_g: 63.7228 grad_norm_d: 30.1925
======> Epoch: 160
Train Epoch: 161 [57.69%] G-Loss: 26.6361 D-Loss: 2.7214 Loss-g-fm: 1.8787 Loss-g-mel: 19.8582 Loss-g-dur: 1.9114 Loss-g-kl: 1.2327 lr: 0.0002 grad_norm_g: 37.0095 grad_norm_d: 22.2139
======> Epoch: 161
Train Epoch: 162 [53.85%] G-Loss: 27.7292 D-Loss: 2.6390 Loss-g-fm: 2.2477 Loss-g-mel: 20.3473 Loss-g-dur: 2.0142 Loss-g-kl: 1.2639 lr: 0.0002 grad_norm_g: 40.6042 grad_norm_d: 7.7511
======> Epoch: 162
Train Epoch: 163 [50.00%] G-Loss: 28.0849 D-Loss: 2.6617 Loss-g-fm: 2.0591 Loss-g-mel: 20.4050 Loss-g-dur: 2.0211 Loss-g-kl: 1.5362 lr: 0.0002 grad_norm_g: 74.1821 grad_norm_d: 9.8744
======> Epoch: 163
Train Epoch: 164 [46.15%] G-Loss: 27.6929 D-Loss: 2.6832 Loss-g-fm: 2.2308 Loss-g-mel: 20.5352 Loss-g-dur: 1.9047 Loss-g-kl: 1.2700 lr: 0.0002 grad_norm_g: 47.8601 grad_norm_d: 7.2233
======> Epoch: 164
Train Epoch: 165 [42.31%] G-Loss: 26.3088 D-Loss: 2.6382 Loss-g-fm: 1.9711 Loss-g-mel: 19.1752 Loss-g-dur: 1.8133 Loss-g-kl: 1.3696 lr: 0.0002 grad_norm_g: 36.9457 grad_norm_d: 6.8878
======> Epoch: 165
Train Epoch: 166 [38.46%] G-Loss: 26.2946 D-Loss: 2.7655 Loss-g-fm: 1.7922 Loss-g-mel: 19.6149 Loss-g-dur: 1.7897 Loss-g-kl: 1.3315 lr: 0.0002 grad_norm_g: 40.2746 grad_norm_d: 7.1192
======> Epoch: 166
Train Epoch: 167 [34.62%] G-Loss: 27.5247 D-Loss: 2.7297 Loss-g-fm: 1.9419 Loss-g-mel: 20.7118 Loss-g-dur: 1.8117 Loss-g-kl: 1.2045 lr: 0.0002 grad_norm_g: 53.7741 grad_norm_d: 5.7956
======> Epoch: 167
Train Epoch: 168 [30.77%] G-Loss: 27.1178 D-Loss: 2.7256 Loss-g-fm: 2.1285 Loss-g-mel: 19.5044 Loss-g-dur: 2.0148 Loss-g-kl: 1.3717 lr: 0.0002 grad_norm_g: 27.0092 grad_norm_d: 10.3728
======> Epoch: 168
Train Epoch: 169 [26.92%] G-Loss: 22.0923 D-Loss: 2.6119 Loss-g-fm: 2.6541 Loss-g-mel: 14.9618 Loss-g-dur: 1.4357 Loss-g-kl: 1.0830 lr: 0.0002 grad_norm_g: 59.0475 grad_norm_d: 6.3246
======> Epoch: 169
Train Epoch: 170 [23.08%] G-Loss: 26.9868 D-Loss: 2.7266 Loss-g-fm: 2.0381 Loss-g-mel: 19.8108 Loss-g-dur: 1.9191 Loss-g-kl: 1.4112 lr: 0.0002 grad_norm_g: 28.5746 grad_norm_d: 14.3185
======> Epoch: 170
Train Epoch: 171 [19.23%] G-Loss: 26.3303 D-Loss: 2.6630 Loss-g-fm: 1.8641 Loss-g-mel: 19.7074 Loss-g-dur: 1.7861 Loss-g-kl: 1.0869 lr: 0.0002 grad_norm_g: 38.2445 grad_norm_d: 5.6160
======> Epoch: 171
Train Epoch: 172 [15.38%] G-Loss: 28.7103 D-Loss: 2.6759 Loss-g-fm: 2.0846 Loss-g-mel: 21.3043 Loss-g-dur: 1.9067 Loss-g-kl: 1.4621 lr: 0.0002 grad_norm_g: 33.1536 grad_norm_d: 11.7365
======> Epoch: 172
Train Epoch: 173 [11.54%] G-Loss: 27.7167 D-Loss: 2.8467 Loss-g-fm: 1.9926 Loss-g-mel: 20.5823 Loss-g-dur: 1.9114 Loss-g-kl: 1.3974 lr: 0.0002 grad_norm_g: 69.6849 grad_norm_d: 42.2668
======> Epoch: 173
Train Epoch: 174 [7.69%] G-Loss: 27.1545 D-Loss: 2.6839 Loss-g-fm: 2.1280 Loss-g-mel: 19.8735 Loss-g-dur: 1.9150 Loss-g-kl: 1.3468 lr: 0.0002 grad_norm_g: 39.7194 grad_norm_d: 14.3255
======> Epoch: 174
Train Epoch: 175 [3.85%] G-Loss: 26.8712 D-Loss: 2.7295 Loss-g-fm: 1.9544 Loss-g-mel: 19.8464 Loss-g-dur: 1.7998 Loss-g-kl: 1.3386 lr: 0.0002 grad_norm_g: 31.5165 grad_norm_d: 12.9706
======> Epoch: 175
Train Epoch: 176 [0.00%] G-Loss: 25.1768 D-Loss: 2.6044 Loss-g-fm: 2.1527 Loss-g-mel: 18.2464 Loss-g-dur: 1.6946 Loss-g-kl: 1.1174 lr: 0.0002 grad_norm_g: 48.6578 grad_norm_d: 9.7941
Train Epoch: 176 [96.15%] G-Loss: 26.2588 D-Loss: 2.7603 Loss-g-fm: 2.0189 Loss-g-mel: 19.2366 Loss-g-dur: 1.8278 Loss-g-kl: 1.2914 lr: 0.0002 grad_norm_g: 23.3964 grad_norm_d: 7.9762
======> Epoch: 176
Train Epoch: 177 [92.31%] G-Loss: 27.5835 D-Loss: 2.6045 Loss-g-fm: 2.5695 Loss-g-mel: 19.8383 Loss-g-dur: 1.9047 Loss-g-kl: 1.2689 lr: 0.0002 grad_norm_g: 47.5398 grad_norm_d: 21.2692
======> Epoch: 177
Train Epoch: 178 [88.46%] G-Loss: 27.3170 D-Loss: 2.7602 Loss-g-fm: 2.1635 Loss-g-mel: 20.2872 Loss-g-dur: 1.8845 Loss-g-kl: 1.1417 lr: 0.0002 grad_norm_g: 62.5218 grad_norm_d: 23.3344
======> Epoch: 178
Train Epoch: 179 [84.62%] G-Loss: 25.3767 D-Loss: 2.7711 Loss-g-fm: 1.7656 Loss-g-mel: 18.7112 Loss-g-dur: 1.8045 Loss-g-kl: 1.3274 lr: 0.0002 grad_norm_g: 41.1448 grad_norm_d: 21.7576
======> Epoch: 179
Train Epoch: 180 [80.77%] G-Loss: 26.5509 D-Loss: 2.7222 Loss-g-fm: 1.8639 Loss-g-mel: 19.7062 Loss-g-dur: 1.8317 Loss-g-kl: 1.0582 lr: 0.0002 grad_norm_g: 45.5099 grad_norm_d: 13.1249
======> Epoch: 180
Train Epoch: 181 [76.92%] G-Loss: 28.8781 D-Loss: 2.7265 Loss-g-fm: 2.3177 Loss-g-mel: 21.4866 Loss-g-dur: 1.9283 Loss-g-kl: 1.2811 lr: 0.0002 grad_norm_g: 58.6780 grad_norm_d: 5.8833
======> Epoch: 181
Train Epoch: 182 [73.08%] G-Loss: 25.0954 D-Loss: 2.8699 Loss-g-fm: 2.2249 Loss-g-mel: 18.3045 Loss-g-dur: 1.6857 Loss-g-kl: 1.2230 lr: 0.0002 grad_norm_g: 45.6668 grad_norm_d: 28.4700
======> Epoch: 182
Train Epoch: 183 [69.23%] G-Loss: 27.0270 D-Loss: 2.7455 Loss-g-fm: 2.0841 Loss-g-mel: 19.8190 Loss-g-dur: 1.9056 Loss-g-kl: 1.1336 lr: 0.0002 grad_norm_g: 106.6029 grad_norm_d: 7.3226
======> Epoch: 183
Train Epoch: 184 [65.38%] G-Loss: 23.7808 D-Loss: 2.6188 Loss-g-fm: 3.0022 Loss-g-mel: 15.9341 Loss-g-dur: 1.3748 Loss-g-kl: 1.1840 lr: 0.0002 grad_norm_g: 141.4496 grad_norm_d: 9.4290
======> Epoch: 184
Train Epoch: 185 [61.54%] G-Loss: 27.5782 D-Loss: 2.6804 Loss-g-fm: 2.3912 Loss-g-mel: 20.4815 Loss-g-dur: 1.7670 Loss-g-kl: 1.0424 lr: 0.0002 grad_norm_g: 31.5284 grad_norm_d: 4.2044
======> Epoch: 185
Train Epoch: 186 [57.69%] G-Loss: 28.4199 D-Loss: 2.6183 Loss-g-fm: 2.5175 Loss-g-mel: 20.8911 Loss-g-dur: 1.8941 Loss-g-kl: 1.2920 lr: 0.0002 grad_norm_g: 40.3460 grad_norm_d: 9.2540
======> Epoch: 186
Train Epoch: 187 [53.85%] G-Loss: 29.3861 D-Loss: 2.6645 Loss-g-fm: 2.6427 Loss-g-mel: 21.1414 Loss-g-dur: 1.9501 Loss-g-kl: 1.3819 lr: 0.0002 grad_norm_g: 90.7787 grad_norm_d: 13.5196
======> Epoch: 187
Train Epoch: 188 [50.00%] G-Loss: 27.0796 D-Loss: 2.6824 Loss-g-fm: 2.5655 Loss-g-mel: 19.4257 Loss-g-dur: 1.8954 Loss-g-kl: 1.2800 lr: 0.0002 grad_norm_g: 43.3352 grad_norm_d: 5.4934
======> Epoch: 188
Train Epoch: 189 [46.15%] G-Loss: 28.2579 D-Loss: 2.7500 Loss-g-fm: 2.3987 Loss-g-mel: 20.6514 Loss-g-dur: 1.8828 Loss-g-kl: 1.3246 lr: 0.0002 grad_norm_g: 63.4804 grad_norm_d: 9.5572
======> Epoch: 189
Train Epoch: 190 [42.31%] G-Loss: 27.0848 D-Loss: 2.7944 Loss-g-fm: 2.8510 Loss-g-mel: 19.1293 Loss-g-dur: 1.6519 Loss-g-kl: 1.1980 lr: 0.0002 grad_norm_g: 47.1277 grad_norm_d: 9.2527
======> Epoch: 190
Train Epoch: 191 [38.46%] G-Loss: 27.9409 D-Loss: 2.6138 Loss-g-fm: 2.4789 Loss-g-mel: 20.7339 Loss-g-dur: 1.7806 Loss-g-kl: 1.0181 lr: 0.0002 grad_norm_g: 72.4367 grad_norm_d: 6.3821
======> Epoch: 191
Train Epoch: 192 [34.62%] G-Loss: 25.7116 D-Loss: 2.7366 Loss-g-fm: 1.8792 Loss-g-mel: 19.0517 Loss-g-dur: 1.8732 Loss-g-kl: 1.2478 lr: 0.0002 grad_norm_g: 46.4311 grad_norm_d: 11.1766
======> Epoch: 192
Train Epoch: 193 [30.77%] G-Loss: 26.3574 D-Loss: 2.7251 Loss-g-fm: 2.1356 Loss-g-mel: 19.4249 Loss-g-dur: 1.8003 Loss-g-kl: 1.1133 lr: 0.0002 grad_norm_g: 23.7586 grad_norm_d: 8.0765
======> Epoch: 193
Train Epoch: 194 [26.92%] G-Loss: 25.4454 D-Loss: 2.6843 Loss-g-fm: 2.0731 Loss-g-mel: 18.3935 Loss-g-dur: 1.7987 Loss-g-kl: 1.3140 lr: 0.0002 grad_norm_g: 53.5394 grad_norm_d: 8.7865
======> Epoch: 194
Train Epoch: 195 [23.08%] G-Loss: 28.4491 D-Loss: 2.5792 Loss-g-fm: 3.1190 Loss-g-mel: 19.8804 Loss-g-dur: 1.8566 Loss-g-kl: 1.5280 lr: 0.0002 grad_norm_g: 65.3330 grad_norm_d: 10.5690
======> Epoch: 195
Train Epoch: 196 [19.23%] G-Loss: 26.2616 D-Loss: 2.8677 Loss-g-fm: 1.8940 Loss-g-mel: 19.0979 Loss-g-dur: 1.8090 Loss-g-kl: 1.3679 lr: 0.0002 grad_norm_g: 94.6178 grad_norm_d: 31.7841
======> Epoch: 196
Train Epoch: 197 [15.38%] G-Loss: 27.2518 D-Loss: 2.5734 Loss-g-fm: 2.4775 Loss-g-mel: 19.8262 Loss-g-dur: 1.8016 Loss-g-kl: 1.2745 lr: 0.0002 grad_norm_g: 52.2802 grad_norm_d: 8.8145
======> Epoch: 197
Train Epoch: 198 [11.54%] G-Loss: 27.3587 D-Loss: 2.7620 Loss-g-fm: 2.3882 Loss-g-mel: 19.9426 Loss-g-dur: 1.8943 Loss-g-kl: 1.4602 lr: 0.0002 grad_norm_g: 49.3758 grad_norm_d: 23.1746
======> Epoch: 198
Train Epoch: 199 [7.69%] G-Loss: 27.8208 D-Loss: 2.6443 Loss-g-fm: 2.4356 Loss-g-mel: 20.0870 Loss-g-dur: 1.8994 Loss-g-kl: 1.4255 lr: 0.0002 grad_norm_g: 26.2018 grad_norm_d: 4.5465
======> Epoch: 199
Train Epoch: 200 [3.85%] G-Loss: 22.3234 D-Loss: 2.6852 Loss-g-fm: 2.9948 Loss-g-mel: 14.7147 Loss-g-dur: 1.4180 Loss-g-kl: 1.2128 lr: 0.0002 grad_norm_g: 56.3529 grad_norm_d: 18.8411
======> Epoch: 200
Train Epoch: 201 [0.00%] G-Loss: 27.3412 D-Loss: 2.7529 Loss-g-fm: 2.2834 Loss-g-mel: 20.2777 Loss-g-dur: 1.8611 Loss-g-kl: 1.1300 lr: 0.0002 grad_norm_g: 40.7126 grad_norm_d: 11.1077
Train Epoch: 201 [96.15%] G-Loss: 28.8187 D-Loss: 2.7604 Loss-g-fm: 2.2447 Loss-g-mel: 21.0556 Loss-g-dur: 1.8248 Loss-g-kl: 1.3121 lr: 0.0002 grad_norm_g: 60.9415 grad_norm_d: 8.7026
======> Epoch: 201
Train Epoch: 202 [92.31%] G-Loss: 27.8091 D-Loss: 2.6255 Loss-g-fm: 2.5496 Loss-g-mel: 20.2315 Loss-g-dur: 1.8781 Loss-g-kl: 1.2719 lr: 0.0002 grad_norm_g: 44.7764 grad_norm_d: 14.9672
======> Epoch: 202
Train Epoch: 203 [88.46%] G-Loss: 27.5767 D-Loss: 2.5929 Loss-g-fm: 2.6002 Loss-g-mel: 19.9947 Loss-g-dur: 1.8808 Loss-g-kl: 1.2143 lr: 0.0002 grad_norm_g: 37.2798 grad_norm_d: 12.2664
======> Epoch: 203
Train Epoch: 204 [84.62%] G-Loss: 26.4001 D-Loss: 2.8316 Loss-g-fm: 2.2265 Loss-g-mel: 18.9972 Loss-g-dur: 1.9001 Loss-g-kl: 1.2129 lr: 0.0002 grad_norm_g: 123.7099 grad_norm_d: 20.2624
======> Epoch: 204
Train Epoch: 205 [80.77%] G-Loss: 22.5715 D-Loss: 2.6329 Loss-g-fm: 3.0664 Loss-g-mel: 14.9454 Loss-g-dur: 1.3170 Loss-g-kl: 1.2702 lr: 0.0002 grad_norm_g: 59.5883 grad_norm_d: 9.5648
======> Epoch: 205
Train Epoch: 206 [76.92%] G-Loss: 27.3664 D-Loss: 2.7019 Loss-g-fm: 2.5036 Loss-g-mel: 19.9367 Loss-g-dur: 1.8440 Loss-g-kl: 1.2165 lr: 0.0002 grad_norm_g: 72.4147 grad_norm_d: 19.3730
======> Epoch: 206
Train Epoch: 207 [73.08%] G-Loss: 27.1444 D-Loss: 2.7076 Loss-g-fm: 2.5502 Loss-g-mel: 19.8934 Loss-g-dur: 1.8054 Loss-g-kl: 1.0430 lr: 0.0002 grad_norm_g: 62.7545 grad_norm_d: 24.6163
======> Epoch: 207
Train Epoch: 208 [69.23%] G-Loss: 27.6509 D-Loss: 2.5831 Loss-g-fm: 2.3883 Loss-g-mel: 20.2834 Loss-g-dur: 1.8207 Loss-g-kl: 1.2106 lr: 0.0002 grad_norm_g: 99.7900 grad_norm_d: 10.0961
======> Epoch: 208
Train Epoch: 209 [65.38%] G-Loss: 27.5411 D-Loss: 2.5993 Loss-g-fm: 2.4810 Loss-g-mel: 19.9053 Loss-g-dur: 1.8178 Loss-g-kl: 1.4236 lr: 0.0002 grad_norm_g: 31.1489 grad_norm_d: 6.7772
======> Epoch: 209
Train Epoch: 210 [61.54%] G-Loss: 28.2306 D-Loss: 2.6775 Loss-g-fm: 2.4181 Loss-g-mel: 20.3600 Loss-g-dur: 1.8744 Loss-g-kl: 1.4964 lr: 0.0002 grad_norm_g: 41.1297 grad_norm_d: 8.8932
======> Epoch: 210
Train Epoch: 211 [57.69%] G-Loss: 25.2321 D-Loss: 2.7382 Loss-g-fm: 2.0713 Loss-g-mel: 18.3027 Loss-g-dur: 1.7825 Loss-g-kl: 1.2729 lr: 0.0002 grad_norm_g: 60.4072 grad_norm_d: 28.1181
======> Epoch: 211
Train Epoch: 212 [53.85%] G-Loss: 26.1320 D-Loss: 2.7730 Loss-g-fm: 2.0083 Loss-g-mel: 19.3158 Loss-g-dur: 1.7727 Loss-g-kl: 1.1318 lr: 0.0002 grad_norm_g: 58.2752 grad_norm_d: 9.2094
======> Epoch: 212
Train Epoch: 213 [50.00%] G-Loss: 29.0698 D-Loss: 2.6396 Loss-g-fm: 3.1005 Loss-g-mel: 20.7154 Loss-g-dur: 1.9300 Loss-g-kl: 1.2979 lr: 0.0002 grad_norm_g: 46.0376 grad_norm_d: 6.8902
======> Epoch: 213
Train Epoch: 214 [46.15%] G-Loss: 26.7700 D-Loss: 2.6084 Loss-g-fm: 2.5458 Loss-g-mel: 19.0268 Loss-g-dur: 1.8427 Loss-g-kl: 1.3397 lr: 0.0002 grad_norm_g: 49.4600 grad_norm_d: 12.2816
======> Epoch: 214
Train Epoch: 215 [42.31%] G-Loss: 27.0856 D-Loss: 2.7173 Loss-g-fm: 2.4396 Loss-g-mel: 19.7626 Loss-g-dur: 1.8271 Loss-g-kl: 1.3605 lr: 0.0002 grad_norm_g: 56.5055 grad_norm_d: 14.1723
======> Epoch: 215
Train Epoch: 216 [38.46%] G-Loss: 28.2007 D-Loss: 2.8148 Loss-g-fm: 2.6116 Loss-g-mel: 20.2805 Loss-g-dur: 1.8786 Loss-g-kl: 1.3659 lr: 0.0002 grad_norm_g: 125.5041 grad_norm_d: 42.7082
======> Epoch: 216
Train Epoch: 217 [34.62%] G-Loss: 26.3264 D-Loss: 2.8203 Loss-g-fm: 2.2468 Loss-g-mel: 19.1801 Loss-g-dur: 1.7953 Loss-g-kl: 1.1572 lr: 0.0002 grad_norm_g: 57.0634 grad_norm_d: 35.5631
======> Epoch: 217
Train Epoch: 218 [30.77%] G-Loss: 25.3294 D-Loss: 2.6638 Loss-g-fm: 2.0551 Loss-g-mel: 18.5509 Loss-g-dur: 1.7905 Loss-g-kl: 1.0748 lr: 0.0002 grad_norm_g: 28.8676 grad_norm_d: 7.5770
======> Epoch: 218
Train Epoch: 219 [26.92%] G-Loss: 26.8137 D-Loss: 2.6654 Loss-g-fm: 2.4266 Loss-g-mel: 19.6350 Loss-g-dur: 1.7756 Loss-g-kl: 1.0355 lr: 0.0002 grad_norm_g: 27.8233 grad_norm_d: 5.9323
======> Epoch: 219
Train Epoch: 220 [23.08%] G-Loss: 23.7919 D-Loss: 2.5966 Loss-g-fm: 3.2500 Loss-g-mel: 15.8660 Loss-g-dur: 1.4746 Loss-g-kl: 1.4171 lr: 0.0002 grad_norm_g: 127.3572 grad_norm_d: 11.3239
======> Epoch: 220
Train Epoch: 221 [19.23%] G-Loss: 28.6095 D-Loss: 2.6529 Loss-g-fm: 2.9615 Loss-g-mel: 20.2266 Loss-g-dur: 1.9747 Loss-g-kl: 1.2496 lr: 0.0002 grad_norm_g: 117.6818 grad_norm_d: 16.5688
======> Epoch: 221
Train Epoch: 222 [15.38%] G-Loss: 26.8248 D-Loss: 2.8462 Loss-g-fm: 2.4572 Loss-g-mel: 19.2749 Loss-g-dur: 1.7733 Loss-g-kl: 1.3241 lr: 0.0002 grad_norm_g: 57.4163 grad_norm_d: 39.8168
======> Epoch: 222
Train Epoch: 223 [11.54%] G-Loss: 28.4111 D-Loss: 2.5575 Loss-g-fm: 3.0129 Loss-g-mel: 19.6911 Loss-g-dur: 1.9195 Loss-g-kl: 1.5168 lr: 0.0002 grad_norm_g: 31.0602 grad_norm_d: 7.0216
======> Epoch: 223
Train Epoch: 224 [7.69%] G-Loss: 27.8821 D-Loss: 2.6569 Loss-g-fm: 2.4963 Loss-g-mel: 20.3522 Loss-g-dur: 1.8551 Loss-g-kl: 1.2390 lr: 0.0002 grad_norm_g: 29.9311 grad_norm_d: 13.1705
======> Epoch: 224
Train Epoch: 225 [3.85%] G-Loss: 28.8924 D-Loss: 2.6152 Loss-g-fm: 2.8471 Loss-g-mel: 20.5807 Loss-g-dur: 1.9729 Loss-g-kl: 1.5654 lr: 0.0002 grad_norm_g: 33.3449 grad_norm_d: 7.5339
======> Epoch: 225
Train Epoch: 226 [0.00%] G-Loss: 26.5123 D-Loss: 2.8653 Loss-g-fm: 2.1801 Loss-g-mel: 19.3518 Loss-g-dur: 1.7958 Loss-g-kl: 1.0685 lr: 0.0002 grad_norm_g: 87.8886 grad_norm_d: 22.9558
Train Epoch: 226 [96.15%] G-Loss: 29.0175 D-Loss: 2.4989 Loss-g-fm: 3.4648 Loss-g-mel: 20.0957 Loss-g-dur: 1.8611 Loss-g-kl: 1.3756 lr: 0.0002 grad_norm_g: 34.3687 grad_norm_d: 7.4548
======> Epoch: 226
Train Epoch: 227 [92.31%] G-Loss: 27.3698 D-Loss: 2.7687 Loss-g-fm: 2.7288 Loss-g-mel: 19.7622 Loss-g-dur: 1.7907 Loss-g-kl: 1.2310 lr: 0.0002 grad_norm_g: 135.7415 grad_norm_d: 41.9822
======> Epoch: 227
Train Epoch: 228 [88.46%] G-Loss: 26.9929 D-Loss: 2.6615 Loss-g-fm: 2.5082 Loss-g-mel: 19.5309 Loss-g-dur: 1.9500 Loss-g-kl: 1.3324 lr: 0.0002 grad_norm_g: 79.2213 grad_norm_d: 22.8616
======> Epoch: 228
Train Epoch: 229 [84.62%] G-Loss: 25.9966 D-Loss: 2.6393 Loss-g-fm: 2.2476 Loss-g-mel: 18.8159 Loss-g-dur: 1.7645 Loss-g-kl: 1.2524 lr: 0.0002 grad_norm_g: 108.6009 grad_norm_d: 8.2184
======> Epoch: 229
Train Epoch: 230 [80.77%] G-Loss: 26.9361 D-Loss: 2.7219 Loss-g-fm: 2.5664 Loss-g-mel: 19.3284 Loss-g-dur: 1.9473 Loss-g-kl: 1.2689 lr: 0.0002 grad_norm_g: 32.6189 grad_norm_d: 8.8164
======> Epoch: 230
Train Epoch: 231 [76.92%] G-Loss: 24.0047 D-Loss: 2.5823 Loss-g-fm: 2.5040 Loss-g-mel: 16.6211 Loss-g-dur: 1.5493 Loss-g-kl: 0.9944 lr: 0.0002 grad_norm_g: 93.3411 grad_norm_d: 4.8753
Saving model and optimizer state at iteration 231 to /ZFS4T/tts/data/VITS/model_saved/G_6000.pth
Saving model and optimizer state at iteration 231 to /ZFS4T/tts/data/VITS/model_saved/D_6000.pth
======> Epoch: 231
Train Epoch: 232 [73.08%] G-Loss: 28.6468 D-Loss: 2.5899 Loss-g-fm: 2.7789 Loss-g-mel: 20.2198 Loss-g-dur: 1.9632 Loss-g-kl: 1.4123 lr: 0.0002 grad_norm_g: 69.2459 grad_norm_d: 12.6838
======> Epoch: 232
Train Epoch: 233 [69.23%] G-Loss: 25.9478 D-Loss: 2.5709 Loss-g-fm: 2.4181 Loss-g-mel: 18.6745 Loss-g-dur: 1.7924 Loss-g-kl: 1.1589 lr: 0.0002 grad_norm_g: 92.2044 grad_norm_d: 5.8437
======> Epoch: 233
Train Epoch: 234 [65.38%] G-Loss: 28.2153 D-Loss: 2.7137 Loss-g-fm: 2.8614 Loss-g-mel: 20.0149 Loss-g-dur: 1.8779 Loss-g-kl: 1.2819 lr: 0.0002 grad_norm_g: 143.5201 grad_norm_d: 34.3892
======> Epoch: 234
Train Epoch: 235 [61.54%] G-Loss: 27.0664 D-Loss: 2.5664 Loss-g-fm: 2.8007 Loss-g-mel: 19.5299 Loss-g-dur: 1.7600 Loss-g-kl: 1.2287 lr: 0.0002 grad_norm_g: 34.1890 grad_norm_d: 10.5328
======> Epoch: 235
Train Epoch: 236 [57.69%] G-Loss: 26.9837 D-Loss: 2.5860 Loss-g-fm: 2.7067 Loss-g-mel: 19.4784 Loss-g-dur: 1.7891 Loss-g-kl: 1.0416 lr: 0.0002 grad_norm_g: 25.1228 grad_norm_d: 6.9793
======> Epoch: 236
Train Epoch: 237 [53.85%] G-Loss: 27.9454 D-Loss: 2.4756 Loss-g-fm: 3.0072 Loss-g-mel: 19.6615 Loss-g-dur: 1.7480 Loss-g-kl: 1.3737 lr: 0.0002 grad_norm_g: 38.9011 grad_norm_d: 8.1621
======> Epoch: 237
Train Epoch: 238 [50.00%] G-Loss: 27.7672 D-Loss: 2.5128 Loss-g-fm: 3.0983 Loss-g-mel: 19.5641 Loss-g-dur: 1.7734 Loss-g-kl: 1.4099 lr: 0.0002 grad_norm_g: 56.3547 grad_norm_d: 6.8610
======> Epoch: 238
Train Epoch: 239 [46.15%] G-Loss: 27.0143 D-Loss: 2.5244 Loss-g-fm: 2.6478 Loss-g-mel: 19.4027 Loss-g-dur: 1.7547 Loss-g-kl: 1.1483 lr: 0.0002 grad_norm_g: 94.9200 grad_norm_d: 11.4439
======> Epoch: 239
Train Epoch: 240 [42.31%] G-Loss: 27.7456 D-Loss: 2.6010 Loss-g-fm: 2.6094 Loss-g-mel: 20.1177 Loss-g-dur: 1.8145 Loss-g-kl: 1.3303 lr: 0.0002 grad_norm_g: 79.5518 grad_norm_d: 8.1248
======> Epoch: 240
Train Epoch: 241 [38.46%] G-Loss: 28.4556 D-Loss: 2.6709 Loss-g-fm: 2.8787 Loss-g-mel: 20.3206 Loss-g-dur: 1.8102 Loss-g-kl: 1.3978 lr: 0.0002 grad_norm_g: 38.4989 grad_norm_d: 5.7183
======> Epoch: 241
Train Epoch: 242 [34.62%] G-Loss: 26.1768 D-Loss: 2.6448 Loss-g-fm: 2.4473 Loss-g-mel: 18.7597 Loss-g-dur: 1.8180 Loss-g-kl: 1.0671 lr: 0.0002 grad_norm_g: 45.1716 grad_norm_d: 13.2378
======> Epoch: 242
Train Epoch: 243 [30.77%] G-Loss: 26.5893 D-Loss: 2.7798 Loss-g-fm: 2.7854 Loss-g-mel: 19.1761 Loss-g-dur: 1.7321 Loss-g-kl: 1.0330 lr: 0.0002 grad_norm_g: 131.4958 grad_norm_d: 32.8940
======> Epoch: 243
Train Epoch: 244 [26.92%] G-Loss: 27.7378 D-Loss: 2.5490 Loss-g-fm: 3.0841 Loss-g-mel: 19.7141 Loss-g-dur: 1.7710 Loss-g-kl: 1.1393 lr: 0.0002 grad_norm_g: 84.5375 grad_norm_d: 11.8034
======> Epoch: 244
Train Epoch: 245 [23.08%] G-Loss: 25.8831 D-Loss: 2.5955 Loss-g-fm: 2.4678 Loss-g-mel: 18.4688 Loss-g-dur: 1.7506 Loss-g-kl: 1.2061 lr: 0.0002 grad_norm_g: 74.7835 grad_norm_d: 11.2217
======> Epoch: 245
Train Epoch: 246 [19.23%] G-Loss: 27.3502 D-Loss: 2.7839 Loss-g-fm: 2.4880 Loss-g-mel: 19.3160 Loss-g-dur: 1.9182 Loss-g-kl: 1.3807 lr: 0.0002 grad_norm_g: 140.5608 grad_norm_d: 17.8183
======> Epoch: 246
Train Epoch: 247 [15.38%] G-Loss: 26.2565 D-Loss: 2.5640 Loss-g-fm: 2.8373 Loss-g-mel: 18.6251 Loss-g-dur: 1.7560 Loss-g-kl: 1.1123 lr: 0.0002 grad_norm_g: 101.9552 grad_norm_d: 8.8622
======> Epoch: 247
Train Epoch: 248 [11.54%] G-Loss: 27.0488 D-Loss: 2.5735 Loss-g-fm: 2.8736 Loss-g-mel: 18.8800 Loss-g-dur: 1.8842 Loss-g-kl: 1.3851 lr: 0.0002 grad_norm_g: 91.5470 grad_norm_d: 11.0199
======> Epoch: 248
Train Epoch: 249 [7.69%] G-Loss: 27.2432 D-Loss: 2.6732 Loss-g-fm: 2.8689 Loss-g-mel: 19.1178 Loss-g-dur: 1.8572 Loss-g-kl: 1.2685 lr: 0.0002 grad_norm_g: 48.0390 grad_norm_d: 5.3331
======> Epoch: 249
Train Epoch: 250 [3.85%] G-Loss: 26.3459 D-Loss: 2.6035 Loss-g-fm: 2.6561 Loss-g-mel: 18.7095 Loss-g-dur: 1.8182 Loss-g-kl: 1.2777 lr: 0.0002 grad_norm_g: 58.6775 grad_norm_d: 7.8130
======> Epoch: 250
Train Epoch: 251 [0.00%] G-Loss: 27.0105 D-Loss: 2.6221 Loss-g-fm: 2.7555 Loss-g-mel: 19.1348 Loss-g-dur: 1.8126 Loss-g-kl: 1.3447 lr: 0.0002 grad_norm_g: 40.6493 grad_norm_d: 19.1495
Train Epoch: 251 [96.15%] G-Loss: 26.6444 D-Loss: 2.6749 Loss-g-fm: 2.3794 Loss-g-mel: 19.1516 Loss-g-dur: 1.7415 Loss-g-kl: 1.2793 lr: 0.0002 grad_norm_g: 70.8689 grad_norm_d: 7.5217
======> Epoch: 251
Train Epoch: 252 [92.31%] G-Loss: 26.5084 D-Loss: 2.7067 Loss-g-fm: 2.6143 Loss-g-mel: 18.8360 Loss-g-dur: 1.8068 Loss-g-kl: 1.2056 lr: 0.0002 grad_norm_g: 114.4699 grad_norm_d: 16.7980
======> Epoch: 252
Train Epoch: 253 [88.46%] G-Loss: 28.5125 D-Loss: 2.5433 Loss-g-fm: 3.1471 Loss-g-mel: 20.3717 Loss-g-dur: 1.8094 Loss-g-kl: 1.2696 lr: 0.0002 grad_norm_g: 20.6408 grad_norm_d: 6.1928
======> Epoch: 253
Train Epoch: 254 [84.62%] G-Loss: 29.0037 D-Loss: 2.5059 Loss-g-fm: 3.2815 Loss-g-mel: 20.5831 Loss-g-dur: 1.8558 Loss-g-kl: 1.4447 lr: 0.0002 grad_norm_g: 41.8378 grad_norm_d: 5.7649
======> Epoch: 254
Train Epoch: 255 [80.77%] G-Loss: 25.6404 D-Loss: 2.4906 Loss-g-fm: 3.1853 Loss-g-mel: 17.5486 Loss-g-dur: 1.5683 Loss-g-kl: 1.3555 lr: 0.0002 grad_norm_g: 36.3550 grad_norm_d: 4.9410
======> Epoch: 255
Train Epoch: 256 [76.92%] G-Loss: 26.3393 D-Loss: 2.5794 Loss-g-fm: 2.9860 Loss-g-mel: 18.2165 Loss-g-dur: 1.7638 Loss-g-kl: 1.3601 lr: 0.0002 grad_norm_g: 32.5960 grad_norm_d: 13.0026
======> Epoch: 256
Train Epoch: 257 [73.08%] G-Loss: 28.0763 D-Loss: 2.5506 Loss-g-fm: 3.1281 Loss-g-mel: 19.8852 Loss-g-dur: 1.8436 Loss-g-kl: 1.3431 lr: 0.0002 grad_norm_g: 30.0727 grad_norm_d: 6.3696
======> Epoch: 257
Train Epoch: 258 [69.23%] G-Loss: 27.1941 D-Loss: 2.5651 Loss-g-fm: 2.9461 Loss-g-mel: 19.0875 Loss-g-dur: 1.7873 Loss-g-kl: 1.3533 lr: 0.0002 grad_norm_g: 22.9332 grad_norm_d: 8.6126
======> Epoch: 258
Train Epoch: 259 [65.38%] G-Loss: 27.8815 D-Loss: 2.7062 Loss-g-fm: 3.0700 Loss-g-mel: 19.3460 Loss-g-dur: 1.9436 Loss-g-kl: 1.3603 lr: 0.0002 grad_norm_g: 39.8273 grad_norm_d: 11.7203
======> Epoch: 259
Train Epoch: 260 [61.54%] G-Loss: 28.5895 D-Loss: 2.5438 Loss-g-fm: 3.4182 Loss-g-mel: 20.0285 Loss-g-dur: 1.8496 Loss-g-kl: 1.2099 lr: 0.0002 grad_norm_g: 64.0480 grad_norm_d: 10.4097
======> Epoch: 260
Train Epoch: 261 [57.69%] G-Loss: 28.8342 D-Loss: 2.4925 Loss-g-fm: 3.3235 Loss-g-mel: 20.6080 Loss-g-dur: 1.7930 Loss-g-kl: 1.2035 lr: 0.0002 grad_norm_g: 34.2244 grad_norm_d: 15.2106
======> Epoch: 261
Train Epoch: 262 [53.85%] G-Loss: 28.2052 D-Loss: 2.5807 Loss-g-fm: 3.0042 Loss-g-mel: 20.0764 Loss-g-dur: 1.8636 Loss-g-kl: 1.2828 lr: 0.0002 grad_norm_g: 66.0714 grad_norm_d: 11.2168
======> Epoch: 262
Train Epoch: 263 [50.00%] G-Loss: 27.3035 D-Loss: 2.6559 Loss-g-fm: 2.7794 Loss-g-mel: 19.5874 Loss-g-dur: 1.8338 Loss-g-kl: 1.2522 lr: 0.0002 grad_norm_g: 88.1582 grad_norm_d: 22.7835
======> Epoch: 263
Train Epoch: 264 [46.15%] G-Loss: 28.8228 D-Loss: 2.5305 Loss-g-fm: 3.4181 Loss-g-mel: 19.9456 Loss-g-dur: 1.8425 Loss-g-kl: 1.4520 lr: 0.0002 grad_norm_g: 20.4476 grad_norm_d: 6.4985
======> Epoch: 264
Train Epoch: 265 [42.31%] G-Loss: 27.8060 D-Loss: 2.5832 Loss-g-fm: 3.0796 Loss-g-mel: 19.8047 Loss-g-dur: 1.8644 Loss-g-kl: 1.3059 lr: 0.0002 grad_norm_g: 68.7665 grad_norm_d: 13.3523
======> Epoch: 265
Train Epoch: 266 [38.46%] G-Loss: 27.5555 D-Loss: 2.5801 Loss-g-fm: 3.2997 Loss-g-mel: 19.4133 Loss-g-dur: 1.8012 Loss-g-kl: 1.1447 lr: 0.0002 grad_norm_g: 143.1710 grad_norm_d: 8.7381
======> Epoch: 266
Train Epoch: 267 [34.62%] G-Loss: 25.9889 D-Loss: 2.7503 Loss-g-fm: 2.7310 Loss-g-mel: 18.5631 Loss-g-dur: 1.7459 Loss-g-kl: 1.0664 lr: 0.0002 grad_norm_g: 55.1594 grad_norm_d: 36.6295
======> Epoch: 267
Train Epoch: 268 [30.77%] G-Loss: 26.9897 D-Loss: 2.7497 Loss-g-fm: 2.6151 Loss-g-mel: 19.3802 Loss-g-dur: 1.7200 Loss-g-kl: 1.3465 lr: 0.0002 grad_norm_g: 25.3489 grad_norm_d: 7.3062
======> Epoch: 268
Train Epoch: 269 [26.92%] G-Loss: 24.3498 D-Loss: 2.6549 Loss-g-fm: 4.3568 Loss-g-mel: 15.0743 Loss-g-dur: 1.2044 Loss-g-kl: 1.0912 lr: 0.0002 grad_norm_g: 82.7530 grad_norm_d: 14.8846
======> Epoch: 269
Train Epoch: 270 [23.08%] G-Loss: 27.8829 D-Loss: 2.6348 Loss-g-fm: 3.0247 Loss-g-mel: 19.7019 Loss-g-dur: 1.8411 Loss-g-kl: 1.4280 lr: 0.0002 grad_norm_g: 39.0342 grad_norm_d: 10.6818
======> Epoch: 270
Train Epoch: 271 [19.23%] G-Loss: 27.7239 D-Loss: 2.5886 Loss-g-fm: 3.1668 Loss-g-mel: 19.6838 Loss-g-dur: 1.7689 Loss-g-kl: 1.1650 lr: 0.0002 grad_norm_g: 42.9069 grad_norm_d: 8.1951
======> Epoch: 271
Train Epoch: 272 [15.38%] G-Loss: 22.0099 D-Loss: 2.6406 Loss-g-fm: 3.5354 Loss-g-mel: 13.7888 Loss-g-dur: 1.2624 Loss-g-kl: 1.2570 lr: 0.0002 grad_norm_g: 227.6564 grad_norm_d: 19.1153
======> Epoch: 272
Train Epoch: 273 [11.54%] G-Loss: 28.2112 D-Loss: 2.5363 Loss-g-fm: 3.1885 Loss-g-mel: 20.0146 Loss-g-dur: 1.7376 Loss-g-kl: 1.2062 lr: 0.0002 grad_norm_g: 42.9891 grad_norm_d: 7.2975
======> Epoch: 273
Train Epoch: 274 [7.69%] G-Loss: 27.1432 D-Loss: 2.5599 Loss-g-fm: 3.1709 Loss-g-mel: 18.9476 Loss-g-dur: 1.7459 Loss-g-kl: 1.1720 lr: 0.0002 grad_norm_g: 66.8755 grad_norm_d: 5.0522
======> Epoch: 274
Train Epoch: 275 [3.85%] G-Loss: 25.6008 D-Loss: 2.6438 Loss-g-fm: 2.6666 Loss-g-mel: 18.3358 Loss-g-dur: 1.7102 Loss-g-kl: 1.1293 lr: 0.0002 grad_norm_g: 32.8456 grad_norm_d: 4.7682
======> Epoch: 275
Train Epoch: 276 [0.00%] G-Loss: 27.4961 D-Loss: 2.5405 Loss-g-fm: 3.4058 Loss-g-mel: 19.2438 Loss-g-dur: 1.7134 Loss-g-kl: 1.1911 lr: 0.0002 grad_norm_g: 71.9070 grad_norm_d: 8.8759
Train Epoch: 276 [96.15%] G-Loss: 28.0724 D-Loss: 2.7772 Loss-g-fm: 3.0394 Loss-g-mel: 19.8084 Loss-g-dur: 1.8002 Loss-g-kl: 1.3327 lr: 0.0002 grad_norm_g: 90.3748 grad_norm_d: 34.1871
======> Epoch: 276
Train Epoch: 277 [92.31%] G-Loss: 29.2947 D-Loss: 2.4594 Loss-g-fm: 3.7655 Loss-g-mel: 20.1397 Loss-g-dur: 1.8129 Loss-g-kl: 1.3252 lr: 0.0002 grad_norm_g: 24.6686 grad_norm_d: 9.9210
======> Epoch: 277
Train Epoch: 278 [88.46%] G-Loss: 27.6427 D-Loss: 2.5705 Loss-g-fm: 3.1460 Loss-g-mel: 19.6750 Loss-g-dur: 1.7493 Loss-g-kl: 1.1267 lr: 0.0002 grad_norm_g: 43.7803 grad_norm_d: 8.8303
======> Epoch: 278
Train Epoch: 279 [84.62%] G-Loss: 26.9083 D-Loss: 2.6675 Loss-g-fm: 3.0881 Loss-g-mel: 18.7541 Loss-g-dur: 1.7227 Loss-g-kl: 1.1505 lr: 0.0002 grad_norm_g: 40.2294 grad_norm_d: 30.4545
======> Epoch: 279
Train Epoch: 280 [80.77%] G-Loss: 27.5707 D-Loss: 2.5165 Loss-g-fm: 3.0960 Loss-g-mel: 19.4427 Loss-g-dur: 1.7243 Loss-g-kl: 1.2711 lr: 0.0002 grad_norm_g: 108.4205 grad_norm_d: 7.9908
======> Epoch: 280
Train Epoch: 281 [76.92%] G-Loss: 28.2569 D-Loss: 2.6877 Loss-g-fm: 3.4766 Loss-g-mel: 19.5004 Loss-g-dur: 1.7485 Loss-g-kl: 1.2783 lr: 0.0002 grad_norm_g: 92.9482 grad_norm_d: 18.3884
======> Epoch: 281
Train Epoch: 282 [73.08%] G-Loss: 26.2819 D-Loss: 2.5324 Loss-g-fm: 3.1065 Loss-g-mel: 18.2457 Loss-g-dur: 1.7065 Loss-g-kl: 1.1481 lr: 0.0002 grad_norm_g: 27.7957 grad_norm_d: 7.8830
======> Epoch: 282
Train Epoch: 283 [69.23%] G-Loss: 25.1399 D-Loss: 2.6059 Loss-g-fm: 2.6117 Loss-g-mel: 17.7270 Loss-g-dur: 1.7213 Loss-g-kl: 1.0691 lr: 0.0002 grad_norm_g: 54.1872 grad_norm_d: 7.9020
======> Epoch: 283
Train Epoch: 284 [65.38%] G-Loss: 28.3118 D-Loss: 2.7315 Loss-g-fm: 3.0565 Loss-g-mel: 19.8600 Loss-g-dur: 1.8005 Loss-g-kl: 1.1172 lr: 0.0002 grad_norm_g: 69.8596 grad_norm_d: 26.5119
======> Epoch: 284
Train Epoch: 285 [61.54%] G-Loss: 28.4532 D-Loss: 2.6131 Loss-g-fm: 3.1066 Loss-g-mel: 20.0435 Loss-g-dur: 1.7994 Loss-g-kl: 1.4845 lr: 0.0002 grad_norm_g: 101.9667 grad_norm_d: 5.4238
======> Epoch: 285
Train Epoch: 286 [57.69%] G-Loss: 28.8652 D-Loss: 2.6729 Loss-g-fm: 3.5216 Loss-g-mel: 20.4430 Loss-g-dur: 1.8187 Loss-g-kl: 1.2309 lr: 0.0002 grad_norm_g: 26.3653 grad_norm_d: 14.1947
======> Epoch: 286
Train Epoch: 287 [53.85%] G-Loss: 27.5180 D-Loss: 2.5892 Loss-g-fm: 3.3661 Loss-g-mel: 19.2427 Loss-g-dur: 1.7347 Loss-g-kl: 1.1304 lr: 0.0002 grad_norm_g: 83.6300 grad_norm_d: 21.0028
======> Epoch: 287
Train Epoch: 288 [50.00%] G-Loss: 28.4509 D-Loss: 2.5501 Loss-g-fm: 3.5208 Loss-g-mel: 19.9181 Loss-g-dur: 1.6842 Loss-g-kl: 1.1778 lr: 0.0002 grad_norm_g: 40.0116 grad_norm_d: 6.1910
======> Epoch: 288
Train Epoch: 289 [46.15%] G-Loss: 26.6966 D-Loss: 2.6037 Loss-g-fm: 2.7828 Loss-g-mel: 19.1861 Loss-g-dur: 1.7708 Loss-g-kl: 1.1797 lr: 0.0002 grad_norm_g: 57.5109 grad_norm_d: 11.1316
======> Epoch: 289
Train Epoch: 290 [42.31%] G-Loss: 27.8020 D-Loss: 2.5068 Loss-g-fm: 3.4238 Loss-g-mel: 19.3611 Loss-g-dur: 1.7198 Loss-g-kl: 1.2261 lr: 0.0002 grad_norm_g: 39.1089 grad_norm_d: 8.5280
======> Epoch: 290
Train Epoch: 291 [38.46%] G-Loss: 27.1145 D-Loss: 2.7664 Loss-g-fm: 2.7205 Loss-g-mel: 19.1353 Loss-g-dur: 1.7174 Loss-g-kl: 1.2412 lr: 0.0002 grad_norm_g: 98.8307 grad_norm_d: 22.1253
======> Epoch: 291
Train Epoch: 292 [34.62%] G-Loss: 28.7984 D-Loss: 2.6312 Loss-g-fm: 3.3115 Loss-g-mel: 20.3536 Loss-g-dur: 1.8408 Loss-g-kl: 1.2305 lr: 0.0002 grad_norm_g: 52.2013 grad_norm_d: 3.4187
======> Epoch: 292
Train Epoch: 293 [30.77%] G-Loss: 27.7829 D-Loss: 2.5556 Loss-g-fm: 3.6502 Loss-g-mel: 19.0969 Loss-g-dur: 1.7835 Loss-g-kl: 1.2615 lr: 0.0002 grad_norm_g: 57.0490 grad_norm_d: 8.0198
======> Epoch: 293
Train Epoch: 294 [26.92%] G-Loss: 25.4435 D-Loss: 2.4697 Loss-g-fm: 3.3958 Loss-g-mel: 17.2308 Loss-g-dur: 1.5529 Loss-g-kl: 1.1606 lr: 0.0002 grad_norm_g: 43.6003 grad_norm_d: 10.4120
======> Epoch: 294
Train Epoch: 295 [23.08%] G-Loss: 27.7144 D-Loss: 2.4803 Loss-g-fm: 3.5055 Loss-g-mel: 19.3037 Loss-g-dur: 1.6887 Loss-g-kl: 1.1299 lr: 0.0002 grad_norm_g: 46.8754 grad_norm_d: 10.0289
======> Epoch: 295
Train Epoch: 296 [19.23%] G-Loss: 26.2262 D-Loss: 2.5924 Loss-g-fm: 3.1124 Loss-g-mel: 18.1368 Loss-g-dur: 1.7300 Loss-g-kl: 1.1966 lr: 0.0002 grad_norm_g: 66.4943 grad_norm_d: 12.8413
======> Epoch: 296
Train Epoch: 297 [15.38%] G-Loss: 27.8624 D-Loss: 2.6342 Loss-g-fm: 3.3382 Loss-g-mel: 19.3782 Loss-g-dur: 1.8414 Loss-g-kl: 1.3115 lr: 0.0002 grad_norm_g: 36.4068 grad_norm_d: 12.8061
======> Epoch: 297
Train Epoch: 298 [11.54%] G-Loss: 26.3476 D-Loss: 2.6574 Loss-g-fm: 2.9344 Loss-g-mel: 18.7687 Loss-g-dur: 1.7295 Loss-g-kl: 0.9809 lr: 0.0002 grad_norm_g: 32.0414 grad_norm_d: 10.4136
======> Epoch: 298
Train Epoch: 299 [7.69%] G-Loss: 27.8699 D-Loss: 2.6132 Loss-g-fm: 3.5055 Loss-g-mel: 19.5893 Loss-g-dur: 1.7346 Loss-g-kl: 1.0200 lr: 0.0002 grad_norm_g: 84.6328 grad_norm_d: 13.8806
======> Epoch: 299
Train Epoch: 300 [3.85%] G-Loss: 28.3301 D-Loss: 2.6219 Loss-g-fm: 3.3476 Loss-g-mel: 20.1967 Loss-g-dur: 1.7139 Loss-g-kl: 1.0395 lr: 0.0002 grad_norm_g: 51.2431 grad_norm_d: 10.0427
======> Epoch: 300
Train Epoch: 301 [0.00%] G-Loss: 29.7547 D-Loss: 2.4640 Loss-g-fm: 4.1897 Loss-g-mel: 20.2011 Loss-g-dur: 1.8281 Loss-g-kl: 1.2060 lr: 0.0002 grad_norm_g: 128.4925 grad_norm_d: 16.9027
Train Epoch: 301 [96.15%] G-Loss: 28.4203 D-Loss: 2.5761 Loss-g-fm: 3.3587 Loss-g-mel: 19.6759 Loss-g-dur: 1.7936 Loss-g-kl: 1.2472 lr: 0.0002 grad_norm_g: 27.7999 grad_norm_d: 4.5561
======> Epoch: 301
Train Epoch: 302 [92.31%] G-Loss: 27.5978 D-Loss: 2.7011 Loss-g-fm: 3.2985 Loss-g-mel: 19.1077 Loss-g-dur: 1.7834 Loss-g-kl: 1.2385 lr: 0.0002 grad_norm_g: 156.0257 grad_norm_d: 15.3561
======> Epoch: 302
Train Epoch: 303 [88.46%] G-Loss: 28.9316 D-Loss: 2.5773 Loss-g-fm: 3.5734 Loss-g-mel: 20.0124 Loss-g-dur: 1.8359 Loss-g-kl: 1.4069 lr: 0.0002 grad_norm_g: 127.2642 grad_norm_d: 7.8424
======> Epoch: 303
Train Epoch: 304 [84.62%] G-Loss: 28.8195 D-Loss: 2.4735 Loss-g-fm: 3.7630 Loss-g-mel: 19.7571 Loss-g-dur: 1.9210 Loss-g-kl: 1.4646 lr: 0.0002 grad_norm_g: 70.3086 grad_norm_d: 4.5318
======> Epoch: 304
Train Epoch: 305 [80.77%] G-Loss: 26.3193 D-Loss: 2.6550 Loss-g-fm: 3.3038 Loss-g-mel: 18.3402 Loss-g-dur: 1.7061 Loss-g-kl: 1.0889 lr: 0.0002 grad_norm_g: 112.9260 grad_norm_d: 11.5760
======> Epoch: 305
Train Epoch: 306 [76.92%] G-Loss: 28.7669 D-Loss: 2.4980 Loss-g-fm: 3.8769 Loss-g-mel: 19.6607 Loss-g-dur: 1.9128 Loss-g-kl: 1.3017 lr: 0.0002 grad_norm_g: 180.0488 grad_norm_d: 6.5090
======> Epoch: 306
Train Epoch: 307 [73.08%] G-Loss: 27.9627 D-Loss: 2.7332 Loss-g-fm: 3.2447 Loss-g-mel: 19.6122 Loss-g-dur: 1.7114 Loss-g-kl: 1.1348 lr: 0.0002 grad_norm_g: 187.1963 grad_norm_d: 26.8630
======> Epoch: 307
Train Epoch: 308 [69.23%] G-Loss: 26.7403 D-Loss: 2.5609 Loss-g-fm: 3.4421 Loss-g-mel: 18.0127 Loss-g-dur: 1.7157 Loss-g-kl: 1.2897 lr: 0.0002 grad_norm_g: 44.9641 grad_norm_d: 7.7240
Saving model and optimizer state at iteration 308 to /ZFS4T/tts/data/VITS/model_saved/G_8000.pth
Saving model and optimizer state at iteration 308 to /ZFS4T/tts/data/VITS/model_saved/D_8000.pth
======> Epoch: 308
Train Epoch: 309 [65.38%] G-Loss: 22.6598 D-Loss: 2.6885 Loss-g-fm: 3.8526 Loss-g-mel: 14.0278 Loss-g-dur: 1.2520 Loss-g-kl: 1.1131 lr: 0.0002 grad_norm_g: 77.2264 grad_norm_d: 8.8644
======> Epoch: 309
Train Epoch: 310 [61.54%] G-Loss: 24.4222 D-Loss: 2.4361 Loss-g-fm: 5.0780 Loss-g-mel: 14.2874 Loss-g-dur: 1.1924 Loss-g-kl: 1.1351 lr: 0.0002 grad_norm_g: 272.7329 grad_norm_d: 12.2008
======> Epoch: 310
Train Epoch: 311 [57.69%] G-Loss: 26.4906 D-Loss: 2.6351 Loss-g-fm: 3.1217 Loss-g-mel: 18.2664 Loss-g-dur: 1.7895 Loss-g-kl: 1.2187 lr: 0.0002 grad_norm_g: 62.4027 grad_norm_d: 14.1175
======> Epoch: 311
Train Epoch: 312 [53.85%] G-Loss: 24.4904 D-Loss: 2.4649 Loss-g-fm: 4.4777 Loss-g-mel: 14.9814 Loss-g-dur: 1.3006 Loss-g-kl: 1.1744 lr: 0.0002 grad_norm_g: 83.1374 grad_norm_d: 13.2740
======> Epoch: 312
Train Epoch: 313 [50.00%] G-Loss: 27.4058 D-Loss: 2.5495 Loss-g-fm: 3.3316 Loss-g-mel: 19.2896 Loss-g-dur: 1.7011 Loss-g-kl: 1.1188 lr: 0.0002 grad_norm_g: 48.9016 grad_norm_d: 6.1781
======> Epoch: 313
Train Epoch: 314 [46.15%] G-Loss: 26.9738 D-Loss: 2.6489 Loss-g-fm: 3.2898 Loss-g-mel: 18.6471 Loss-g-dur: 1.7767 Loss-g-kl: 1.3133 lr: 0.0002 grad_norm_g: 61.4766 grad_norm_d: 8.9042
======> Epoch: 314
Train Epoch: 315 [42.31%] G-Loss: 29.2571 D-Loss: 2.4935 Loss-g-fm: 3.9424 Loss-g-mel: 19.9582 Loss-g-dur: 1.8159 Loss-g-kl: 1.3755 lr: 0.0002 grad_norm_g: 51.3780 grad_norm_d: 7.3812
======> Epoch: 315
Train Epoch: 316 [38.46%] G-Loss: 28.7974 D-Loss: 2.4573 Loss-g-fm: 3.6434 Loss-g-mel: 19.7682 Loss-g-dur: 1.7991 Loss-g-kl: 1.3865 lr: 0.0002 grad_norm_g: 79.1527 grad_norm_d: 3.4399
======> Epoch: 316
Train Epoch: 317 [34.62%] G-Loss: 26.7495 D-Loss: 2.5883 Loss-g-fm: 3.0774 Loss-g-mel: 18.8165 Loss-g-dur: 1.7359 Loss-g-kl: 1.2456 lr: 0.0002 grad_norm_g: 76.7386 grad_norm_d: 6.8557
======> Epoch: 317
Train Epoch: 318 [30.77%] G-Loss: 28.6547 D-Loss: 2.5192 Loss-g-fm: 3.6988 Loss-g-mel: 19.8195 Loss-g-dur: 1.7666 Loss-g-kl: 1.3136 lr: 0.0002 grad_norm_g: 56.5344 grad_norm_d: 6.7412
======> Epoch: 318
Train Epoch: 319 [26.92%] G-Loss: 30.6228 D-Loss: 2.5497 Loss-g-fm: 4.2829 Loss-g-mel: 20.9724 Loss-g-dur: 1.7916 Loss-g-kl: 1.3040 lr: 0.0002 grad_norm_g: 36.2122 grad_norm_d: 5.9711
======> Epoch: 319
Train Epoch: 320 [23.08%] G-Loss: 27.6997 D-Loss: 2.6062 Loss-g-fm: 3.4546 Loss-g-mel: 19.4240 Loss-g-dur: 1.6684 Loss-g-kl: 1.0242 lr: 0.0002 grad_norm_g: 55.1173 grad_norm_d: 5.6033
======> Epoch: 320
Train Epoch: 321 [19.23%] G-Loss: 23.1496 D-Loss: 2.5875 Loss-g-fm: 4.4954 Loss-g-mel: 13.7233 Loss-g-dur: 1.1846 Loss-g-kl: 1.1434 lr: 0.0002 grad_norm_g: 78.0741 grad_norm_d: 17.2047
======> Epoch: 321
Train Epoch: 322 [15.38%] G-Loss: 28.7086 D-Loss: 2.5176 Loss-g-fm: 3.8691 Loss-g-mel: 19.7350 Loss-g-dur: 1.6938 Loss-g-kl: 1.3980 lr: 0.0002 grad_norm_g: 83.4149 grad_norm_d: 5.0201
======> Epoch: 322
Train Epoch: 323 [11.54%] G-Loss: 26.7741 D-Loss: 2.5920 Loss-g-fm: 3.3337 Loss-g-mel: 18.5680 Loss-g-dur: 1.7257 Loss-g-kl: 0.9306 lr: 0.0002 grad_norm_g: 64.9359 grad_norm_d: 8.1252
======> Epoch: 323
Train Epoch: 324 [7.69%] G-Loss: 27.4114 D-Loss: 2.5247 Loss-g-fm: 3.4369 Loss-g-mel: 19.0471 Loss-g-dur: 1.6547 Loss-g-kl: 1.2656 lr: 0.0002 grad_norm_g: 78.3984 grad_norm_d: 7.7869
======> Epoch: 324
Train Epoch: 325 [3.85%] G-Loss: 29.2992 D-Loss: 2.4624 Loss-g-fm: 3.9873 Loss-g-mel: 20.0262 Loss-g-dur: 1.6739 Loss-g-kl: 1.4889 lr: 0.0002 grad_norm_g: 136.8257 grad_norm_d: 3.9292
======> Epoch: 325
Train Epoch: 326 [0.00%] G-Loss: 27.2315 D-Loss: 2.5890 Loss-g-fm: 3.2952 Loss-g-mel: 19.0037 Loss-g-dur: 1.6516 Loss-g-kl: 1.2365 lr: 0.0002 grad_norm_g: 39.7078 grad_norm_d: 9.4542
Train Epoch: 326 [96.15%] G-Loss: 29.2439 D-Loss: 2.5966 Loss-g-fm: 3.9014 Loss-g-mel: 20.0912 Loss-g-dur: 1.8272 Loss-g-kl: 1.2877 lr: 0.0002 grad_norm_g: 84.7277 grad_norm_d: 13.1079
======> Epoch: 326
Train Epoch: 327 [92.31%] G-Loss: 27.3129 D-Loss: 2.5700 Loss-g-fm: 3.3040 Loss-g-mel: 18.8297 Loss-g-dur: 1.7840 Loss-g-kl: 1.2715 lr: 0.0002 grad_norm_g: 104.8739 grad_norm_d: 4.2708
======> Epoch: 327
Train Epoch: 328 [88.46%] G-Loss: 28.5780 D-Loss: 2.4663 Loss-g-fm: 3.9796 Loss-g-mel: 19.3925 Loss-g-dur: 1.6831 Loss-g-kl: 1.3550 lr: 0.0002 grad_norm_g: 31.3119 grad_norm_d: 8.5654
======> Epoch: 328
Train Epoch: 329 [84.62%] G-Loss: 27.3346 D-Loss: 2.5530 Loss-g-fm: 3.3738 Loss-g-mel: 18.8553 Loss-g-dur: 1.6671 Loss-g-kl: 1.2806 lr: 0.0002 grad_norm_g: 39.6608 grad_norm_d: 13.7260
======> Epoch: 329
Train Epoch: 330 [80.77%] G-Loss: 29.9060 D-Loss: 2.5594 Loss-g-fm: 3.7487 Loss-g-mel: 20.4005 Loss-g-dur: 1.8379 Loss-g-kl: 1.4641 lr: 0.0002 grad_norm_g: 172.6682 grad_norm_d: 14.4192
======> Epoch: 330
Train Epoch: 331 [76.92%] G-Loss: 27.1087 D-Loss: 2.5557 Loss-g-fm: 3.2462 Loss-g-mel: 18.8568 Loss-g-dur: 1.7675 Loss-g-kl: 1.2017 lr: 0.0002 grad_norm_g: 81.1150 grad_norm_d: 4.0705
======> Epoch: 331
Train Epoch: 332 [73.08%] G-Loss: 28.6007 D-Loss: 2.4139 Loss-g-fm: 3.9297 Loss-g-mel: 19.3943 Loss-g-dur: 1.7957 Loss-g-kl: 1.3853 lr: 0.0002 grad_norm_g: 174.2258 grad_norm_d: 4.7034
======> Epoch: 332
Train Epoch: 333 [69.23%] G-Loss: 28.6550 D-Loss: 2.5360 Loss-g-fm: 3.8417 Loss-g-mel: 19.6547 Loss-g-dur: 1.7244 Loss-g-kl: 1.1882 lr: 0.0002 grad_norm_g: 110.1292 grad_norm_d: 22.3571
======> Epoch: 333
Train Epoch: 334 [65.38%] G-Loss: 28.2776 D-Loss: 2.6836 Loss-g-fm: 3.6997 Loss-g-mel: 19.8049 Loss-g-dur: 1.7620 Loss-g-kl: 1.2067 lr: 0.0002 grad_norm_g: 121.1292 grad_norm_d: 16.8185
======> Epoch: 334
Train Epoch: 335 [61.54%] G-Loss: 27.6061 D-Loss: 2.6318 Loss-g-fm: 3.7122 Loss-g-mel: 18.9155 Loss-g-dur: 1.6719 Loss-g-kl: 1.2164 lr: 0.0002 grad_norm_g: 68.5358 grad_norm_d: 6.3176
======> Epoch: 335
Train Epoch: 336 [57.69%] G-Loss: 28.3059 D-Loss: 2.4860 Loss-g-fm: 3.8095 Loss-g-mel: 19.3026 Loss-g-dur: 1.7277 Loss-g-kl: 1.2625 lr: 0.0002 grad_norm_g: 39.3178 grad_norm_d: 20.1404
======> Epoch: 336
Train Epoch: 337 [53.85%] G-Loss: 22.5054 D-Loss: 2.5495 Loss-g-fm: 4.4186 Loss-g-mel: 13.3872 Loss-g-dur: 1.2361 Loss-g-kl: 1.1594 lr: 0.0002 grad_norm_g: 185.4197 grad_norm_d: 29.5660
======> Epoch: 337
Train Epoch: 338 [50.00%] G-Loss: 27.1732 D-Loss: 2.5626 Loss-g-fm: 3.2106 Loss-g-mel: 18.7095 Loss-g-dur: 1.7213 Loss-g-kl: 1.3533 lr: 0.0002 grad_norm_g: 81.2755 grad_norm_d: 14.4472
======> Epoch: 338
Train Epoch: 339 [46.15%] G-Loss: 29.0880 D-Loss: 2.5311 Loss-g-fm: 3.9485 Loss-g-mel: 19.8218 Loss-g-dur: 1.7775 Loss-g-kl: 1.3944 lr: 0.0002 grad_norm_g: 100.3201 grad_norm_d: 11.3434
======> Epoch: 339
Train Epoch: 340 [42.31%] G-Loss: 27.1549 D-Loss: 2.6095 Loss-g-fm: 3.5041 Loss-g-mel: 19.0338 Loss-g-dur: 1.6835 Loss-g-kl: 1.0683 lr: 0.0002 grad_norm_g: 54.7427 grad_norm_d: 19.4397
======> Epoch: 340
Train Epoch: 341 [38.46%] G-Loss: 28.8566 D-Loss: 2.5721 Loss-g-fm: 4.0146 Loss-g-mel: 19.5003 Loss-g-dur: 1.7864 Loss-g-kl: 1.3490 lr: 0.0002 grad_norm_g: 24.2686 grad_norm_d: 7.9301
======> Epoch: 341
Train Epoch: 342 [34.62%] G-Loss: 27.4377 D-Loss: 2.6054 Loss-g-fm: 3.3917 Loss-g-mel: 18.9096 Loss-g-dur: 1.7866 Loss-g-kl: 1.4221 lr: 0.0002 grad_norm_g: 28.9036 grad_norm_d: 15.1955
======> Epoch: 342
Train Epoch: 343 [30.77%] G-Loss: 29.6174 D-Loss: 2.4356 Loss-g-fm: 4.2196 Loss-g-mel: 19.8850 Loss-g-dur: 1.8707 Loss-g-kl: 1.4356 lr: 0.0002 grad_norm_g: 49.2965 grad_norm_d: 8.6252
======> Epoch: 343
Train Epoch: 344 [26.92%] G-Loss: 26.4160 D-Loss: 2.5727 Loss-g-fm: 3.2942 Loss-g-mel: 18.3139 Loss-g-dur: 1.6788 Loss-g-kl: 1.1298 lr: 0.0002 grad_norm_g: 154.2002 grad_norm_d: 15.2577
======> Epoch: 344
Train Epoch: 345 [23.08%] G-Loss: 28.1862 D-Loss: 2.4973 Loss-g-fm: 3.8663 Loss-g-mel: 19.1791 Loss-g-dur: 1.7302 Loss-g-kl: 1.2180 lr: 0.0002 grad_norm_g: 57.4531 grad_norm_d: 7.6156
======> Epoch: 345
Train Epoch: 346 [19.23%] G-Loss: 30.3011 D-Loss: 2.5393 Loss-g-fm: 4.0190 Loss-g-mel: 20.9426 Loss-g-dur: 1.7931 Loss-g-kl: 1.3983 lr: 0.0002 grad_norm_g: 167.1207 grad_norm_d: 5.6987
======> Epoch: 346
Train Epoch: 347 [15.38%] G-Loss: 31.1248 D-Loss: 2.5548 Loss-g-fm: 4.9140 Loss-g-mel: 20.6734 Loss-g-dur: 1.7801 Loss-g-kl: 1.4163 lr: 0.0002 grad_norm_g: 32.2191 grad_norm_d: 8.8097
======> Epoch: 347
Train Epoch: 348 [11.54%] G-Loss: 27.8069 D-Loss: 2.5322 Loss-g-fm: 3.8622 Loss-g-mel: 18.5678 Loss-g-dur: 1.6687 Loss-g-kl: 1.4133 lr: 0.0002 grad_norm_g: 57.4959 grad_norm_d: 4.4902
======> Epoch: 348
Train Epoch: 349 [7.69%] G-Loss: 27.3835 D-Loss: 2.5598 Loss-g-fm: 3.8107 Loss-g-mel: 18.5733 Loss-g-dur: 1.6568 Loss-g-kl: 1.2093 lr: 0.0002 grad_norm_g: 58.1920 grad_norm_d: 9.4162
======> Epoch: 349
Train Epoch: 350 [3.85%] G-Loss: 27.1247 D-Loss: 2.4898 Loss-g-fm: 3.7885 Loss-g-mel: 18.3559 Loss-g-dur: 1.6189 Loss-g-kl: 1.1237 lr: 0.0002 grad_norm_g: 75.0748 grad_norm_d: 9.4878
======> Epoch: 350
Train Epoch: 351 [0.00%] G-Loss: 27.9831 D-Loss: 2.5590 Loss-g-fm: 3.9540 Loss-g-mel: 19.0295 Loss-g-dur: 1.7553 Loss-g-kl: 1.3589 lr: 0.0002 grad_norm_g: 75.5416 grad_norm_d: 11.8118
Train Epoch: 351 [96.15%] G-Loss: 26.6642 D-Loss: 2.6506 Loss-g-fm: 3.3704 Loss-g-mel: 18.3463 Loss-g-dur: 1.7686 Loss-g-kl: 1.2563 lr: 0.0002 grad_norm_g: 92.7740 grad_norm_d: 9.1774
======> Epoch: 351
Train Epoch: 352 [92.31%] G-Loss: 29.5247 D-Loss: 2.4826 Loss-g-fm: 4.2378 Loss-g-mel: 19.9823 Loss-g-dur: 1.7562 Loss-g-kl: 1.3854 lr: 0.0002 grad_norm_g: 101.6790 grad_norm_d: 20.3825
======> Epoch: 352
Train Epoch: 353 [88.46%] G-Loss: 29.8952 D-Loss: 2.6265 Loss-g-fm: 4.4009 Loss-g-mel: 20.3122 Loss-g-dur: 1.7652 Loss-g-kl: 1.2845 lr: 0.0002 grad_norm_g: 106.6408 grad_norm_d: 8.8079
======> Epoch: 353
Train Epoch: 354 [84.62%] G-Loss: 28.4411 D-Loss: 2.7109 Loss-g-fm: 3.9171 Loss-g-mel: 19.4605 Loss-g-dur: 1.7230 Loss-g-kl: 1.2895 lr: 0.0002 grad_norm_g: 74.1658 grad_norm_d: 7.3279
======> Epoch: 354
Train Epoch: 355 [80.77%] G-Loss: 28.6304 D-Loss: 2.4680 Loss-g-fm: 4.1693 Loss-g-mel: 19.4199 Loss-g-dur: 1.7773 Loss-g-kl: 1.1729 lr: 0.0002 grad_norm_g: 114.5075 grad_norm_d: 7.3613
======> Epoch: 355
Train Epoch: 356 [76.92%] G-Loss: 28.0037 D-Loss: 2.5972 Loss-g-fm: 3.8103 Loss-g-mel: 19.2228 Loss-g-dur: 1.7803 Loss-g-kl: 1.3065 lr: 0.0002 grad_norm_g: 152.3308 grad_norm_d: 13.9092
======> Epoch: 356
Train Epoch: 357 [73.08%] G-Loss: 26.4028 D-Loss: 2.5809 Loss-g-fm: 3.4515 Loss-g-mel: 17.8828 Loss-g-dur: 1.6650 Loss-g-kl: 1.0706 lr: 0.0002 grad_norm_g: 67.2404 grad_norm_d: 10.1263
======> Epoch: 357
Train Epoch: 358 [69.23%] G-Loss: 29.1272 D-Loss: 2.4336 Loss-g-fm: 4.2328 Loss-g-mel: 19.6033 Loss-g-dur: 1.7810 Loss-g-kl: 1.2931 lr: 0.0002 grad_norm_g: 50.1924 grad_norm_d: 6.8484
======> Epoch: 358
Train Epoch: 359 [65.38%] G-Loss: 28.5382 D-Loss: 2.6929 Loss-g-fm: 3.5698 Loss-g-mel: 19.9776 Loss-g-dur: 1.6838 Loss-g-kl: 1.2946 lr: 0.0002 grad_norm_g: 142.1239 grad_norm_d: 20.1407
======> Epoch: 359
Train Epoch: 360 [61.54%] G-Loss: 27.9207 D-Loss: 2.4039 Loss-g-fm: 3.9539 Loss-g-mel: 19.0801 Loss-g-dur: 1.6619 Loss-g-kl: 1.2311 lr: 0.0002 grad_norm_g: 118.8833 grad_norm_d: 5.6193
======> Epoch: 360
Train Epoch: 361 [57.69%] G-Loss: 28.1171 D-Loss: 2.5080 Loss-g-fm: 3.9333 Loss-g-mel: 18.9191 Loss-g-dur: 1.6864 Loss-g-kl: 1.3834 lr: 0.0002 grad_norm_g: 229.6836 grad_norm_d: 23.0630
======> Epoch: 361
Train Epoch: 362 [53.85%] G-Loss: 27.5467 D-Loss: 2.7976 Loss-g-fm: 3.3144 Loss-g-mel: 18.5550 Loss-g-dur: 1.7050 Loss-g-kl: 1.3827 lr: 0.0002 grad_norm_g: 168.3618 grad_norm_d: 25.3323
======> Epoch: 362
Train Epoch: 363 [50.00%] G-Loss: 26.6791 D-Loss: 2.8716 Loss-g-fm: 3.3705 Loss-g-mel: 18.2130 Loss-g-dur: 1.6796 Loss-g-kl: 1.2335 lr: 0.0002 grad_norm_g: 108.7196 grad_norm_d: 32.8026
======> Epoch: 363
Train Epoch: 364 [46.15%] G-Loss: 27.8263 D-Loss: 2.6813 Loss-g-fm: 3.7569 Loss-g-mel: 18.9330 Loss-g-dur: 1.7188 Loss-g-kl: 1.4479 lr: 0.0002 grad_norm_g: 85.8119 grad_norm_d: 7.7898
======> Epoch: 364
Train Epoch: 365 [42.31%] G-Loss: 29.4807 D-Loss: 2.4966 Loss-g-fm: 4.1742 Loss-g-mel: 20.0577 Loss-g-dur: 1.7712 Loss-g-kl: 1.4200 lr: 0.0002 grad_norm_g: 86.2898 grad_norm_d: 8.6658
======> Epoch: 365
Train Epoch: 366 [38.46%] G-Loss: 29.5230 D-Loss: 2.4041 Loss-g-fm: 4.4432 Loss-g-mel: 19.8891 Loss-g-dur: 1.7439 Loss-g-kl: 1.2841 lr: 0.0002 grad_norm_g: 90.0030 grad_norm_d: 10.5340
======> Epoch: 366
Train Epoch: 367 [34.62%] G-Loss: 29.0432 D-Loss: 2.6093 Loss-g-fm: 4.1588 Loss-g-mel: 19.8103 Loss-g-dur: 1.7609 Loss-g-kl: 1.3099 lr: 0.0002 grad_norm_g: 220.5771 grad_norm_d: 19.5258
======> Epoch: 367
Train Epoch: 368 [30.77%] G-Loss: 29.2037 D-Loss: 2.5546 Loss-g-fm: 4.3354 Loss-g-mel: 19.8697 Loss-g-dur: 1.6963 Loss-g-kl: 1.2011 lr: 0.0002 grad_norm_g: 141.2788 grad_norm_d: 5.1965
======> Epoch: 368
Train Epoch: 369 [26.92%] G-Loss: 29.1473 D-Loss: 2.4133 Loss-g-fm: 4.4907 Loss-g-mel: 19.3860 Loss-g-dur: 1.7178 Loss-g-kl: 1.3688 lr: 0.0002 grad_norm_g: 90.5277 grad_norm_d: 10.6380
======> Epoch: 369
Train Epoch: 370 [23.08%] G-Loss: 28.0142 D-Loss: 2.4322 Loss-g-fm: 3.9426 Loss-g-mel: 19.2624 Loss-g-dur: 1.6716 Loss-g-kl: 1.0591 lr: 0.0002 grad_norm_g: 80.7082 grad_norm_d: 10.3596
======> Epoch: 370
Train Epoch: 371 [19.23%] G-Loss: 28.6125 D-Loss: 2.4309 Loss-g-fm: 4.3141 Loss-g-mel: 19.2418 Loss-g-dur: 1.6623 Loss-g-kl: 1.2673 lr: 0.0002 grad_norm_g: 34.9694 grad_norm_d: 7.8522
======> Epoch: 371
Train Epoch: 372 [15.38%] G-Loss: 26.7681 D-Loss: 2.3901 Loss-g-fm: 4.5603 Loss-g-mel: 17.3028 Loss-g-dur: 1.4545 Loss-g-kl: 1.2503 lr: 0.0002 grad_norm_g: 245.0594 grad_norm_d: 11.2390
======> Epoch: 372
Train Epoch: 373 [11.54%] G-Loss: 29.9271 D-Loss: 2.3178 Loss-g-fm: 4.7566 Loss-g-mel: 19.6061 Loss-g-dur: 1.7322 Loss-g-kl: 1.4672 lr: 0.0002 grad_norm_g: 227.1200 grad_norm_d: 5.0151
======> Epoch: 373
Train Epoch: 374 [7.69%] G-Loss: 24.0503 D-Loss: 2.4055 Loss-g-fm: 5.4774 Loss-g-mel: 13.4519 Loss-g-dur: 1.1779 Loss-g-kl: 1.3215 lr: 0.0002 grad_norm_g: 180.9670 grad_norm_d: 8.0438
======> Epoch: 374
Train Epoch: 375 [3.85%] G-Loss: 27.5153 D-Loss: 2.5404 Loss-g-fm: 3.9449 Loss-g-mel: 18.7200 Loss-g-dur: 1.6914 Loss-g-kl: 1.0841 lr: 0.0002 grad_norm_g: 29.2798 grad_norm_d: 9.5994
======> Epoch: 375
Train Epoch: 376 [0.00%] G-Loss: 28.3584 D-Loss: 2.4685 Loss-g-fm: 4.2779 Loss-g-mel: 18.9900 Loss-g-dur: 1.7198 Loss-g-kl: 1.2730 lr: 0.0002 grad_norm_g: 209.7469 grad_norm_d: 10.6324
Train Epoch: 376 [96.15%] G-Loss: 27.5358 D-Loss: 2.5922 Loss-g-fm: 3.7424 Loss-g-mel: 18.8832 Loss-g-dur: 1.6877 Loss-g-kl: 1.2649 lr: 0.0002 grad_norm_g: 39.3260 grad_norm_d: 9.1009
======> Epoch: 376
Train Epoch: 377 [92.31%] G-Loss: 29.2765 D-Loss: 2.4898 Loss-g-fm: 4.4336 Loss-g-mel: 19.5448 Loss-g-dur: 1.8141 Loss-g-kl: 1.3573 lr: 0.0002 grad_norm_g: 159.9972 grad_norm_d: 5.9583
======> Epoch: 377
Train Epoch: 378 [88.46%] G-Loss: 29.7583 D-Loss: 2.5026 Loss-g-fm: 4.2329 Loss-g-mel: 19.8012 Loss-g-dur: 1.8723 Loss-g-kl: 1.5040 lr: 0.0002 grad_norm_g: 49.2967 grad_norm_d: 5.1020
======> Epoch: 378
Train Epoch: 379 [84.62%] G-Loss: 27.2652 D-Loss: 2.6902 Loss-g-fm: 3.5536 Loss-g-mel: 19.1037 Loss-g-dur: 1.6148 Loss-g-kl: 1.1884 lr: 0.0002 grad_norm_g: 164.8918 grad_norm_d: 27.8921
======> Epoch: 379
Train Epoch: 380 [80.77%] G-Loss: 29.6026 D-Loss: 2.4876 Loss-g-fm: 4.5288 Loss-g-mel: 20.0603 Loss-g-dur: 1.7782 Loss-g-kl: 1.1424 lr: 0.0002 grad_norm_g: 28.4764 grad_norm_d: 5.7007
======> Epoch: 380
Train Epoch: 381 [76.92%] G-Loss: 29.7697 D-Loss: 2.5747 Loss-g-fm: 4.3306 Loss-g-mel: 20.0242 Loss-g-dur: 1.7221 Loss-g-kl: 1.3899 lr: 0.0002 grad_norm_g: 185.9501 grad_norm_d: 21.9059
======> Epoch: 381
Train Epoch: 382 [73.08%] G-Loss: 28.5975 D-Loss: 2.5442 Loss-g-fm: 4.0056 Loss-g-mel: 19.4227 Loss-g-dur: 1.6352 Loss-g-kl: 1.3840 lr: 0.0002 grad_norm_g: 209.1465 grad_norm_d: 16.4580
======> Epoch: 382
Train Epoch: 383 [69.23%] G-Loss: 27.1076 D-Loss: 2.5554 Loss-g-fm: 3.8013 Loss-g-mel: 18.5320 Loss-g-dur: 1.6681 Loss-g-kl: 1.0948 lr: 0.0002 grad_norm_g: 176.1706 grad_norm_d: 12.1531
======> Epoch: 383
Train Epoch: 384 [65.38%] G-Loss: 29.0589 D-Loss: 2.4779 Loss-g-fm: 4.4483 Loss-g-mel: 19.7363 Loss-g-dur: 1.7086 Loss-g-kl: 1.2006 lr: 0.0002 grad_norm_g: 290.8418 grad_norm_d: 20.1977
======> Epoch: 384
Train Epoch: 385 [61.54%] G-Loss: 24.8854 D-Loss: 2.3942 Loss-g-fm: 5.3883 Loss-g-mel: 14.6810 Loss-g-dur: 1.2289 Loss-g-kl: 1.1943 lr: 0.0002 grad_norm_g: 61.0103 grad_norm_d: 12.3212
Saving model and optimizer state at iteration 385 to /ZFS4T/tts/data/VITS/model_saved/G_10000.pth
Saving model and optimizer state at iteration 385 to /ZFS4T/tts/data/VITS/model_saved/D_10000.pth
======> Epoch: 385
Train Epoch: 386 [57.69%] G-Loss: 28.3975 D-Loss: 2.5053 Loss-g-fm: 4.3372 Loss-g-mel: 18.9722 Loss-g-dur: 1.7503 Loss-g-kl: 1.1385 lr: 0.0002 grad_norm_g: 383.0834 grad_norm_d: 10.4635
======> Epoch: 386
Train Epoch: 387 [53.85%] G-Loss: 29.4725 D-Loss: 2.5427 Loss-g-fm: 4.2667 Loss-g-mel: 19.9326 Loss-g-dur: 1.8237 Loss-g-kl: 1.4051 lr: 0.0002 grad_norm_g: 350.7672 grad_norm_d: 13.3298
======> Epoch: 387
Train Epoch: 388 [50.00%] G-Loss: 27.1686 D-Loss: 2.5554 Loss-g-fm: 3.6209 Loss-g-mel: 18.7306 Loss-g-dur: 1.6830 Loss-g-kl: 1.1012 lr: 0.0002 grad_norm_g: 123.8319 grad_norm_d: 13.8287
======> Epoch: 388
Train Epoch: 389 [46.15%] G-Loss: 27.9196 D-Loss: 2.6053 Loss-g-fm: 3.8180 Loss-g-mel: 18.9520 Loss-g-dur: 1.7428 Loss-g-kl: 1.5101 lr: 0.0002 grad_norm_g: 33.6124 grad_norm_d: 13.7106
======> Epoch: 389
Train Epoch: 390 [42.31%] G-Loss: 27.8209 D-Loss: 2.5976 Loss-g-fm: 3.9476 Loss-g-mel: 19.2299 Loss-g-dur: 1.6721 Loss-g-kl: 0.9959 lr: 0.0002 grad_norm_g: 97.2064 grad_norm_d: 11.5831
======> Epoch: 390
Train Epoch: 391 [38.46%] G-Loss: 24.4934 D-Loss: 2.5615 Loss-g-fm: 5.1995 Loss-g-mel: 14.1109 Loss-g-dur: 1.1352 Loss-g-kl: 1.2914 lr: 0.0002 grad_norm_g: 296.7011 grad_norm_d: 12.6870
======> Epoch: 391
Train Epoch: 392 [34.62%] G-Loss: 27.0779 D-Loss: 2.5583 Loss-g-fm: 3.7592 Loss-g-mel: 18.4779 Loss-g-dur: 1.6502 Loss-g-kl: 1.1053 lr: 0.0002 grad_norm_g: 175.3538 grad_norm_d: 4.3246
======> Epoch: 392
Train Epoch: 393 [30.77%] G-Loss: 30.1507 D-Loss: 2.5645 Loss-g-fm: 4.4376 Loss-g-mel: 20.4610 Loss-g-dur: 1.7686 Loss-g-kl: 1.3766 lr: 0.0002 grad_norm_g: 103.1207 grad_norm_d: 4.6811
======> Epoch: 393
Train Epoch: 394 [26.92%] G-Loss: 27.2002 D-Loss: 2.5970 Loss-g-fm: 3.8454 Loss-g-mel: 18.2909 Loss-g-dur: 1.6387 Loss-g-kl: 1.3175 lr: 0.0002 grad_norm_g: 170.8592 grad_norm_d: 23.7412
======> Epoch: 394
Train Epoch: 395 [23.08%] G-Loss: 29.1095 D-Loss: 2.5019 Loss-g-fm: 4.5435 Loss-g-mel: 19.4716 Loss-g-dur: 1.7120 Loss-g-kl: 1.1474 lr: 0.0002 grad_norm_g: 325.3553 grad_norm_d: 14.7727
======> Epoch: 395
Train Epoch: 396 [19.23%] G-Loss: 27.0341 D-Loss: 2.4430 Loss-g-fm: 4.1644 Loss-g-mel: 18.0635 Loss-g-dur: 1.6580 Loss-g-kl: 1.0264 lr: 0.0002 grad_norm_g: 60.5683 grad_norm_d: 5.3061
======> Epoch: 396
Train Epoch: 397 [15.38%] G-Loss: 27.8441 D-Loss: 2.5181 Loss-g-fm: 4.0226 Loss-g-mel: 19.0769 Loss-g-dur: 1.6221 Loss-g-kl: 1.1265 lr: 0.0002 grad_norm_g: 75.0506 grad_norm_d: 5.6353
======> Epoch: 397
Train Epoch: 398 [11.54%] G-Loss: 27.7898 D-Loss: 2.4794 Loss-g-fm: 4.1118 Loss-g-mel: 18.8964 Loss-g-dur: 1.6405 Loss-g-kl: 1.1381 lr: 0.0002 grad_norm_g: 31.7003 grad_norm_d: 12.6462
======> Epoch: 398
Train Epoch: 399 [7.69%] G-Loss: 28.6794 D-Loss: 2.4330 Loss-g-fm: 4.3911 Loss-g-mel: 19.2372 Loss-g-dur: 1.6588 Loss-g-kl: 1.1664 lr: 0.0002 grad_norm_g: 191.2550 grad_norm_d: 9.5174
======> Epoch: 399
Train Epoch: 400 [3.85%] G-Loss: 23.5091 D-Loss: 2.6851 Loss-g-fm: 4.9200 Loss-g-mel: 13.7074 Loss-g-dur: 1.1533 Loss-g-kl: 1.2742 lr: 0.0002 grad_norm_g: 68.1021 grad_norm_d: 18.8279
======> Epoch: 400
Train Epoch: 401 [0.00%] G-Loss: 28.8416 D-Loss: 2.4390 Loss-g-fm: 4.4775 Loss-g-mel: 19.6211 Loss-g-dur: 1.6716 Loss-g-kl: 1.1057 lr: 0.0002 grad_norm_g: 75.3925 grad_norm_d: 9.5705
Train Epoch: 401 [96.15%] G-Loss: 23.6845 D-Loss: 2.5583 Loss-g-fm: 4.8821 Loss-g-mel: 14.2927 Loss-g-dur: 1.1599 Loss-g-kl: 1.3075 lr: 0.0002 grad_norm_g: 109.6720 grad_norm_d: 13.9263
======> Epoch: 401
Train Epoch: 402 [92.31%] G-Loss: 28.6328 D-Loss: 2.5054 Loss-g-fm: 4.4830 Loss-g-mel: 19.4445 Loss-g-dur: 1.6808 Loss-g-kl: 1.1669 lr: 0.0002 grad_norm_g: 70.8897 grad_norm_d: 7.7074
======> Epoch: 402
Train Epoch: 403 [88.46%] G-Loss: 28.1347 D-Loss: 2.7222 Loss-g-fm: 4.0242 Loss-g-mel: 19.1068 Loss-g-dur: 1.7251 Loss-g-kl: 1.2597 lr: 0.0002 grad_norm_g: 60.7995 grad_norm_d: 7.5203
======> Epoch: 403
Train Epoch: 404 [84.62%] G-Loss: 27.7295 D-Loss: 2.5268 Loss-g-fm: 4.2226 Loss-g-mel: 18.6184 Loss-g-dur: 1.6391 Loss-g-kl: 1.1363 lr: 0.0002 grad_norm_g: 303.8479 grad_norm_d: 13.2598
======> Epoch: 404
Train Epoch: 405 [80.77%] G-Loss: 29.1280 D-Loss: 2.3731 Loss-g-fm: 4.6997 Loss-g-mel: 18.9514 Loss-g-dur: 1.8020 Loss-g-kl: 1.3990 lr: 0.0002 grad_norm_g: 291.6682 grad_norm_d: 15.8341
======> Epoch: 405
Train Epoch: 406 [76.92%] G-Loss: 29.2161 D-Loss: 2.5288 Loss-g-fm: 4.6191 Loss-g-mel: 19.2442 Loss-g-dur: 1.7460 Loss-g-kl: 1.4000 lr: 0.0002 grad_norm_g: 203.9766 grad_norm_d: 8.3835
======> Epoch: 406
Train Epoch: 407 [73.08%] G-Loss: 24.2872 D-Loss: 2.2911 Loss-g-fm: 5.7537 Loss-g-mel: 13.4609 Loss-g-dur: 1.1950 Loss-g-kl: 1.1216 lr: 0.0002 grad_norm_g: 734.3866 grad_norm_d: 18.0038
======> Epoch: 407
Train Epoch: 408 [69.23%] G-Loss: 29.9200 D-Loss: 2.3607 Loss-g-fm: 4.7828 Loss-g-mel: 19.8361 Loss-g-dur: 1.7569 Loss-g-kl: 1.3611 lr: 0.0002 grad_norm_g: 154.3483 grad_norm_d: 5.4854
======> Epoch: 408
Train Epoch: 409 [65.38%] G-Loss: 29.3632 D-Loss: 2.4685 Loss-g-fm: 4.4500 Loss-g-mel: 19.6345 Loss-g-dur: 1.7440 Loss-g-kl: 1.3023 lr: 0.0002 grad_norm_g: 249.9337 grad_norm_d: 8.9851
======> Epoch: 409
Train Epoch: 410 [61.54%] G-Loss: 28.3295 D-Loss: 2.5283 Loss-g-fm: 4.2735 Loss-g-mel: 18.9172 Loss-g-dur: 1.7510 Loss-g-kl: 1.4782 lr: 0.0002 grad_norm_g: 286.3810 grad_norm_d: 10.4113
======> Epoch: 410
Train Epoch: 411 [57.69%] G-Loss: 30.2992 D-Loss: 2.6897 Loss-g-fm: 4.5711 Loss-g-mel: 19.9181 Loss-g-dur: 1.9416 Loss-g-kl: 1.5058 lr: 0.0002 grad_norm_g: 196.4800 grad_norm_d: 21.7353
======> Epoch: 411
Train Epoch: 412 [53.85%] G-Loss: 25.2950 D-Loss: 2.5006 Loss-g-fm: 6.0055 Loss-g-mel: 14.0483 Loss-g-dur: 1.1844 Loss-g-kl: 1.2758 lr: 0.0002 grad_norm_g: 67.6855 grad_norm_d: 16.9571
======> Epoch: 412
Train Epoch: 413 [50.00%] G-Loss: 27.8020 D-Loss: 2.3873 Loss-g-fm: 5.1363 Loss-g-mel: 17.7373 Loss-g-dur: 1.4176 Loss-g-kl: 1.3054 lr: 0.0002 grad_norm_g: 326.3024 grad_norm_d: 15.2124
======> Epoch: 413
Train Epoch: 414 [46.15%] G-Loss: 28.0629 D-Loss: 2.5216 Loss-g-fm: 4.3913 Loss-g-mel: 18.6180 Loss-g-dur: 1.6704 Loss-g-kl: 1.1870 lr: 0.0002 grad_norm_g: 407.7597 grad_norm_d: 12.2635
======> Epoch: 414
Train Epoch: 415 [42.31%] G-Loss: 28.0438 D-Loss: 2.5307 Loss-g-fm: 4.0451 Loss-g-mel: 19.0288 Loss-g-dur: 1.6406 Loss-g-kl: 1.1376 lr: 0.0002 grad_norm_g: 74.2972 grad_norm_d: 13.0678
======> Epoch: 415
Train Epoch: 416 [38.46%] G-Loss: 28.9858 D-Loss: 2.5055 Loss-g-fm: 4.8896 Loss-g-mel: 18.9440 Loss-g-dur: 1.6378 Loss-g-kl: 1.1364 lr: 0.0002 grad_norm_g: 425.8155 grad_norm_d: 4.4610
======> Epoch: 416
Train Epoch: 417 [34.62%] G-Loss: 27.9444 D-Loss: 2.5791 Loss-g-fm: 4.0835 Loss-g-mel: 18.6994 Loss-g-dur: 1.6178 Loss-g-kl: 1.3077 lr: 0.0002 grad_norm_g: 800.6182 grad_norm_d: 31.7064
======> Epoch: 417
Train Epoch: 418 [30.77%] G-Loss: 28.6758 D-Loss: 2.5650 Loss-g-fm: 4.3974 Loss-g-mel: 19.3738 Loss-g-dur: 1.6472 Loss-g-kl: 1.2792 lr: 0.0002 grad_norm_g: 72.4534 grad_norm_d: 26.7638
======> Epoch: 418
Train Epoch: 419 [26.92%] G-Loss: 29.4512 D-Loss: 2.4602 Loss-g-fm: 4.5406 Loss-g-mel: 19.3623 Loss-g-dur: 1.7317 Loss-g-kl: 1.4375 lr: 0.0002 grad_norm_g: 114.0561 grad_norm_d: 9.9775
======> Epoch: 419
Train Epoch: 420 [23.08%] G-Loss: 29.3543 D-Loss: 2.1752 Loss-g-fm: 5.8537 Loss-g-mel: 18.3110 Loss-g-dur: 1.4235 Loss-g-kl: 1.3433 lr: 0.0002 grad_norm_g: 332.4612 grad_norm_d: 10.8615
======> Epoch: 420
Train Epoch: 421 [19.23%] G-Loss: 27.4290 D-Loss: 2.5217 Loss-g-fm: 4.1614 Loss-g-mel: 18.1113 Loss-g-dur: 1.6123 Loss-g-kl: 1.4020 lr: 0.0002 grad_norm_g: 204.0244 grad_norm_d: 9.5184
======> Epoch: 421
Train Epoch: 422 [15.38%] G-Loss: 26.9527 D-Loss: 2.6046 Loss-g-fm: 3.9309 Loss-g-mel: 18.2844 Loss-g-dur: 1.6682 Loss-g-kl: 1.1904 lr: 0.0002 grad_norm_g: 244.0592 grad_norm_d: 17.0874
======> Epoch: 422
Train Epoch: 423 [11.54%] G-Loss: 28.4527 D-Loss: 2.5484 Loss-g-fm: 4.3045 Loss-g-mel: 18.9502 Loss-g-dur: 1.7349 Loss-g-kl: 1.2590 lr: 0.0002 grad_norm_g: 89.4563 grad_norm_d: 15.1674
======> Epoch: 423
Train Epoch: 424 [7.69%] G-Loss: 28.4374 D-Loss: 2.5655 Loss-g-fm: 4.4557 Loss-g-mel: 19.3691 Loss-g-dur: 1.6285 Loss-g-kl: 0.9879 lr: 0.0002 grad_norm_g: 391.0304 grad_norm_d: 14.2869
======> Epoch: 424
Train Epoch: 425 [3.85%] G-Loss: 29.5065 D-Loss: 2.5294 Loss-g-fm: 4.5967 Loss-g-mel: 19.6310 Loss-g-dur: 1.7320 Loss-g-kl: 1.4373 lr: 0.0002 grad_norm_g: 151.9258 grad_norm_d: 8.7496
======> Epoch: 425
Train Epoch: 426 [0.00%] G-Loss: 28.2256 D-Loss: 2.4773 Loss-g-fm: 4.5762 Loss-g-mel: 18.8993 Loss-g-dur: 1.6535 Loss-g-kl: 0.9744 lr: 0.0002 grad_norm_g: 416.9924 grad_norm_d: 15.6958
Train Epoch: 426 [96.15%] G-Loss: 27.8475 D-Loss: 2.6868 Loss-g-fm: 3.8801 Loss-g-mel: 18.7240 Loss-g-dur: 1.6352 Loss-g-kl: 1.3848 lr: 0.0002 grad_norm_g: 119.6366 grad_norm_d: 11.3244
======> Epoch: 426
Train Epoch: 427 [92.31%] G-Loss: 29.9704 D-Loss: 2.4614 Loss-g-fm: 5.0598 Loss-g-mel: 19.8461 Loss-g-dur: 1.6619 Loss-g-kl: 1.3676 lr: 0.0002 grad_norm_g: 303.5524 grad_norm_d: 10.3557
======> Epoch: 427
Train Epoch: 428 [88.46%] G-Loss: 27.8810 D-Loss: 2.4744 Loss-g-fm: 4.0864 Loss-g-mel: 18.4736 Loss-g-dur: 1.6672 Loss-g-kl: 1.2308 lr: 0.0002 grad_norm_g: 291.5717 grad_norm_d: 17.0291
======> Epoch: 428
Train Epoch: 429 [84.62%] G-Loss: 27.2751 D-Loss: 2.6094 Loss-g-fm: 3.9661 Loss-g-mel: 18.3505 Loss-g-dur: 1.6173 Loss-g-kl: 1.2880 lr: 0.0002 grad_norm_g: 215.2898 grad_norm_d: 14.3398
======> Epoch: 429
Train Epoch: 430 [80.77%] G-Loss: 27.0807 D-Loss: 2.4836 Loss-g-fm: 4.2020 Loss-g-mel: 18.3417 Loss-g-dur: 1.5924 Loss-g-kl: 1.0384 lr: 0.0002 grad_norm_g: 103.3428 grad_norm_d: 7.1963
======> Epoch: 430
Train Epoch: 431 [76.92%] G-Loss: 28.3144 D-Loss: 2.5617 Loss-g-fm: 4.1923 Loss-g-mel: 19.1592 Loss-g-dur: 1.6302 Loss-g-kl: 1.2092 lr: 0.0002 grad_norm_g: 30.5780 grad_norm_d: 6.2914
======> Epoch: 431
Train Epoch: 432 [73.08%] G-Loss: 29.9368 D-Loss: 2.4593 Loss-g-fm: 5.1849 Loss-g-mel: 18.9718 Loss-g-dur: 1.7804 Loss-g-kl: 1.3857 lr: 0.0002 grad_norm_g: 334.4878 grad_norm_d: 18.2017
======> Epoch: 432
Train Epoch: 433 [69.23%] G-Loss: 30.7678 D-Loss: 2.3357 Loss-g-fm: 5.0977 Loss-g-mel: 20.4609 Loss-g-dur: 1.7412 Loss-g-kl: 1.2429 lr: 0.0002 grad_norm_g: 316.3947 grad_norm_d: 15.0476
======> Epoch: 433
Train Epoch: 434 [65.38%] G-Loss: 29.2644 D-Loss: 2.3957 Loss-g-fm: 4.8498 Loss-g-mel: 19.0222 Loss-g-dur: 1.8145 Loss-g-kl: 1.4142 lr: 0.0002 grad_norm_g: 638.1671 grad_norm_d: 24.3879
======> Epoch: 434
Train Epoch: 435 [61.54%] G-Loss: 27.6398 D-Loss: 2.4480 Loss-g-fm: 4.3114 Loss-g-mel: 18.3638 Loss-g-dur: 1.6181 Loss-g-kl: 1.1450 lr: 0.0002 grad_norm_g: 548.1048 grad_norm_d: 34.1820
======> Epoch: 435
Train Epoch: 436 [57.69%] G-Loss: 29.2110 D-Loss: 2.4555 Loss-g-fm: 4.8457 Loss-g-mel: 18.9328 Loss-g-dur: 1.7623 Loss-g-kl: 1.2060 lr: 0.0002 grad_norm_g: 628.3369 grad_norm_d: 19.8791
======> Epoch: 436
Train Epoch: 437 [53.85%] G-Loss: 29.2240 D-Loss: 2.3913 Loss-g-fm: 4.9822 Loss-g-mel: 19.2849 Loss-g-dur: 1.5825 Loss-g-kl: 1.1862 lr: 0.0002 grad_norm_g: 400.0498 grad_norm_d: 15.9852
======> Epoch: 437
Train Epoch: 438 [50.00%] G-Loss: 28.1546 D-Loss: 2.4260 Loss-g-fm: 4.4904 Loss-g-mel: 18.6406 Loss-g-dur: 1.6194 Loss-g-kl: 1.3031 lr: 0.0002 grad_norm_g: 617.5371 grad_norm_d: 28.4963
======> Epoch: 438
Train Epoch: 439 [46.15%] G-Loss: 29.2330 D-Loss: 2.5075 Loss-g-fm: 4.9051 Loss-g-mel: 19.2538 Loss-g-dur: 1.6012 Loss-g-kl: 1.3496 lr: 0.0002 grad_norm_g: 432.6144 grad_norm_d: 17.5722
======> Epoch: 439
Train Epoch: 440 [42.31%] G-Loss: 29.0869 D-Loss: 2.4154 Loss-g-fm: 5.1385 Loss-g-mel: 18.9244 Loss-g-dur: 1.5997 Loss-g-kl: 1.1494 lr: 0.0002 grad_norm_g: 388.9333 grad_norm_d: 13.8578
======> Epoch: 440
Train Epoch: 441 [38.46%] G-Loss: 30.2328 D-Loss: 2.3761 Loss-g-fm: 5.2934 Loss-g-mel: 19.9351 Loss-g-dur: 1.7098 Loss-g-kl: 1.2269 lr: 0.0002 grad_norm_g: 398.1879 grad_norm_d: 13.6884
======> Epoch: 441
Train Epoch: 442 [34.62%] G-Loss: 29.0890 D-Loss: 2.5885 Loss-g-fm: 4.3797 Loss-g-mel: 19.9635 Loss-g-dur: 1.6248 Loss-g-kl: 1.1311 lr: 0.0002 grad_norm_g: 502.5688 grad_norm_d: 22.1066
======> Epoch: 442
Train Epoch: 443 [30.77%] G-Loss: 27.9484 D-Loss: 2.4392 Loss-g-fm: 4.5353 Loss-g-mel: 18.5081 Loss-g-dur: 1.6444 Loss-g-kl: 1.0390 lr: 0.0002 grad_norm_g: 135.4228 grad_norm_d: 9.7813
======> Epoch: 443
Train Epoch: 444 [26.92%] G-Loss: 30.9583 D-Loss: 2.4377 Loss-g-fm: 5.3389 Loss-g-mel: 19.9283 Loss-g-dur: 1.7299 Loss-g-kl: 1.4539 lr: 0.0002 grad_norm_g: 482.4173 grad_norm_d: 17.3999
======> Epoch: 444
Train Epoch: 445 [23.08%] G-Loss: 22.0890 D-Loss: 2.3839 Loss-g-fm: 4.9633 Loss-g-mel: 12.3856 Loss-g-dur: 1.1030 Loss-g-kl: 1.2721 lr: 0.0002 grad_norm_g: 602.7396 grad_norm_d: 18.3168
======> Epoch: 445
Train Epoch: 446 [19.23%] G-Loss: 24.4295 D-Loss: 2.4919 Loss-g-fm: 6.2692 Loss-g-mel: 12.9425 Loss-g-dur: 1.1339 Loss-g-kl: 1.2067 lr: 0.0002 grad_norm_g: 506.7214 grad_norm_d: 16.5981
======> Epoch: 446
Train Epoch: 447 [15.38%] G-Loss: 22.8013 D-Loss: 2.5851 Loss-g-fm: 5.4751 Loss-g-mel: 12.7579 Loss-g-dur: 1.0961 Loss-g-kl: 1.2428 lr: 0.0002 grad_norm_g: 406.1926 grad_norm_d: 20.8176
======> Epoch: 447
Train Epoch: 448 [11.54%] G-Loss: 30.6818 D-Loss: 2.5567 Loss-g-fm: 5.1922 Loss-g-mel: 19.7786 Loss-g-dur: 1.6570 Loss-g-kl: 1.3142 lr: 0.0002 grad_norm_g: 96.5427 grad_norm_d: 29.3117
======> Epoch: 448
Train Epoch: 449 [7.69%] G-Loss: 27.9693 D-Loss: 2.4350 Loss-g-fm: 4.5545 Loss-g-mel: 18.5509 Loss-g-dur: 1.6036 Loss-g-kl: 1.1282 lr: 0.0002 grad_norm_g: 278.0478 grad_norm_d: 8.4899
======> Epoch: 449
Train Epoch: 450 [3.85%] G-Loss: 28.1249 D-Loss: 2.3957 Loss-g-fm: 4.8090 Loss-g-mel: 18.2390 Loss-g-dur: 1.6097 Loss-g-kl: 1.1979 lr: 0.0002 grad_norm_g: 711.8023 grad_norm_d: 18.0708
======> Epoch: 450
Train Epoch: 451 [0.00%] G-Loss: 29.8547 D-Loss: 2.3858 Loss-g-fm: 5.2065 Loss-g-mel: 19.7159 Loss-g-dur: 1.6019 Loss-g-kl: 1.0086 lr: 0.0002 grad_norm_g: 675.5061 grad_norm_d: 24.4513
Train Epoch: 451 [96.15%] G-Loss: 28.9865 D-Loss: 2.5998 Loss-g-fm: 4.8207 Loss-g-mel: 19.0343 Loss-g-dur: 1.6052 Loss-g-kl: 1.1590 lr: 0.0002 grad_norm_g: 390.3489 grad_norm_d: 33.4496
======> Epoch: 451
Train Epoch: 452 [92.31%] G-Loss: 26.0119 D-Loss: 2.3824 Loss-g-fm: 4.5505 Loss-g-mel: 16.7058 Loss-g-dur: 1.3828 Loss-g-kl: 1.1395 lr: 0.0002 grad_norm_g: 100.1407 grad_norm_d: 9.6277
======> Epoch: 452
Train Epoch: 453 [88.46%] G-Loss: 27.6176 D-Loss: 2.4760 Loss-g-fm: 4.5380 Loss-g-mel: 18.4907 Loss-g-dur: 1.5795 Loss-g-kl: 1.0841 lr: 0.0002 grad_norm_g: 222.2095 grad_norm_d: 14.9453
======> Epoch: 453
Train Epoch: 454 [84.62%] G-Loss: 30.0550 D-Loss: 2.2824 Loss-g-fm: 5.2961 Loss-g-mel: 19.7932 Loss-g-dur: 1.6502 Loss-g-kl: 1.2694 lr: 0.0002 grad_norm_g: 413.8473 grad_norm_d: 13.7865
======> Epoch: 454
Train Epoch: 455 [80.77%] G-Loss: 30.0404 D-Loss: 2.4533 Loss-g-fm: 4.8214 Loss-g-mel: 19.7409 Loss-g-dur: 1.7059 Loss-g-kl: 1.3013 lr: 0.0002 grad_norm_g: 500.3874 grad_norm_d: 19.0487
======> Epoch: 455
Train Epoch: 456 [76.92%] G-Loss: 29.4551 D-Loss: 2.4608 Loss-g-fm: 4.8133 Loss-g-mel: 19.0888 Loss-g-dur: 1.7069 Loss-g-kl: 1.2888 lr: 0.0002 grad_norm_g: 549.3356 grad_norm_d: 16.9722
======> Epoch: 456
Train Epoch: 457 [73.08%] G-Loss: 31.1498 D-Loss: 2.5119 Loss-g-fm: 5.3350 Loss-g-mel: 20.3752 Loss-g-dur: 1.7378 Loss-g-kl: 1.4611 lr: 0.0002 grad_norm_g: 203.7324 grad_norm_d: 21.8090
======> Epoch: 457
Train Epoch: 458 [69.23%] G-Loss: 30.9337 D-Loss: 2.3482 Loss-g-fm: 5.7604 Loss-g-mel: 19.8806 Loss-g-dur: 1.6316 Loss-g-kl: 1.1983 lr: 0.0002 grad_norm_g: 652.2653 grad_norm_d: 29.1452
======> Epoch: 458
Train Epoch: 459 [65.38%] G-Loss: 26.5272 D-Loss: 2.4483 Loss-g-fm: 4.5242 Loss-g-mel: 17.3282 Loss-g-dur: 1.6023 Loss-g-kl: 0.9139 lr: 0.0002 grad_norm_g: 173.8060 grad_norm_d: 19.1373
======> Epoch: 459
Train Epoch: 460 [61.54%] G-Loss: 27.9590 D-Loss: 2.4704 Loss-g-fm: 4.6923 Loss-g-mel: 18.5611 Loss-g-dur: 1.6281 Loss-g-kl: 1.0245 lr: 0.0002 grad_norm_g: 719.6875 grad_norm_d: 30.2686
======> Epoch: 460
Train Epoch: 461 [57.69%] G-Loss: 29.4780 D-Loss: 2.4604 Loss-g-fm: 4.8108 Loss-g-mel: 19.2438 Loss-g-dur: 1.7187 Loss-g-kl: 1.3615 lr: 0.0002 grad_norm_g: 562.8287 grad_norm_d: 14.0584
======> Epoch: 461
Train Epoch: 462 [53.85%] G-Loss: 29.4375 D-Loss: 2.4536 Loss-g-fm: 4.8050 Loss-g-mel: 19.0311 Loss-g-dur: 1.7754 Loss-g-kl: 1.4476 lr: 0.0002 grad_norm_g: 789.1452 grad_norm_d: 28.7458
Saving model and optimizer state at iteration 462 to /ZFS4T/tts/data/VITS/model_saved/G_12000.pth
Saving model and optimizer state at iteration 462 to /ZFS4T/tts/data/VITS/model_saved/D_12000.pth
======> Epoch: 462
Train Epoch: 463 [50.00%] G-Loss: 28.0691 D-Loss: 2.5171 Loss-g-fm: 4.4061 Loss-g-mel: 18.4893 Loss-g-dur: 1.6067 Loss-g-kl: 1.3113 lr: 0.0002 grad_norm_g: 286.8486 grad_norm_d: 8.9666
======> Epoch: 463
Train Epoch: 464 [46.15%] G-Loss: 30.4930 D-Loss: 2.3936 Loss-g-fm: 5.5199 Loss-g-mel: 19.9014 Loss-g-dur: 1.7111 Loss-g-kl: 1.3775 lr: 0.0002 grad_norm_g: 186.1598 grad_norm_d: 6.4786
======> Epoch: 464
Train Epoch: 465 [42.31%] G-Loss: 29.0168 D-Loss: 2.4601 Loss-g-fm: 5.0404 Loss-g-mel: 18.7808 Loss-g-dur: 1.5511 Loss-g-kl: 1.2532 lr: 0.0002 grad_norm_g: 908.7828 grad_norm_d: 37.5034
======> Epoch: 465
Train Epoch: 466 [38.46%] G-Loss: 31.3935 D-Loss: 2.6076 Loss-g-fm: 5.2922 Loss-g-mel: 19.9504 Loss-g-dur: 1.8747 Loss-g-kl: 1.4515 lr: 0.0002 grad_norm_g: 787.7973 grad_norm_d: 27.9468
======> Epoch: 466
Train Epoch: 467 [34.62%] G-Loss: 29.2425 D-Loss: 2.4384 Loss-g-fm: 5.2183 Loss-g-mel: 18.9820 Loss-g-dur: 1.6686 Loss-g-kl: 1.2248 lr: 0.0002 grad_norm_g: 731.5870 grad_norm_d: 35.0998
======> Epoch: 467
Train Epoch: 468 [30.77%] G-Loss: 29.6785 D-Loss: 2.3866 Loss-g-fm: 5.1981 Loss-g-mel: 19.1423 Loss-g-dur: 1.5870 Loss-g-kl: 1.3260 lr: 0.0002 grad_norm_g: 629.2180 grad_norm_d: 15.5328
======> Epoch: 468
Train Epoch: 469 [26.92%] G-Loss: 28.7295 D-Loss: 2.4684 Loss-g-fm: 4.8140 Loss-g-mel: 18.9114 Loss-g-dur: 1.5822 Loss-g-kl: 1.2508 lr: 0.0002 grad_norm_g: 96.5536 grad_norm_d: 11.9525
======> Epoch: 469
Train Epoch: 470 [23.08%] G-Loss: 29.5814 D-Loss: 2.3382 Loss-g-fm: 5.2060 Loss-g-mel: 19.1811 Loss-g-dur: 1.6712 Loss-g-kl: 1.2333 lr: 0.0002 grad_norm_g: 1036.1269 grad_norm_d: 46.5753
======> Epoch: 470
Train Epoch: 471 [19.23%] G-Loss: 29.2113 D-Loss: 2.5231 Loss-g-fm: 4.9306 Loss-g-mel: 19.1991 Loss-g-dur: 1.6848 Loss-g-kl: 1.2042 lr: 0.0002 grad_norm_g: 268.3948 grad_norm_d: 30.6532
======> Epoch: 471
Train Epoch: 472 [15.38%] G-Loss: 25.2803 D-Loss: 2.4664 Loss-g-fm: 4.1219 Loss-g-mel: 16.7024 Loss-g-dur: 1.3127 Loss-g-kl: 1.0864 lr: 0.0002 grad_norm_g: 627.5970 grad_norm_d: 18.0276
======> Epoch: 472
Train Epoch: 473 [11.54%] G-Loss: 28.4461 D-Loss: 2.4651 Loss-g-fm: 4.8668 Loss-g-mel: 18.4939 Loss-g-dur: 1.6094 Loss-g-kl: 1.3124 lr: 0.0002 grad_norm_g: 715.5764 grad_norm_d: 19.6624
======> Epoch: 473
Train Epoch: 474 [7.69%] G-Loss: 30.1273 D-Loss: 2.5154 Loss-g-fm: 4.9876 Loss-g-mel: 19.7856 Loss-g-dur: 1.6991 Loss-g-kl: 1.3636 lr: 0.0002 grad_norm_g: 811.1309 grad_norm_d: 18.6867
======> Epoch: 474
Train Epoch: 475 [3.85%] G-Loss: 29.0565 D-Loss: 2.4878 Loss-g-fm: 5.0014 Loss-g-mel: 19.0713 Loss-g-dur: 1.5699 Loss-g-kl: 1.1416 lr: 0.0002 grad_norm_g: 946.7119 grad_norm_d: 51.0545
======> Epoch: 475
Train Epoch: 476 [0.00%] G-Loss: 28.6204 D-Loss: 2.5243 Loss-g-fm: 4.8775 Loss-g-mel: 19.0003 Loss-g-dur: 1.6167 Loss-g-kl: 1.0544 lr: 0.0002 grad_norm_g: 745.9573 grad_norm_d: 28.6178
Train Epoch: 476 [96.15%] G-Loss: 28.9990 D-Loss: 2.4369 Loss-g-fm: 5.2777 Loss-g-mel: 18.6746 Loss-g-dur: 1.6082 Loss-g-kl: 1.2636 lr: 0.0002 grad_norm_g: 767.9402 grad_norm_d: 22.2383
======> Epoch: 476
Train Epoch: 477 [92.31%] G-Loss: 30.7168 D-Loss: 2.3320 Loss-g-fm: 5.5692 Loss-g-mel: 19.7294 Loss-g-dur: 1.6825 Loss-g-kl: 1.4712 lr: 0.0002 grad_norm_g: 504.4839 grad_norm_d: 23.7113
======> Epoch: 477
Train Epoch: 478 [88.46%] G-Loss: 28.2464 D-Loss: 2.5103 Loss-g-fm: 4.4861 Loss-g-mel: 18.5980 Loss-g-dur: 1.5897 Loss-g-kl: 1.3828 lr: 0.0002 grad_norm_g: 465.3702 grad_norm_d: 7.6257
======> Epoch: 478
Train Epoch: 479 [84.62%] G-Loss: 30.1214 D-Loss: 2.5919 Loss-g-fm: 4.9860 Loss-g-mel: 19.6263 Loss-g-dur: 1.7596 Loss-g-kl: 1.3685 lr: 0.0002 grad_norm_g: 863.1626 grad_norm_d: 54.7001
======> Epoch: 479
Train Epoch: 480 [80.77%] G-Loss: 24.8583 D-Loss: 2.4662 Loss-g-fm: 6.4756 Loss-g-mel: 13.1082 Loss-g-dur: 1.2357 Loss-g-kl: 1.2460 lr: 0.0002 grad_norm_g: 903.9410 grad_norm_d: 28.3516
======> Epoch: 480
Train Epoch: 481 [76.92%] G-Loss: 29.0168 D-Loss: 2.5351 Loss-g-fm: 4.9318 Loss-g-mel: 19.0233 Loss-g-dur: 1.5714 Loss-g-kl: 1.3231 lr: 0.0002 grad_norm_g: 378.8070 grad_norm_d: 31.9387
======> Epoch: 481
Train Epoch: 482 [73.08%] G-Loss: 29.7987 D-Loss: 2.4120 Loss-g-fm: 5.5742 Loss-g-mel: 18.8126 Loss-g-dur: 1.5454 Loss-g-kl: 1.3756 lr: 0.0002 grad_norm_g: 875.9277 grad_norm_d: 25.8102
======> Epoch: 482
Train Epoch: 483 [69.23%] G-Loss: 28.3131 D-Loss: 2.4288 Loss-g-fm: 4.8591 Loss-g-mel: 18.5139 Loss-g-dur: 1.5764 Loss-g-kl: 1.1773 lr: 0.0002 grad_norm_g: 236.0994 grad_norm_d: 10.3754
======> Epoch: 483
Train Epoch: 484 [65.38%] G-Loss: 31.5384 D-Loss: 2.3655 Loss-g-fm: 6.2835 Loss-g-mel: 19.8934 Loss-g-dur: 1.6516 Loss-g-kl: 1.3957 lr: 0.0002 grad_norm_g: 972.0043 grad_norm_d: 17.0763
======> Epoch: 484
Train Epoch: 485 [61.54%] G-Loss: 27.8066 D-Loss: 2.4148 Loss-g-fm: 4.7859 Loss-g-mel: 18.0731 Loss-g-dur: 1.5800 Loss-g-kl: 1.0185 lr: 0.0002 grad_norm_g: 936.7808 grad_norm_d: 26.3992
======> Epoch: 485
Train Epoch: 486 [57.69%] G-Loss: 28.0500 D-Loss: 2.4497 Loss-g-fm: 4.4693 Loss-g-mel: 18.7675 Loss-g-dur: 1.6276 Loss-g-kl: 1.0983 lr: 0.0002 grad_norm_g: 775.6145 grad_norm_d: 12.8938
======> Epoch: 486
Train Epoch: 487 [53.85%] G-Loss: 29.4851 D-Loss: 2.5874 Loss-g-fm: 4.6955 Loss-g-mel: 19.2459 Loss-g-dur: 1.7144 Loss-g-kl: 1.3871 lr: 0.0002 grad_norm_g: 681.4378 grad_norm_d: 33.1082
======> Epoch: 487
Train Epoch: 488 [50.00%] G-Loss: 29.6271 D-Loss: 2.4036 Loss-g-fm: 5.4133 Loss-g-mel: 18.7244 Loss-g-dur: 1.7190 Loss-g-kl: 1.3274 lr: 0.0002 grad_norm_g: 968.1307 grad_norm_d: 43.7014
======> Epoch: 488
Train Epoch: 489 [46.15%] G-Loss: 29.0312 D-Loss: 2.4412 Loss-g-fm: 5.1902 Loss-g-mel: 19.0397 Loss-g-dur: 1.5748 Loss-g-kl: 1.1479 lr: 0.0002 grad_norm_g: 684.9570 grad_norm_d: 49.2960
======> Epoch: 489
Train Epoch: 490 [42.31%] G-Loss: 29.5728 D-Loss: 2.4260 Loss-g-fm: 4.9743 Loss-g-mel: 19.3809 Loss-g-dur: 1.6354 Loss-g-kl: 1.4057 lr: 0.0002 grad_norm_g: 308.3632 grad_norm_d: 11.6966
======> Epoch: 490
Train Epoch: 491 [38.46%] G-Loss: 30.5433 D-Loss: 2.3352 Loss-g-fm: 5.6429 Loss-g-mel: 19.0579 Loss-g-dur: 1.8221 Loss-g-kl: 1.4421 lr: 0.0002 grad_norm_g: 1099.5029 grad_norm_d: 34.4555
======> Epoch: 491
Train Epoch: 492 [34.62%] G-Loss: 30.6229 D-Loss: 2.6021 Loss-g-fm: 5.8026 Loss-g-mel: 19.8506 Loss-g-dur: 1.6068 Loss-g-kl: 1.0667 lr: 0.0002 grad_norm_g: 670.9437 grad_norm_d: 45.9403
======> Epoch: 492
Train Epoch: 493 [30.77%] G-Loss: 28.3993 D-Loss: 2.3549 Loss-g-fm: 5.0665 Loss-g-mel: 18.4957 Loss-g-dur: 1.5564 Loss-g-kl: 1.1230 lr: 0.0002 grad_norm_g: 653.4023 grad_norm_d: 21.7250
======> Epoch: 493
Train Epoch: 494 [26.92%] G-Loss: 27.6369 D-Loss: 2.3653 Loss-g-fm: 5.3541 Loss-g-mel: 17.0253 Loss-g-dur: 1.3964 Loss-g-kl: 1.3309 lr: 0.0002 grad_norm_g: 1219.6361 grad_norm_d: 70.5068
======> Epoch: 494
Train Epoch: 495 [23.08%] G-Loss: 30.7790 D-Loss: 2.4921 Loss-g-fm: 5.5242 Loss-g-mel: 20.0076 Loss-g-dur: 1.5644 Loss-g-kl: 1.5238 lr: 0.0002 grad_norm_g: 551.0940 grad_norm_d: 28.2855
======> Epoch: 495
Train Epoch: 496 [19.23%] G-Loss: 28.3200 D-Loss: 2.5048 Loss-g-fm: 4.8484 Loss-g-mel: 18.5727 Loss-g-dur: 1.5717 Loss-g-kl: 1.1486 lr: 0.0002 grad_norm_g: 627.2435 grad_norm_d: 47.7351
======> Epoch: 496
Train Epoch: 497 [15.38%] G-Loss: 28.3374 D-Loss: 2.4767 Loss-g-fm: 4.5721 Loss-g-mel: 18.7969 Loss-g-dur: 1.5722 Loss-g-kl: 1.2032 lr: 0.0002 grad_norm_g: 986.5674 grad_norm_d: 48.1885
======> Epoch: 497
Train Epoch: 498 [11.54%] G-Loss: 23.2926 D-Loss: 2.3859 Loss-g-fm: 5.6706 Loss-g-mel: 12.3657 Loss-g-dur: 1.1295 Loss-g-kl: 1.3675 lr: 0.0002 grad_norm_g: 120.8429 grad_norm_d: 6.4800
======> Epoch: 498
Train Epoch: 499 [7.69%] G-Loss: 29.7544 D-Loss: 2.5517 Loss-g-fm: 5.4451 Loss-g-mel: 19.2042 Loss-g-dur: 1.5910 Loss-g-kl: 1.3243 lr: 0.0002 grad_norm_g: 664.7068 grad_norm_d: 40.4540
======> Epoch: 499
Train Epoch: 500 [3.85%] G-Loss: 28.3281 D-Loss: 2.4592 Loss-g-fm: 4.8879 Loss-g-mel: 18.4372 Loss-g-dur: 1.5820 Loss-g-kl: 1.1569 lr: 0.0002 grad_norm_g: 585.3624 grad_norm_d: 36.4841
======> Epoch: 500
Train Epoch: 501 [0.00%] G-Loss: 29.9263 D-Loss: 2.3696 Loss-g-fm: 5.6660 Loss-g-mel: 18.9039 Loss-g-dur: 1.6838 Loss-g-kl: 1.3458 lr: 0.0002 grad_norm_g: 1199.2036 grad_norm_d: 35.0908
Train Epoch: 501 [96.15%] G-Loss: 26.9291 D-Loss: 2.4987 Loss-g-fm: 4.3041 Loss-g-mel: 17.6464 Loss-g-dur: 1.5355 Loss-g-kl: 1.1876 lr: 0.0002 grad_norm_g: 925.3743 grad_norm_d: 24.2504
======> Epoch: 501
Train Epoch: 502 [92.31%] G-Loss: 29.1885 D-Loss: 2.3614 Loss-g-fm: 5.3139 Loss-g-mel: 18.7636 Loss-g-dur: 1.5836 Loss-g-kl: 1.1067 lr: 0.0002 grad_norm_g: 1045.1137 grad_norm_d: 42.7500
======> Epoch: 502
Train Epoch: 503 [88.46%] G-Loss: 29.3824 D-Loss: 2.4679 Loss-g-fm: 5.4687 Loss-g-mel: 18.7714 Loss-g-dur: 1.6275 Loss-g-kl: 1.1495 lr: 0.0002 grad_norm_g: 428.1210 grad_norm_d: 20.3939
======> Epoch: 503
Train Epoch: 504 [84.62%] G-Loss: 29.0624 D-Loss: 2.4602 Loss-g-fm: 5.1813 Loss-g-mel: 18.8305 Loss-g-dur: 1.5276 Loss-g-kl: 1.3397 lr: 0.0002 grad_norm_g: 342.3692 grad_norm_d: 10.1265
======> Epoch: 504
Train Epoch: 505 [80.77%] G-Loss: 29.3498 D-Loss: 2.3499 Loss-g-fm: 5.6563 Loss-g-mel: 18.3478 Loss-g-dur: 1.6704 Loss-g-kl: 1.3607 lr: 0.0002 grad_norm_g: 558.1223 grad_norm_d: 23.5316
======> Epoch: 505
Train Epoch: 506 [76.92%] G-Loss: 27.3840 D-Loss: 2.5571 Loss-g-fm: 4.5710 Loss-g-mel: 17.8859 Loss-g-dur: 1.5849 Loss-g-kl: 1.1710 lr: 0.0002 grad_norm_g: 504.4908 grad_norm_d: 26.5939
======> Epoch: 506
Train Epoch: 507 [73.08%] G-Loss: 29.3936 D-Loss: 2.3588 Loss-g-fm: 5.1385 Loss-g-mel: 19.1590 Loss-g-dur: 1.7024 Loss-g-kl: 1.4151 lr: 0.0002 grad_norm_g: 840.4707 grad_norm_d: 34.4296
======> Epoch: 507
Train Epoch: 508 [69.23%] G-Loss: 28.6550 D-Loss: 2.4728 Loss-g-fm: 4.7877 Loss-g-mel: 18.7234 Loss-g-dur: 1.5381 Loss-g-kl: 1.3886 lr: 0.0002 grad_norm_g: 745.3764 grad_norm_d: 30.2264
======> Epoch: 508
Train Epoch: 509 [65.38%] G-Loss: 30.4269 D-Loss: 2.4973 Loss-g-fm: 5.6954 Loss-g-mel: 19.3402 Loss-g-dur: 1.6885 Loss-g-kl: 1.3119 lr: 0.0002 grad_norm_g: 987.6462 grad_norm_d: 29.6519
======> Epoch: 509
Train Epoch: 510 [61.54%] G-Loss: 30.5018 D-Loss: 2.5667 Loss-g-fm: 5.6146 Loss-g-mel: 19.4982 Loss-g-dur: 1.7674 Loss-g-kl: 1.3204 lr: 0.0002 grad_norm_g: 925.0256 grad_norm_d: 62.3537
======> Epoch: 510
Train Epoch: 511 [57.69%] G-Loss: 30.3633 D-Loss: 2.4349 Loss-g-fm: 5.4344 Loss-g-mel: 19.5048 Loss-g-dur: 1.6950 Loss-g-kl: 1.4990 lr: 0.0002 grad_norm_g: 533.0867 grad_norm_d: 26.0801
======> Epoch: 511
Train Epoch: 512 [53.85%] G-Loss: 24.8379 D-Loss: 2.4591 Loss-g-fm: 6.3712 Loss-g-mel: 13.5554 Loss-g-dur: 1.0844 Loss-g-kl: 1.2597 lr: 0.0002 grad_norm_g: 1096.5591 grad_norm_d: 33.8794
======> Epoch: 512
Train Epoch: 513 [50.00%] G-Loss: 30.0402 D-Loss: 2.5032 Loss-g-fm: 5.7831 Loss-g-mel: 19.4234 Loss-g-dur: 1.5942 Loss-g-kl: 1.1794 lr: 0.0002 grad_norm_g: 193.9027 grad_norm_d: 7.0449
======> Epoch: 513
Train Epoch: 514 [46.15%] G-Loss: 29.9639 D-Loss: 2.5275 Loss-g-fm: 5.0370 Loss-g-mel: 19.6077 Loss-g-dur: 1.6020 Loss-g-kl: 1.3552 lr: 0.0002 grad_norm_g: 356.3996 grad_norm_d: 12.1562
======> Epoch: 514
Train Epoch: 515 [42.31%] G-Loss: 29.2156 D-Loss: 2.6436 Loss-g-fm: 5.0085 Loss-g-mel: 18.5181 Loss-g-dur: 1.6014 Loss-g-kl: 1.1462 lr: 0.0002 grad_norm_g: 927.0367 grad_norm_d: 49.6281
======> Epoch: 515
Train Epoch: 516 [38.46%] G-Loss: 23.5299 D-Loss: 2.4027 Loss-g-fm: 6.3583 Loss-g-mel: 12.2534 Loss-g-dur: 1.0869 Loss-g-kl: 1.2876 lr: 0.0002 grad_norm_g: 141.0801 grad_norm_d: 18.3969
======> Epoch: 516
Train Epoch: 517 [34.62%] G-Loss: 24.4101 D-Loss: 2.3799 Loss-g-fm: 6.4267 Loss-g-mel: 13.1742 Loss-g-dur: 1.1409 Loss-g-kl: 1.2099 lr: 0.0002 grad_norm_g: 848.0457 grad_norm_d: 29.4625
======> Epoch: 517
Train Epoch: 518 [30.77%] G-Loss: 30.1555 D-Loss: 2.4476 Loss-g-fm: 5.3795 Loss-g-mel: 19.3384 Loss-g-dur: 1.6363 Loss-g-kl: 1.3381 lr: 0.0002 grad_norm_g: 382.5833 grad_norm_d: 10.0543
======> Epoch: 518
Train Epoch: 519 [26.92%] G-Loss: 30.0737 D-Loss: 2.2271 Loss-g-fm: 7.1560 Loss-g-mel: 17.5332 Loss-g-dur: 1.3982 Loss-g-kl: 1.4660 lr: 0.0002 grad_norm_g: 466.8190 grad_norm_d: 22.6321
======> Epoch: 519
Train Epoch: 520 [23.08%] G-Loss: 27.8293 D-Loss: 2.5430 Loss-g-fm: 4.6381 Loss-g-mel: 18.2384 Loss-g-dur: 1.5230 Loss-g-kl: 1.2390 lr: 0.0002 grad_norm_g: 175.4167 grad_norm_d: 39.8359
======> Epoch: 520
Train Epoch: 521 [19.23%] G-Loss: 30.3491 D-Loss: 2.4876 Loss-g-fm: 5.6387 Loss-g-mel: 19.5529 Loss-g-dur: 1.5933 Loss-g-kl: 0.9432 lr: 0.0002 grad_norm_g: 955.3062 grad_norm_d: 62.4686
======> Epoch: 521
Train Epoch: 522 [15.38%] G-Loss: 29.6109 D-Loss: 2.3303 Loss-g-fm: 5.6628 Loss-g-mel: 18.9219 Loss-g-dur: 1.5302 Loss-g-kl: 1.1152 lr: 0.0002 grad_norm_g: 1134.1449 grad_norm_d: 52.8822
======> Epoch: 522
Train Epoch: 523 [11.54%] G-Loss: 27.7087 D-Loss: 2.2437 Loss-g-fm: 5.9351 Loss-g-mel: 16.6275 Loss-g-dur: 1.3953 Loss-g-kl: 1.1956 lr: 0.0002 grad_norm_g: 1331.9266 grad_norm_d: 69.3171
======> Epoch: 523
Train Epoch: 524 [7.69%] G-Loss: 29.6992 D-Loss: 2.4344 Loss-g-fm: 5.7030 Loss-g-mel: 18.9731 Loss-g-dur: 1.5592 Loss-g-kl: 1.1230 lr: 0.0002 grad_norm_g: 790.5704 grad_norm_d: 42.0438
======> Epoch: 524
Train Epoch: 525 [3.85%] G-Loss: 28.8129 D-Loss: 2.4253 Loss-g-fm: 5.0798 Loss-g-mel: 18.7418 Loss-g-dur: 1.5748 Loss-g-kl: 1.1565 lr: 0.0002 grad_norm_g: 950.7017 grad_norm_d: 43.8516
======> Epoch: 525
Train Epoch: 526 [0.00%] G-Loss: 29.6533 D-Loss: 2.3610 Loss-g-fm: 5.8656 Loss-g-mel: 18.8656 Loss-g-dur: 1.6016 Loss-g-kl: 1.0133 lr: 0.0002 grad_norm_g: 1033.0236 grad_norm_d: 50.8452
Train Epoch: 526 [96.15%] G-Loss: 28.5235 D-Loss: 2.6317 Loss-g-fm: 4.6994 Loss-g-mel: 18.7319 Loss-g-dur: 1.6885 Loss-g-kl: 1.2138 lr: 0.0002 grad_norm_g: 89.8811 grad_norm_d: 33.7217
======> Epoch: 526
Train Epoch: 527 [92.31%] G-Loss: 30.0915 D-Loss: 2.6339 Loss-g-fm: 5.6507 Loss-g-mel: 19.4565 Loss-g-dur: 1.6830 Loss-g-kl: 1.3549 lr: 0.0002 grad_norm_g: 352.2950 grad_norm_d: 46.5033
======> Epoch: 527
Train Epoch: 528 [88.46%] G-Loss: 29.0073 D-Loss: 2.3470 Loss-g-fm: 5.4306 Loss-g-mel: 18.5201 Loss-g-dur: 1.5516 Loss-g-kl: 1.2230 lr: 0.0002 grad_norm_g: 548.0515 grad_norm_d: 38.1042
======> Epoch: 528
Train Epoch: 529 [84.62%] G-Loss: 28.8287 D-Loss: 2.4015 Loss-g-fm: 5.5153 Loss-g-mel: 18.2294 Loss-g-dur: 1.5640 Loss-g-kl: 1.3701 lr: 0.0002 grad_norm_g: 959.6323 grad_norm_d: 44.1082
======> Epoch: 529
Train Epoch: 530 [80.77%] G-Loss: 30.6896 D-Loss: 2.4379 Loss-g-fm: 5.7740 Loss-g-mel: 19.7942 Loss-g-dur: 1.5941 Loss-g-kl: 1.3507 lr: 0.0002 grad_norm_g: 914.1739 grad_norm_d: 31.2598
======> Epoch: 530
Train Epoch: 531 [76.92%] G-Loss: 29.3543 D-Loss: 2.4671 Loss-g-fm: 5.3737 Loss-g-mel: 18.9265 Loss-g-dur: 1.5929 Loss-g-kl: 1.1726 lr: 0.0002 grad_norm_g: 965.5621 grad_norm_d: 54.6632
======> Epoch: 531
Train Epoch: 532 [73.08%] G-Loss: 29.2819 D-Loss: 2.3748 Loss-g-fm: 5.5450 Loss-g-mel: 18.3789 Loss-g-dur: 1.6543 Loss-g-kl: 1.2891 lr: 0.0002 grad_norm_g: 956.7381 grad_norm_d: 32.9539
======> Epoch: 532
Train Epoch: 533 [69.23%] G-Loss: 31.0458 D-Loss: 2.3289 Loss-g-fm: 6.3027 Loss-g-mel: 19.4793 Loss-g-dur: 1.5374 Loss-g-kl: 1.4217 lr: 0.0002 grad_norm_g: 991.3868 grad_norm_d: 41.6788
======> Epoch: 533
Train Epoch: 534 [65.38%] G-Loss: 27.3860 D-Loss: 2.5254 Loss-g-fm: 4.7918 Loss-g-mel: 17.8662 Loss-g-dur: 1.5143 Loss-g-kl: 1.1037 lr: 0.0002 grad_norm_g: 681.2173 grad_norm_d: 38.3325
======> Epoch: 534
Train Epoch: 535 [61.54%] G-Loss: 31.3620 D-Loss: 2.2680 Loss-g-fm: 5.9295 Loss-g-mel: 19.8788 Loss-g-dur: 1.7673 Loss-g-kl: 1.3830 lr: 0.0002 grad_norm_g: 121.2874 grad_norm_d: 13.1870
======> Epoch: 535
Train Epoch: 536 [57.69%] G-Loss: 30.1437 D-Loss: 2.3156 Loss-g-fm: 5.9474 Loss-g-mel: 19.1544 Loss-g-dur: 1.5216 Loss-g-kl: 1.1574 lr: 0.0002 grad_norm_g: 943.0964 grad_norm_d: 45.0644
======> Epoch: 536
Train Epoch: 537 [53.85%] G-Loss: 29.3064 D-Loss: 2.4719 Loss-g-fm: 5.3098 Loss-g-mel: 18.7677 Loss-g-dur: 1.6078 Loss-g-kl: 1.1774 lr: 0.0002 grad_norm_g: 435.9603 grad_norm_d: 19.3922
======> Epoch: 537
Train Epoch: 538 [50.00%] G-Loss: 30.8652 D-Loss: 2.4461 Loss-g-fm: 6.0711 Loss-g-mel: 19.2249 Loss-g-dur: 1.7636 Loss-g-kl: 1.4787 lr: 0.0002 grad_norm_g: 844.8849 grad_norm_d: 39.5198
======> Epoch: 538
Train Epoch: 539 [46.15%] G-Loss: 29.7023 D-Loss: 2.5905 Loss-g-fm: 5.0727 Loss-g-mel: 18.7536 Loss-g-dur: 1.6044 Loss-g-kl: 1.4523 lr: 0.0002 grad_norm_g: 399.5107 grad_norm_d: 31.5099
Saving model and optimizer state at iteration 539 to /ZFS4T/tts/data/VITS/model_saved/G_14000.pth
Saving model and optimizer state at iteration 539 to /ZFS4T/tts/data/VITS/model_saved/D_14000.pth
======> Epoch: 539
Train Epoch: 540 [42.31%] G-Loss: 31.0615 D-Loss: 2.3667 Loss-g-fm: 6.1028 Loss-g-mel: 19.7873 Loss-g-dur: 1.5754 Loss-g-kl: 1.2288 lr: 0.0002 grad_norm_g: 913.6334 grad_norm_d: 35.5897
======> Epoch: 540
Train Epoch: 541 [38.46%] G-Loss: 27.3577 D-Loss: 2.4498 Loss-g-fm: 5.4165 Loss-g-mel: 16.8898 Loss-g-dur: 1.4033 Loss-g-kl: 1.2302 lr: 0.0002 grad_norm_g: 506.2551 grad_norm_d: 45.9879
======> Epoch: 541
Train Epoch: 542 [34.62%] G-Loss: 26.7020 D-Loss: 2.3784 Loss-g-fm: 5.0222 Loss-g-mel: 16.4861 Loss-g-dur: 1.3220 Loss-g-kl: 1.2787 lr: 0.0002 grad_norm_g: 587.6322 grad_norm_d: 39.0065
======> Epoch: 542
Train Epoch: 543 [30.77%] G-Loss: 30.5358 D-Loss: 2.3779 Loss-g-fm: 5.7733 Loss-g-mel: 19.4125 Loss-g-dur: 1.6903 Loss-g-kl: 1.4100 lr: 0.0002 grad_norm_g: 331.8711 grad_norm_d: 8.7374
======> Epoch: 543
Train Epoch: 544 [26.92%] G-Loss: 29.8054 D-Loss: 2.3910 Loss-g-fm: 5.6257 Loss-g-mel: 19.0550 Loss-g-dur: 1.5718 Loss-g-kl: 1.2150 lr: 0.0002 grad_norm_g: 302.1965 grad_norm_d: 27.4772
======> Epoch: 544
Train Epoch: 545 [23.08%] G-Loss: 24.9641 D-Loss: 2.3426 Loss-g-fm: 6.7383 Loss-g-mel: 13.4944 Loss-g-dur: 1.1028 Loss-g-kl: 1.3843 lr: 0.0002 grad_norm_g: 477.3785 grad_norm_d: 11.4699
======> Epoch: 545
Train Epoch: 546 [19.23%] G-Loss: 30.0292 D-Loss: 2.3786 Loss-g-fm: 5.8141 Loss-g-mel: 18.6016 Loss-g-dur: 1.6650 Loss-g-kl: 1.3453 lr: 0.0002 grad_norm_g: 996.4247 grad_norm_d: 43.0159
======> Epoch: 546
Train Epoch: 547 [15.38%] G-Loss: 29.9497 D-Loss: 2.5265 Loss-g-fm: 5.6248 Loss-g-mel: 19.2325 Loss-g-dur: 1.7045 Loss-g-kl: 1.2703 lr: 0.0002 grad_norm_g: 71.6491 grad_norm_d: 12.9586
======> Epoch: 547
Train Epoch: 548 [11.54%] G-Loss: 29.6382 D-Loss: 2.5086 Loss-g-fm: 6.1064 Loss-g-mel: 18.2220 Loss-g-dur: 1.6531 Loss-g-kl: 1.4656 lr: 0.0002 grad_norm_g: 858.6865 grad_norm_d: 41.1779
======> Epoch: 548
Train Epoch: 549 [7.69%] G-Loss: 31.6939 D-Loss: 2.2491 Loss-g-fm: 6.5948 Loss-g-mel: 19.5833 Loss-g-dur: 1.6287 Loss-g-kl: 1.4776 lr: 0.0002 grad_norm_g: 950.7422 grad_norm_d: 43.3859
======> Epoch: 549
Train Epoch: 550 [3.85%] G-Loss: 29.8269 D-Loss: 2.4006 Loss-g-fm: 5.7311 Loss-g-mel: 18.9431 Loss-g-dur: 1.5392 Loss-g-kl: 1.2833 lr: 0.0002 grad_norm_g: 1121.6465 grad_norm_d: 46.8093
======> Epoch: 550
Train Epoch: 551 [0.00%] G-Loss: 29.0306 D-Loss: 2.4763 Loss-g-fm: 5.5804 Loss-g-mel: 18.1848 Loss-g-dur: 1.5376 Loss-g-kl: 1.3768 lr: 0.0002 grad_norm_g: 1049.1112 grad_norm_d: 84.3582
Train Epoch: 551 [96.15%] G-Loss: 29.2001 D-Loss: 2.4553 Loss-g-fm: 5.4261 Loss-g-mel: 18.5696 Loss-g-dur: 1.5595 Loss-g-kl: 1.4879 lr: 0.0002 grad_norm_g: 790.3347 grad_norm_d: 72.5201
======> Epoch: 551
Train Epoch: 552 [92.31%] G-Loss: 24.9994 D-Loss: 2.2065 Loss-g-fm: 6.6123 Loss-g-mel: 12.5875 Loss-g-dur: 1.1144 Loss-g-kl: 1.3309 lr: 0.0002 grad_norm_g: 1395.3408 grad_norm_d: 43.4899
======> Epoch: 552
Train Epoch: 553 [88.46%] G-Loss: 31.0852 D-Loss: 2.4821 Loss-g-fm: 5.8378 Loss-g-mel: 19.8127 Loss-g-dur: 1.7740 Loss-g-kl: 1.2312 lr: 0.0002 grad_norm_g: 115.7782 grad_norm_d: 11.3865
======> Epoch: 553
Train Epoch: 554 [84.62%] G-Loss: 29.6079 D-Loss: 2.4796 Loss-g-fm: 5.5292 Loss-g-mel: 18.9306 Loss-g-dur: 1.7374 Loss-g-kl: 1.3221 lr: 0.0002 grad_norm_g: 571.7757 grad_norm_d: 27.5753
======> Epoch: 554
Train Epoch: 555 [80.77%] G-Loss: 30.9248 D-Loss: 2.3525 Loss-g-fm: 6.0050 Loss-g-mel: 19.6145 Loss-g-dur: 1.6508 Loss-g-kl: 1.4057 lr: 0.0002 grad_norm_g: 632.9871 grad_norm_d: 45.1391
======> Epoch: 555
Train Epoch: 556 [76.92%] G-Loss: 24.0276 D-Loss: 2.4404 Loss-g-fm: 6.0558 Loss-g-mel: 12.9032 Loss-g-dur: 1.1023 Loss-g-kl: 1.3438 lr: 0.0002 grad_norm_g: 717.1739 grad_norm_d: 56.2958
======> Epoch: 556
Train Epoch: 557 [73.08%] G-Loss: 31.3499 D-Loss: 2.3497 Loss-g-fm: 6.5552 Loss-g-mel: 19.2185 Loss-g-dur: 1.6719 Loss-g-kl: 1.4085 lr: 0.0002 grad_norm_g: 1085.5697 grad_norm_d: 80.6241
======> Epoch: 557
Train Epoch: 558 [69.23%] G-Loss: 32.1907 D-Loss: 2.2101 Loss-g-fm: 6.7399 Loss-g-mel: 19.8939 Loss-g-dur: 1.7158 Loss-g-kl: 1.3774 lr: 0.0002 grad_norm_g: 243.0807 grad_norm_d: 11.7522
======> Epoch: 558
Train Epoch: 559 [65.38%] G-Loss: 29.1125 D-Loss: 2.3674 Loss-g-fm: 5.5705 Loss-g-mel: 18.5264 Loss-g-dur: 1.5292 Loss-g-kl: 1.2389 lr: 0.0002 grad_norm_g: 724.8212 grad_norm_d: 35.0979
======> Epoch: 559
Train Epoch: 560 [61.54%] G-Loss: 30.8190 D-Loss: 2.2734 Loss-g-fm: 6.1933 Loss-g-mel: 19.2177 Loss-g-dur: 1.6557 Loss-g-kl: 1.4157 lr: 0.0002 grad_norm_g: 529.5792 grad_norm_d: 33.3843
======> Epoch: 560
Train Epoch: 561 [57.69%] G-Loss: 30.1470 D-Loss: 2.4790 Loss-g-fm: 5.4173 Loss-g-mel: 19.2323 Loss-g-dur: 1.6582 Loss-g-kl: 1.5330 lr: 0.0002 grad_norm_g: 68.9200 grad_norm_d: 15.7858
======> Epoch: 561
Train Epoch: 562 [53.85%] G-Loss: 29.5096 D-Loss: 2.2157 Loss-g-fm: 6.8916 Loss-g-mel: 17.5721 Loss-g-dur: 1.3650 Loss-g-kl: 1.2604 lr: 0.0002 grad_norm_g: 828.4046 grad_norm_d: 46.4563
======> Epoch: 562
Train Epoch: 563 [50.00%] G-Loss: 29.9143 D-Loss: 2.4683 Loss-g-fm: 5.4876 Loss-g-mel: 19.0198 Loss-g-dur: 1.6587 Loss-g-kl: 1.4648 lr: 0.0002 grad_norm_g: 322.1537 grad_norm_d: 23.0082
======> Epoch: 563
Train Epoch: 564 [46.15%] G-Loss: 29.4095 D-Loss: 2.4330 Loss-g-fm: 5.6640 Loss-g-mel: 18.5934 Loss-g-dur: 1.5651 Loss-g-kl: 1.3732 lr: 0.0002 grad_norm_g: 547.9800 grad_norm_d: 12.3095
======> Epoch: 564
Train Epoch: 565 [42.31%] G-Loss: 29.9467 D-Loss: 2.3310 Loss-g-fm: 6.1907 Loss-g-mel: 18.7142 Loss-g-dur: 1.5055 Loss-g-kl: 1.1092 lr: 0.0002 grad_norm_g: 61.0599 grad_norm_d: 14.2081
======> Epoch: 565
Train Epoch: 566 [38.46%] G-Loss: 28.8167 D-Loss: 2.4466 Loss-g-fm: 5.5658 Loss-g-mel: 18.0806 Loss-g-dur: 1.5289 Loss-g-kl: 1.2901 lr: 0.0002 grad_norm_g: 908.0249 grad_norm_d: 51.0912
======> Epoch: 566
Train Epoch: 567 [34.62%] G-Loss: 25.8702 D-Loss: 2.3866 Loss-g-fm: 6.8963 Loss-g-mel: 13.4900 Loss-g-dur: 1.1075 Loss-g-kl: 1.4276 lr: 0.0002 grad_norm_g: 539.1376 grad_norm_d: 19.0787
======> Epoch: 567
Train Epoch: 568 [30.77%] G-Loss: 28.5418 D-Loss: 2.4223 Loss-g-fm: 5.1745 Loss-g-mel: 18.2146 Loss-g-dur: 1.5481 Loss-g-kl: 1.2577 lr: 0.0002 grad_norm_g: 536.4465 grad_norm_d: 4.0798
======> Epoch: 568
Train Epoch: 569 [26.92%] G-Loss: 30.7173 D-Loss: 2.3355 Loss-g-fm: 6.2584 Loss-g-mel: 19.0601 Loss-g-dur: 1.5855 Loss-g-kl: 1.3872 lr: 0.0002 grad_norm_g: 720.7047 grad_norm_d: 31.8099
======> Epoch: 569
Train Epoch: 570 [23.08%] G-Loss: 31.5147 D-Loss: 2.3064 Loss-g-fm: 6.3812 Loss-g-mel: 19.6240 Loss-g-dur: 1.7394 Loss-g-kl: 1.5165 lr: 0.0002 grad_norm_g: 462.9105 grad_norm_d: 18.5473
======> Epoch: 570
Train Epoch: 571 [19.23%] G-Loss: 28.9873 D-Loss: 2.4794 Loss-g-fm: 5.5449 Loss-g-mel: 18.1360 Loss-g-dur: 1.5518 Loss-g-kl: 1.3474 lr: 0.0002 grad_norm_g: 788.4886 grad_norm_d: 108.4487
======> Epoch: 571
Train Epoch: 572 [15.38%] G-Loss: 30.0126 D-Loss: 2.4546 Loss-g-fm: 5.9266 Loss-g-mel: 19.1642 Loss-g-dur: 1.5391 Loss-g-kl: 1.2009 lr: 0.0002 grad_norm_g: 749.6422 grad_norm_d: 40.7253
======> Epoch: 572
Train Epoch: 573 [11.54%] G-Loss: 30.0399 D-Loss: 2.3692 Loss-g-fm: 5.7165 Loss-g-mel: 19.2026 Loss-g-dur: 1.6355 Loss-g-kl: 1.3941 lr: 0.0002 grad_norm_g: 777.4743 grad_norm_d: 55.5481
======> Epoch: 573
Train Epoch: 574 [7.69%] G-Loss: 30.9171 D-Loss: 2.2927 Loss-g-fm: 6.0142 Loss-g-mel: 19.4473 Loss-g-dur: 1.7211 Loss-g-kl: 1.4087 lr: 0.0002 grad_norm_g: 934.1595 grad_norm_d: 38.8875
======> Epoch: 574
Train Epoch: 575 [3.85%] G-Loss: 29.0197 D-Loss: 2.4126 Loss-g-fm: 5.5262 Loss-g-mel: 18.4128 Loss-g-dur: 1.5297 Loss-g-kl: 1.2969 lr: 0.0002 grad_norm_g: 605.7446 grad_norm_d: 49.1378
======> Epoch: 575
Train Epoch: 576 [0.00%] G-Loss: 29.7918 D-Loss: 2.3194 Loss-g-fm: 5.9978 Loss-g-mel: 18.7227 Loss-g-dur: 1.5792 Loss-g-kl: 1.1821 lr: 0.0002 grad_norm_g: 237.9426 grad_norm_d: 40.7681
Train Epoch: 576 [96.15%] G-Loss: 25.0683 D-Loss: 2.1977 Loss-g-fm: 7.1367 Loss-g-mel: 12.9571 Loss-g-dur: 1.0488 Loss-g-kl: 1.2917 lr: 0.0002 grad_norm_g: 979.0579 grad_norm_d: 33.3094
======> Epoch: 576
Train Epoch: 577 [92.31%] G-Loss: 30.8172 D-Loss: 2.4195 Loss-g-fm: 6.0363 Loss-g-mel: 19.2612 Loss-g-dur: 1.6616 Loss-g-kl: 1.5204 lr: 0.0002 grad_norm_g: 882.0666 grad_norm_d: 59.3347
======> Epoch: 577
Train Epoch: 578 [88.46%] G-Loss: 30.1763 D-Loss: 2.3741 Loss-g-fm: 5.8712 Loss-g-mel: 19.1796 Loss-g-dur: 1.6899 Loss-g-kl: 1.4050 lr: 0.0002 grad_norm_g: 1080.8023 grad_norm_d: 36.5996
======> Epoch: 578
Train Epoch: 579 [84.62%] G-Loss: 23.7827 D-Loss: 2.2079 Loss-g-fm: 6.6098 Loss-g-mel: 11.8131 Loss-g-dur: 1.1138 Loss-g-kl: 1.3332 lr: 0.0002 grad_norm_g: 200.6738 grad_norm_d: 5.4202
======> Epoch: 579
Train Epoch: 580 [80.77%] G-Loss: 29.6646 D-Loss: 2.5104 Loss-g-fm: 5.7073 Loss-g-mel: 19.0366 Loss-g-dur: 1.6063 Loss-g-kl: 1.4836 lr: 0.0002 grad_norm_g: 316.8362 grad_norm_d: 26.1927
======> Epoch: 580
Train Epoch: 581 [76.92%] G-Loss: 31.2734 D-Loss: 2.2876 Loss-g-fm: 6.2262 Loss-g-mel: 19.5666 Loss-g-dur: 1.7332 Loss-g-kl: 1.4155 lr: 0.0002 grad_norm_g: 141.0698 grad_norm_d: 28.3790
======> Epoch: 581
Train Epoch: 582 [73.08%] G-Loss: 29.7159 D-Loss: 2.5190 Loss-g-fm: 5.6382 Loss-g-mel: 18.4036 Loss-g-dur: 1.5706 Loss-g-kl: 1.3051 lr: 0.0002 grad_norm_g: 945.1156 grad_norm_d: 42.3059
======> Epoch: 582
Train Epoch: 583 [69.23%] G-Loss: 31.5097 D-Loss: 2.3520 Loss-g-fm: 6.6279 Loss-g-mel: 19.4172 Loss-g-dur: 1.6284 Loss-g-kl: 1.3552 lr: 0.0002 grad_norm_g: 1128.8026 grad_norm_d: 38.6459
======> Epoch: 583
Train Epoch: 584 [65.38%] G-Loss: 28.4964 D-Loss: 2.3755 Loss-g-fm: 5.5868 Loss-g-mel: 18.0968 Loss-g-dur: 1.5066 Loss-g-kl: 1.1127 lr: 0.0002 grad_norm_g: 847.2227 grad_norm_d: 32.6206
======> Epoch: 584
Train Epoch: 585 [61.54%] G-Loss: 31.5805 D-Loss: 2.2543 Loss-g-fm: 6.4866 Loss-g-mel: 19.3301 Loss-g-dur: 1.6581 Loss-g-kl: 1.4826 lr: 0.0002 grad_norm_g: 668.8259 grad_norm_d: 18.8475
======> Epoch: 585
Train Epoch: 586 [57.69%] G-Loss: 27.5159 D-Loss: 2.4548 Loss-g-fm: 4.7671 Loss-g-mel: 17.9618 Loss-g-dur: 1.5651 Loss-g-kl: 1.0941 lr: 0.0002 grad_norm_g: 420.4237 grad_norm_d: 16.8674
======> Epoch: 586
Train Epoch: 587 [53.85%] G-Loss: 29.8591 D-Loss: 2.2909 Loss-g-fm: 5.8235 Loss-g-mel: 18.8702 Loss-g-dur: 1.5717 Loss-g-kl: 1.2003 lr: 0.0002 grad_norm_g: 1009.1308 grad_norm_d: 35.3803
======> Epoch: 587
Train Epoch: 588 [50.00%] G-Loss: 30.6132 D-Loss: 2.4121 Loss-g-fm: 6.4562 Loss-g-mel: 19.0644 Loss-g-dur: 1.5162 Loss-g-kl: 1.3123 lr: 0.0002 grad_norm_g: 1270.4267 grad_norm_d: 81.7513
======> Epoch: 588
Train Epoch: 589 [46.15%] G-Loss: 31.2144 D-Loss: 2.2204 Loss-g-fm: 6.5080 Loss-g-mel: 19.1651 Loss-g-dur: 1.5062 Loss-g-kl: 1.4963 lr: 0.0002 grad_norm_g: 1080.2021 grad_norm_d: 46.9255
======> Epoch: 589
Train Epoch: 590 [42.31%] G-Loss: 32.1488 D-Loss: 2.1864 Loss-g-fm: 6.9805 Loss-g-mel: 19.6416 Loss-g-dur: 1.5711 Loss-g-kl: 1.3867 lr: 0.0002 grad_norm_g: 1040.4940 grad_norm_d: 43.5602
======> Epoch: 590
Train Epoch: 591 [38.46%] G-Loss: 32.9990 D-Loss: 2.2621 Loss-g-fm: 6.9979 Loss-g-mel: 20.6459 Loss-g-dur: 1.6305 Loss-g-kl: 1.4145 lr: 0.0002 grad_norm_g: 651.7189 grad_norm_d: 38.5190
======> Epoch: 591
Train Epoch: 592 [34.62%] G-Loss: 30.1890 D-Loss: 2.2625 Loss-g-fm: 6.0296 Loss-g-mel: 18.8082 Loss-g-dur: 1.6952 Loss-g-kl: 1.4085 lr: 0.0002 grad_norm_g: 376.0995 grad_norm_d: 23.6624
======> Epoch: 592
Train Epoch: 593 [30.77%] G-Loss: 30.8798 D-Loss: 2.4025 Loss-g-fm: 6.1021 Loss-g-mel: 19.5438 Loss-g-dur: 1.7442 Loss-g-kl: 1.3978 lr: 0.0002 grad_norm_g: 219.0400 grad_norm_d: 24.5398
======> Epoch: 593
Train Epoch: 594 [26.92%] G-Loss: 30.0400 D-Loss: 2.2603 Loss-g-fm: 6.2623 Loss-g-mel: 18.4354 Loss-g-dur: 1.5540 Loss-g-kl: 1.3127 lr: 0.0002 grad_norm_g: 1017.5417 grad_norm_d: 55.1562
======> Epoch: 594
Train Epoch: 595 [23.08%] G-Loss: 24.7108 D-Loss: 2.3949 Loss-g-fm: 6.4853 Loss-g-mel: 12.7092 Loss-g-dur: 1.1016 Loss-g-kl: 1.3471 lr: 0.0002 grad_norm_g: 1108.8042 grad_norm_d: 22.3334
======> Epoch: 595
Train Epoch: 596 [19.23%] G-Loss: 30.8171 D-Loss: 2.3656 Loss-g-fm: 6.2244 Loss-g-mel: 19.3613 Loss-g-dur: 1.5153 Loss-g-kl: 1.2276 lr: 0.0002 grad_norm_g: 94.5615 grad_norm_d: 17.2981
======> Epoch: 596
Train Epoch: 597 [15.38%] G-Loss: 29.7750 D-Loss: 2.4222 Loss-g-fm: 6.0285 Loss-g-mel: 18.4569 Loss-g-dur: 1.5033 Loss-g-kl: 1.3374 lr: 0.0002 grad_norm_g: 1067.9239 grad_norm_d: 67.4910
======> Epoch: 597
Train Epoch: 598 [11.54%] G-Loss: 29.8248 D-Loss: 2.3045 Loss-g-fm: 6.0050 Loss-g-mel: 18.7042 Loss-g-dur: 1.4852 Loss-g-kl: 1.3629 lr: 0.0002 grad_norm_g: 964.4374 grad_norm_d: 49.3259
======> Epoch: 598
Train Epoch: 599 [7.69%] G-Loss: 29.5861 D-Loss: 2.4154 Loss-g-fm: 5.5430 Loss-g-mel: 18.7623 Loss-g-dur: 1.6584 Loss-g-kl: 1.3042 lr: 0.0002 grad_norm_g: 474.5533 grad_norm_d: 22.2772
======> Epoch: 599
Train Epoch: 600 [3.85%] G-Loss: 31.8143 D-Loss: 2.3612 Loss-g-fm: 6.0896 Loss-g-mel: 19.6115 Loss-g-dur: 1.6631 Loss-g-kl: 1.3574 lr: 0.0002 grad_norm_g: 1028.8797 grad_norm_d: 42.7504
======> Epoch: 600
Train Epoch: 601 [0.00%] G-Loss: 32.6971 D-Loss: 2.2333 Loss-g-fm: 7.1955 Loss-g-mel: 19.9641 Loss-g-dur: 1.7108 Loss-g-kl: 1.3318 lr: 0.0002 grad_norm_g: 994.1219 grad_norm_d: 46.3539
Train Epoch: 601 [96.15%] G-Loss: 29.7214 D-Loss: 2.4469 Loss-g-fm: 5.8164 Loss-g-mel: 18.7984 Loss-g-dur: 1.5280 Loss-g-kl: 1.2800 lr: 0.0002 grad_norm_g: 802.7224 grad_norm_d: 64.7308
======> Epoch: 601
Train Epoch: 602 [92.31%] G-Loss: 31.6898 D-Loss: 2.2669 Loss-g-fm: 6.3322 Loss-g-mel: 19.7001 Loss-g-dur: 1.6152 Loss-g-kl: 1.3629 lr: 0.0002 grad_norm_g: 523.5142 grad_norm_d: 29.4981
======> Epoch: 602
Train Epoch: 603 [88.46%] G-Loss: 25.4542 D-Loss: 2.4557 Loss-g-fm: 7.4168 Loss-g-mel: 12.8531 Loss-g-dur: 1.0625 Loss-g-kl: 1.1975 lr: 0.0002 grad_norm_g: 106.9239 grad_norm_d: 18.0711
======> Epoch: 603
Train Epoch: 604 [84.62%] G-Loss: 30.7785 D-Loss: 2.2802 Loss-g-fm: 6.5468 Loss-g-mel: 18.8050 Loss-g-dur: 1.5360 Loss-g-kl: 1.2342 lr: 0.0002 grad_norm_g: 1082.7486 grad_norm_d: 53.0022
======> Epoch: 604
Train Epoch: 605 [80.77%] G-Loss: 30.5256 D-Loss: 2.5038 Loss-g-fm: 6.1091 Loss-g-mel: 19.3534 Loss-g-dur: 1.5295 Loss-g-kl: 1.4041 lr: 0.0002 grad_norm_g: 858.3907 grad_norm_d: 50.1279
======> Epoch: 605
Train Epoch: 606 [76.92%] G-Loss: 29.3042 D-Loss: 2.5573 Loss-g-fm: 5.3022 Loss-g-mel: 18.8043 Loss-g-dur: 1.5047 Loss-g-kl: 1.1565 lr: 0.0002 grad_norm_g: 734.6311 grad_norm_d: 23.6511
======> Epoch: 606
Train Epoch: 607 [73.08%] G-Loss: 28.5579 D-Loss: 2.3504 Loss-g-fm: 5.6594 Loss-g-mel: 17.6533 Loss-g-dur: 1.5052 Loss-g-kl: 1.3438 lr: 0.0002 grad_norm_g: 937.6349 grad_norm_d: 60.3815
======> Epoch: 607
Train Epoch: 608 [69.23%] G-Loss: 25.0788 D-Loss: 2.2315 Loss-g-fm: 6.6980 Loss-g-mel: 12.8686 Loss-g-dur: 1.0508 Loss-g-kl: 1.2619 lr: 0.0002 grad_norm_g: 1045.5691 grad_norm_d: 36.0655
======> Epoch: 608
Train Epoch: 609 [65.38%] G-Loss: 30.7157 D-Loss: 2.4226 Loss-g-fm: 6.2798 Loss-g-mel: 18.8683 Loss-g-dur: 1.7193 Loss-g-kl: 1.4127 lr: 0.0002 grad_norm_g: 1103.5382 grad_norm_d: 55.2441
======> Epoch: 609
Train Epoch: 610 [61.54%] G-Loss: 28.8698 D-Loss: 2.5136 Loss-g-fm: 5.2923 Loss-g-mel: 18.5054 Loss-g-dur: 1.5395 Loss-g-kl: 1.1191 lr: 0.0002 grad_norm_g: 279.1927 grad_norm_d: 45.9103
======> Epoch: 610
Train Epoch: 611 [57.69%] G-Loss: 25.4863 D-Loss: 2.5085 Loss-g-fm: 7.5762 Loss-g-mel: 12.8774 Loss-g-dur: 1.0093 Loss-g-kl: 1.3118 lr: 0.0002 grad_norm_g: 1123.3948 grad_norm_d: 41.2722
======> Epoch: 611
Train Epoch: 612 [53.85%] G-Loss: 23.8553 D-Loss: 2.4434 Loss-g-fm: 6.6709 Loss-g-mel: 12.0510 Loss-g-dur: 1.2456 Loss-g-kl: 1.2068 lr: 0.0002 grad_norm_g: 801.9881 grad_norm_d: 48.8432
======> Epoch: 612
Train Epoch: 613 [50.00%] G-Loss: 30.8385 D-Loss: 2.4175 Loss-g-fm: 6.4324 Loss-g-mel: 19.0490 Loss-g-dur: 1.6502 Loss-g-kl: 1.3610 lr: 0.0002 grad_norm_g: 707.9981 grad_norm_d: 4.3668
======> Epoch: 613
Train Epoch: 614 [46.15%] G-Loss: 29.9394 D-Loss: 2.4224 Loss-g-fm: 6.2440 Loss-g-mel: 18.4009 Loss-g-dur: 1.6268 Loss-g-kl: 1.3556 lr: 0.0002 grad_norm_g: 599.4056 grad_norm_d: 46.4589
======> Epoch: 614
Train Epoch: 615 [42.31%] G-Loss: 28.6754 D-Loss: 2.5199 Loss-g-fm: 5.0447 Loss-g-mel: 18.4201 Loss-g-dur: 1.5042 Loss-g-kl: 1.3827 lr: 0.0002 grad_norm_g: 276.4447 grad_norm_d: 19.5033
======> Epoch: 615
Train Epoch: 616 [38.46%] G-Loss: 30.5361 D-Loss: 2.3405 Loss-g-fm: 6.4210 Loss-g-mel: 19.1208 Loss-g-dur: 1.4939 Loss-g-kl: 1.0801 lr: 0.0002 grad_norm_g: 1093.3208 grad_norm_d: 59.9137
Saving model and optimizer state at iteration 616 to /ZFS4T/tts/data/VITS/model_saved/G_16000.pth
Saving model and optimizer state at iteration 616 to /ZFS4T/tts/data/VITS/model_saved/D_16000.pth
======> Epoch: 616
Train Epoch: 617 [34.62%] G-Loss: 23.4156 D-Loss: 2.4580 Loss-g-fm: 5.9000 Loss-g-mel: 12.4325 Loss-g-dur: 1.0341 Loss-g-kl: 1.3731 lr: 0.0002 grad_norm_g: 855.2960 grad_norm_d: 27.6932
======> Epoch: 617
Train Epoch: 618 [30.77%] G-Loss: 23.9113 D-Loss: 2.4989 Loss-g-fm: 6.4550 Loss-g-mel: 12.6124 Loss-g-dur: 1.1383 Loss-g-kl: 1.2077 lr: 0.0002 grad_norm_g: 995.0650 grad_norm_d: 37.9910
======> Epoch: 618
Train Epoch: 619 [26.92%] G-Loss: 31.4753 D-Loss: 2.2682 Loss-g-fm: 6.7122 Loss-g-mel: 19.0629 Loss-g-dur: 1.6156 Loss-g-kl: 1.2999 lr: 0.0002 grad_norm_g: 970.6357 grad_norm_d: 49.6948
======> Epoch: 619
Train Epoch: 620 [23.08%] G-Loss: 30.8318 D-Loss: 2.3392 Loss-g-fm: 6.4377 Loss-g-mel: 18.9083 Loss-g-dur: 1.5889 Loss-g-kl: 1.2698 lr: 0.0002 grad_norm_g: 1038.4027 grad_norm_d: 60.4294
======> Epoch: 620
Train Epoch: 621 [19.23%] G-Loss: 30.1465 D-Loss: 2.2402 Loss-g-fm: 6.3311 Loss-g-mel: 18.7464 Loss-g-dur: 1.5065 Loss-g-kl: 1.1377 lr: 0.0002 grad_norm_g: 896.3545 grad_norm_d: 31.9270
======> Epoch: 621
Train Epoch: 622 [15.38%] G-Loss: 31.4173 D-Loss: 2.4001 Loss-g-fm: 6.0447 Loss-g-mel: 19.7569 Loss-g-dur: 1.6484 Loss-g-kl: 1.5886 lr: 0.0002 grad_norm_g: 150.7449 grad_norm_d: 47.0390
======> Epoch: 622
Train Epoch: 623 [11.54%] G-Loss: 28.5222 D-Loss: 2.4006 Loss-g-fm: 5.6617 Loss-g-mel: 17.8921 Loss-g-dur: 1.4924 Loss-g-kl: 1.1422 lr: 0.0002 grad_norm_g: 1037.9897 grad_norm_d: 57.2797
======> Epoch: 623
Train Epoch: 624 [7.69%] G-Loss: 29.9145 D-Loss: 2.3136 Loss-g-fm: 5.9163 Loss-g-mel: 18.6771 Loss-g-dur: 1.5054 Loss-g-kl: 1.3453 lr: 0.0002 grad_norm_g: 956.3280 grad_norm_d: 49.8456
======> Epoch: 624
Train Epoch: 625 [3.85%] G-Loss: 30.0688 D-Loss: 2.3111 Loss-g-fm: 6.2821 Loss-g-mel: 18.4839 Loss-g-dur: 1.5175 Loss-g-kl: 1.2135 lr: 0.0002 grad_norm_g: 1168.5128 grad_norm_d: 42.3012
======> Epoch: 625
Train Epoch: 626 [0.00%] G-Loss: 28.5680 D-Loss: 2.4815 Loss-g-fm: 5.6537 Loss-g-mel: 17.8454 Loss-g-dur: 1.4887 Loss-g-kl: 1.2411 lr: 0.0002 grad_norm_g: 169.4275 grad_norm_d: 24.5066
Train Epoch: 626 [96.15%] G-Loss: 29.0176 D-Loss: 2.3019 Loss-g-fm: 5.7739 Loss-g-mel: 17.6471 Loss-g-dur: 1.5480 Loss-g-kl: 1.3105 lr: 0.0002 grad_norm_g: 1016.5351 grad_norm_d: 34.9982
======> Epoch: 626
Train Epoch: 627 [92.31%] G-Loss: 28.9227 D-Loss: 2.4045 Loss-g-fm: 5.9134 Loss-g-mel: 18.0090 Loss-g-dur: 1.4828 Loss-g-kl: 1.1273 lr: 0.0002 grad_norm_g: 821.3048 grad_norm_d: 61.0395
======> Epoch: 627
Train Epoch: 628 [88.46%] G-Loss: 31.4830 D-Loss: 2.4810 Loss-g-fm: 6.2017 Loss-g-mel: 19.5133 Loss-g-dur: 1.6827 Loss-g-kl: 1.3568 lr: 0.0002 grad_norm_g: 423.3905 grad_norm_d: 17.2871
======> Epoch: 628
Train Epoch: 629 [84.62%] G-Loss: 28.9870 D-Loss: 2.3501 Loss-g-fm: 5.6251 Loss-g-mel: 18.3623 Loss-g-dur: 1.4876 Loss-g-kl: 1.2550 lr: 0.0002 grad_norm_g: 156.9529 grad_norm_d: 9.0100
======> Epoch: 629
Train Epoch: 630 [80.77%] G-Loss: 30.1884 D-Loss: 2.3482 Loss-g-fm: 6.2723 Loss-g-mel: 18.9174 Loss-g-dur: 1.6233 Loss-g-kl: 1.1589 lr: 0.0002 grad_norm_g: 210.2684 grad_norm_d: 32.2024
======> Epoch: 630
Train Epoch: 631 [76.92%] G-Loss: 30.1876 D-Loss: 2.2052 Loss-g-fm: 6.6356 Loss-g-mel: 18.1701 Loss-g-dur: 1.4767 Loss-g-kl: 1.4964 lr: 0.0002 grad_norm_g: 1127.1557 grad_norm_d: 84.3094
======> Epoch: 631
Train Epoch: 632 [73.08%] G-Loss: 30.4433 D-Loss: 2.3922 Loss-g-fm: 6.2073 Loss-g-mel: 19.3606 Loss-g-dur: 1.4912 Loss-g-kl: 1.2212 lr: 0.0002 grad_norm_g: 124.2166 grad_norm_d: 17.7169
======> Epoch: 632
Train Epoch: 633 [69.23%] G-Loss: 31.1208 D-Loss: 2.4101 Loss-g-fm: 5.8221 Loss-g-mel: 19.7003 Loss-g-dur: 1.5158 Loss-g-kl: 1.2461 lr: 0.0002 grad_norm_g: 117.7829 grad_norm_d: 13.1350
======> Epoch: 633
Train Epoch: 634 [65.38%] G-Loss: 30.9175 D-Loss: 2.2664 Loss-g-fm: 6.5891 Loss-g-mel: 19.0035 Loss-g-dur: 1.4989 Loss-g-kl: 1.2970 lr: 0.0002 grad_norm_g: 798.8100 grad_norm_d: 50.5834
======> Epoch: 634
Train Epoch: 635 [61.54%] G-Loss: 28.9514 D-Loss: 2.4059 Loss-g-fm: 5.4081 Loss-g-mel: 18.6404 Loss-g-dur: 1.5510 Loss-g-kl: 1.1619 lr: 0.0002 grad_norm_g: 333.1883 grad_norm_d: 32.9635
======> Epoch: 635
Train Epoch: 636 [57.69%] G-Loss: 29.5781 D-Loss: 2.4955 Loss-g-fm: 5.8621 Loss-g-mel: 18.2191 Loss-g-dur: 1.4968 Loss-g-kl: 1.2788 lr: 0.0002 grad_norm_g: 662.7750 grad_norm_d: 35.1859
======> Epoch: 636
Train Epoch: 637 [53.85%] G-Loss: 24.1457 D-Loss: 2.2663 Loss-g-fm: 6.7389 Loss-g-mel: 11.9018 Loss-g-dur: 1.0502 Loss-g-kl: 1.2416 lr: 0.0002 grad_norm_g: 1346.5055 grad_norm_d: 43.7459
======> Epoch: 637
Train Epoch: 638 [50.00%] G-Loss: 30.1320 D-Loss: 2.3468 Loss-g-fm: 6.5349 Loss-g-mel: 18.7283 Loss-g-dur: 1.4974 Loss-g-kl: 1.0676 lr: 0.0002 grad_norm_g: 906.4065 grad_norm_d: 62.4362
======> Epoch: 638
Train Epoch: 639 [46.15%] G-Loss: 29.8593 D-Loss: 2.3763 Loss-g-fm: 6.2455 Loss-g-mel: 18.5196 Loss-g-dur: 1.5195 Loss-g-kl: 1.1335 lr: 0.0002 grad_norm_g: 293.7387 grad_norm_d: 50.3088
======> Epoch: 639
Train Epoch: 640 [42.31%] G-Loss: 29.2741 D-Loss: 2.4119 Loss-g-fm: 5.8817 Loss-g-mel: 18.0595 Loss-g-dur: 1.5110 Loss-g-kl: 1.3336 lr: 0.0002 grad_norm_g: 1095.7426 grad_norm_d: 53.1207
======> Epoch: 640
Train Epoch: 641 [38.46%] G-Loss: 30.6759 D-Loss: 2.2506 Loss-g-fm: 6.4482 Loss-g-mel: 18.9092 Loss-g-dur: 1.6284 Loss-g-kl: 1.3499 lr: 0.0002 grad_norm_g: 785.2560 grad_norm_d: 50.1627
======> Epoch: 641
Train Epoch: 642 [34.62%] G-Loss: 30.6500 D-Loss: 2.4397 Loss-g-fm: 6.5161 Loss-g-mel: 19.0708 Loss-g-dur: 1.6868 Loss-g-kl: 1.1506 lr: 0.0002 grad_norm_g: 953.1769 grad_norm_d: 72.7479
======> Epoch: 642
Train Epoch: 643 [30.77%] G-Loss: 31.0700 D-Loss: 2.3179 Loss-g-fm: 6.2436 Loss-g-mel: 19.3053 Loss-g-dur: 1.4955 Loss-g-kl: 1.4454 lr: 0.0002 grad_norm_g: 677.6157 grad_norm_d: 19.2991
======> Epoch: 643
Train Epoch: 644 [26.92%] G-Loss: 29.0470 D-Loss: 2.4086 Loss-g-fm: 5.7797 Loss-g-mel: 18.3205 Loss-g-dur: 1.4903 Loss-g-kl: 1.3416 lr: 0.0002 grad_norm_g: 898.7805 grad_norm_d: 59.7206
======> Epoch: 644
Train Epoch: 645 [23.08%] G-Loss: 23.6721 D-Loss: 2.2673 Loss-g-fm: 7.1212 Loss-g-mel: 11.6602 Loss-g-dur: 1.0356 Loss-g-kl: 1.2712 lr: 0.0002 grad_norm_g: 1041.8220 grad_norm_d: 31.1410
======> Epoch: 645
Train Epoch: 646 [19.23%] G-Loss: 31.6416 D-Loss: 2.3050 Loss-g-fm: 6.7736 Loss-g-mel: 19.5948 Loss-g-dur: 1.6173 Loss-g-kl: 1.3238 lr: 0.0002 grad_norm_g: 565.7115 grad_norm_d: 31.2242
======> Epoch: 646
Train Epoch: 647 [15.38%] G-Loss: 25.6642 D-Loss: 2.3645 Loss-g-fm: 5.2600 Loss-g-mel: 15.6654 Loss-g-dur: 1.2410 Loss-g-kl: 1.1528 lr: 0.0002 grad_norm_g: 1120.4111 grad_norm_d: 75.4567
======> Epoch: 647
Train Epoch: 648 [11.54%] G-Loss: 24.0359 D-Loss: 2.3197 Loss-g-fm: 7.1104 Loss-g-mel: 11.4533 Loss-g-dur: 1.1581 Loss-g-kl: 1.4244 lr: 0.0002 grad_norm_g: 1292.8169 grad_norm_d: 52.3759
======> Epoch: 648
Train Epoch: 649 [7.69%] G-Loss: 31.8767 D-Loss: 2.2950 Loss-g-fm: 7.0809 Loss-g-mel: 19.6355 Loss-g-dur: 1.4710 Loss-g-kl: 1.2661 lr: 0.0002 grad_norm_g: 1142.6756 grad_norm_d: 65.3881
======> Epoch: 649
Train Epoch: 650 [3.85%] G-Loss: 32.8900 D-Loss: 2.2579 Loss-g-fm: 6.9471 Loss-g-mel: 20.3324 Loss-g-dur: 1.6688 Loss-g-kl: 1.5851 lr: 0.0002 grad_norm_g: 651.6534 grad_norm_d: 49.9482
======> Epoch: 650
Train Epoch: 651 [0.00%] G-Loss: 30.6935 D-Loss: 2.4166 Loss-g-fm: 6.4054 Loss-g-mel: 18.8770 Loss-g-dur: 1.5147 Loss-g-kl: 1.3263 lr: 0.0002 grad_norm_g: 1044.8559 grad_norm_d: 69.0958
Train Epoch: 651 [96.15%] G-Loss: 31.0812 D-Loss: 2.3993 Loss-g-fm: 6.6094 Loss-g-mel: 19.1938 Loss-g-dur: 1.6075 Loss-g-kl: 1.4300 lr: 0.0002 grad_norm_g: 556.4279 grad_norm_d: 59.0844
======> Epoch: 651
Train Epoch: 652 [92.31%] G-Loss: 29.5161 D-Loss: 2.3407 Loss-g-fm: 6.1197 Loss-g-mel: 18.4201 Loss-g-dur: 1.4937 Loss-g-kl: 1.0615 lr: 0.0002 grad_norm_g: 58.3499 grad_norm_d: 29.6929
======> Epoch: 652
Train Epoch: 653 [88.46%] G-Loss: 31.3807 D-Loss: 2.2757 Loss-g-fm: 6.6941 Loss-g-mel: 19.3106 Loss-g-dur: 1.5086 Loss-g-kl: 1.4634 lr: 0.0002 grad_norm_g: 963.9575 grad_norm_d: 74.6962
======> Epoch: 653
Train Epoch: 654 [84.62%] G-Loss: 31.0109 D-Loss: 2.3150 Loss-g-fm: 6.6397 Loss-g-mel: 18.9423 Loss-g-dur: 1.5416 Loss-g-kl: 1.4164 lr: 0.0002 grad_norm_g: 915.8976 grad_norm_d: 46.5278
======> Epoch: 654
Train Epoch: 655 [80.77%] G-Loss: 32.1270 D-Loss: 2.3368 Loss-g-fm: 7.1323 Loss-g-mel: 19.1295 Loss-g-dur: 1.6963 Loss-g-kl: 1.5882 lr: 0.0002 grad_norm_g: 951.8207 grad_norm_d: 54.7340
======> Epoch: 655
Train Epoch: 656 [76.92%] G-Loss: 31.6843 D-Loss: 2.3422 Loss-g-fm: 6.9848 Loss-g-mel: 18.9015 Loss-g-dur: 1.6520 Loss-g-kl: 1.3405 lr: 0.0002 grad_norm_g: 1042.0136 grad_norm_d: 56.4542
======> Epoch: 656
Train Epoch: 657 [73.08%] G-Loss: 28.9227 D-Loss: 2.3436 Loss-g-fm: 5.8699 Loss-g-mel: 17.7142 Loss-g-dur: 1.4887 Loss-g-kl: 1.3933 lr: 0.0002 grad_norm_g: 121.9142 grad_norm_d: 16.9404
======> Epoch: 657
Train Epoch: 658 [69.23%] G-Loss: 31.1195 D-Loss: 2.2681 Loss-g-fm: 7.0455 Loss-g-mel: 18.5822 Loss-g-dur: 1.6336 Loss-g-kl: 1.3513 lr: 0.0002 grad_norm_g: 1303.2297 grad_norm_d: 57.0206
======> Epoch: 658
Train Epoch: 659 [65.38%] G-Loss: 29.4290 D-Loss: 2.2907 Loss-g-fm: 5.8824 Loss-g-mel: 18.1650 Loss-g-dur: 1.5661 Loss-g-kl: 1.2076 lr: 0.0002 grad_norm_g: 1089.8477 grad_norm_d: 74.7323
======> Epoch: 659
Train Epoch: 660 [61.54%] G-Loss: 31.8634 D-Loss: 2.3735 Loss-g-fm: 6.9842 Loss-g-mel: 19.3490 Loss-g-dur: 1.6077 Loss-g-kl: 1.4203 lr: 0.0002 grad_norm_g: 1155.9720 grad_norm_d: 61.6940
======> Epoch: 660
Train Epoch: 661 [57.69%] G-Loss: 28.1358 D-Loss: 2.4555 Loss-g-fm: 5.4848 Loss-g-mel: 17.3220 Loss-g-dur: 1.4745 Loss-g-kl: 1.2172 lr: 0.0002 grad_norm_g: 1144.1546 grad_norm_d: 70.2049
======> Epoch: 661
Train Epoch: 662 [53.85%] G-Loss: 24.5242 D-Loss: 2.3984 Loss-g-fm: 7.5502 Loss-g-mel: 11.6444 Loss-g-dur: 1.0155 Loss-g-kl: 1.2554 lr: 0.0002 grad_norm_g: 1269.7562 grad_norm_d: 54.9793
======> Epoch: 662
Train Epoch: 663 [50.00%] G-Loss: 31.6827 D-Loss: 2.2735 Loss-g-fm: 7.0457 Loss-g-mel: 19.0867 Loss-g-dur: 1.5199 Loss-g-kl: 1.5649 lr: 0.0002 grad_norm_g: 121.8940 grad_norm_d: 34.9172
======> Epoch: 663
Train Epoch: 664 [46.15%] G-Loss: 30.0343 D-Loss: 2.3901 Loss-g-fm: 6.2286 Loss-g-mel: 18.9145 Loss-g-dur: 1.5966 Loss-g-kl: 1.2930 lr: 0.0002 grad_norm_g: 888.7791 grad_norm_d: 55.9408
======> Epoch: 664
Train Epoch: 665 [42.31%] G-Loss: 31.0702 D-Loss: 2.4195 Loss-g-fm: 6.7172 Loss-g-mel: 18.8411 Loss-g-dur: 1.5715 Loss-g-kl: 1.4901 lr: 0.0002 grad_norm_g: 1025.1854 grad_norm_d: 72.0669
======> Epoch: 665
Train Epoch: 666 [38.46%] G-Loss: 29.0374 D-Loss: 2.7465 Loss-g-fm: 5.5055 Loss-g-mel: 18.5045 Loss-g-dur: 1.5604 Loss-g-kl: 1.2662 lr: 0.0002 grad_norm_g: 460.5457 grad_norm_d: 84.7970
======> Epoch: 666
Train Epoch: 667 [34.62%] G-Loss: 31.4306 D-Loss: 2.4168 Loss-g-fm: 6.4689 Loss-g-mel: 19.4601 Loss-g-dur: 1.6429 Loss-g-kl: 1.4448 lr: 0.0002 grad_norm_g: 619.4652 grad_norm_d: 34.2028
======> Epoch: 667
Train Epoch: 668 [30.77%] G-Loss: 23.4050 D-Loss: 2.3614 Loss-g-fm: 6.3753 Loss-g-mel: 12.2261 Loss-g-dur: 1.0006 Loss-g-kl: 1.2833 lr: 0.0002 grad_norm_g: 666.7625 grad_norm_d: 34.2559
======> Epoch: 668
Train Epoch: 669 [26.92%] G-Loss: 32.0706 D-Loss: 2.3921 Loss-g-fm: 6.9883 Loss-g-mel: 19.6639 Loss-g-dur: 1.4806 Loss-g-kl: 1.3117 lr: 0.0002 grad_norm_g: 378.7763 grad_norm_d: 23.5744
======> Epoch: 669
Train Epoch: 670 [23.08%] G-Loss: 29.6461 D-Loss: 2.3595 Loss-g-fm: 6.3090 Loss-g-mel: 18.2536 Loss-g-dur: 1.5059 Loss-g-kl: 1.4092 lr: 0.0002 grad_norm_g: 732.6455 grad_norm_d: 29.5089
======> Epoch: 670
Train Epoch: 671 [19.23%] G-Loss: 32.0024 D-Loss: 2.3098 Loss-g-fm: 7.1200 Loss-g-mel: 19.7977 Loss-g-dur: 1.4845 Loss-g-kl: 1.1280 lr: 0.0002 grad_norm_g: 1099.9823 grad_norm_d: 46.0059
======> Epoch: 671
Train Epoch: 672 [15.38%] G-Loss: 31.2340 D-Loss: 2.4432 Loss-g-fm: 6.9212 Loss-g-mel: 19.0377 Loss-g-dur: 1.5525 Loss-g-kl: 1.3723 lr: 0.0002 grad_norm_g: 871.7053 grad_norm_d: 74.1403
======> Epoch: 672
Train Epoch: 673 [11.54%] G-Loss: 29.5870 D-Loss: 2.4647 Loss-g-fm: 6.2037 Loss-g-mel: 18.4408 Loss-g-dur: 1.4828 Loss-g-kl: 1.1463 lr: 0.0002 grad_norm_g: 1211.9113 grad_norm_d: 84.9618
======> Epoch: 673
Train Epoch: 674 [7.69%] G-Loss: 31.6135 D-Loss: 2.2501 Loss-g-fm: 7.1016 Loss-g-mel: 19.2064 Loss-g-dur: 1.4807 Loss-g-kl: 1.3543 lr: 0.0002 grad_norm_g: 922.2805 grad_norm_d: 49.1907
======> Epoch: 674
Train Epoch: 675 [3.85%] G-Loss: 31.2955 D-Loss: 2.3271 Loss-g-fm: 6.7835 Loss-g-mel: 19.3068 Loss-g-dur: 1.5662 Loss-g-kl: 1.4188 lr: 0.0002 grad_norm_g: 831.2506 grad_norm_d: 56.8541
======> Epoch: 675
Train Epoch: 676 [0.00%] G-Loss: 30.2251 D-Loss: 2.3740 Loss-g-fm: 6.5531 Loss-g-mel: 18.7545 Loss-g-dur: 1.5245 Loss-g-kl: 1.2689 lr: 0.0002 grad_norm_g: 576.3449 grad_norm_d: 62.4183
Train Epoch: 676 [96.15%] G-Loss: 29.9195 D-Loss: 2.4379 Loss-g-fm: 5.9310 Loss-g-mel: 18.8218 Loss-g-dur: 1.5936 Loss-g-kl: 1.5387 lr: 0.0002 grad_norm_g: 867.3425 grad_norm_d: 60.8985
======> Epoch: 676
Train Epoch: 677 [92.31%] G-Loss: 28.7148 D-Loss: 2.4348 Loss-g-fm: 5.7298 Loss-g-mel: 18.0674 Loss-g-dur: 1.4813 Loss-g-kl: 1.3084 lr: 0.0002 grad_norm_g: 805.9927 grad_norm_d: 49.2814
======> Epoch: 677
Train Epoch: 678 [88.46%] G-Loss: 31.8365 D-Loss: 2.2166 Loss-g-fm: 7.4430 Loss-g-mel: 18.9805 Loss-g-dur: 1.4774 Loss-g-kl: 1.3968 lr: 0.0002 grad_norm_g: 1033.1431 grad_norm_d: 72.7700
======> Epoch: 678
Train Epoch: 679 [84.62%] G-Loss: 30.8974 D-Loss: 2.3297 Loss-g-fm: 6.4975 Loss-g-mel: 18.9307 Loss-g-dur: 1.6723 Loss-g-kl: 1.3201 lr: 0.0002 grad_norm_g: 1174.9845 grad_norm_d: 65.0472
======> Epoch: 679
Train Epoch: 680 [80.77%] G-Loss: 27.5390 D-Loss: 2.4544 Loss-g-fm: 5.0888 Loss-g-mel: 17.6391 Loss-g-dur: 1.4955 Loss-g-kl: 1.2568 lr: 0.0002 grad_norm_g: 379.4937 grad_norm_d: 65.9373
======> Epoch: 680
Train Epoch: 681 [76.92%] G-Loss: 33.0772 D-Loss: 2.1656 Loss-g-fm: 7.4963 Loss-g-mel: 19.5641 Loss-g-dur: 1.5761 Loss-g-kl: 1.4736 lr: 0.0002 grad_norm_g: 1100.5537 grad_norm_d: 49.5448
======> Epoch: 681
Train Epoch: 682 [73.08%] G-Loss: 30.4437 D-Loss: 2.5193 Loss-g-fm: 6.5380 Loss-g-mel: 18.5719 Loss-g-dur: 1.6409 Loss-g-kl: 1.3057 lr: 0.0002 grad_norm_g: 423.3641 grad_norm_d: 44.5320
======> Epoch: 682
Train Epoch: 683 [69.23%] G-Loss: 29.5360 D-Loss: 2.3672 Loss-g-fm: 6.4090 Loss-g-mel: 18.3974 Loss-g-dur: 1.5075 Loss-g-kl: 1.0729 lr: 0.0002 grad_norm_g: 690.7770 grad_norm_d: 69.7526
======> Epoch: 683
Train Epoch: 684 [65.38%] G-Loss: 32.5270 D-Loss: 2.3331 Loss-g-fm: 7.4666 Loss-g-mel: 19.6476 Loss-g-dur: 1.6588 Loss-g-kl: 1.5018 lr: 0.0002 grad_norm_g: 983.1045 grad_norm_d: 63.8894
======> Epoch: 684
Train Epoch: 685 [61.54%] G-Loss: 29.0575 D-Loss: 2.3255 Loss-g-fm: 6.0854 Loss-g-mel: 17.8874 Loss-g-dur: 1.4960 Loss-g-kl: 1.2120 lr: 0.0002 grad_norm_g: 1018.5312 grad_norm_d: 74.5108
======> Epoch: 685
Train Epoch: 686 [57.69%] G-Loss: 30.1500 D-Loss: 2.5646 Loss-g-fm: 6.3417 Loss-g-mel: 18.2179 Loss-g-dur: 1.4982 Loss-g-kl: 1.2047 lr: 0.0002 grad_norm_g: 963.3944 grad_norm_d: 67.2491
======> Epoch: 686
Train Epoch: 687 [53.85%] G-Loss: 30.0467 D-Loss: 2.3588 Loss-g-fm: 6.7076 Loss-g-mel: 18.2717 Loss-g-dur: 1.4738 Loss-g-kl: 1.0666 lr: 0.0002 grad_norm_g: 1236.7926 grad_norm_d: 95.9589
======> Epoch: 687
Train Epoch: 688 [50.00%] G-Loss: 29.7028 D-Loss: 2.4094 Loss-g-fm: 5.9824 Loss-g-mel: 18.7461 Loss-g-dur: 1.4738 Loss-g-kl: 1.2393 lr: 0.0002 grad_norm_g: 938.7645 grad_norm_d: 93.8563
======> Epoch: 688
Train Epoch: 689 [46.15%] G-Loss: 31.3192 D-Loss: 2.2530 Loss-g-fm: 6.8644 Loss-g-mel: 19.0521 Loss-g-dur: 1.5368 Loss-g-kl: 1.2263 lr: 0.0002 grad_norm_g: 496.4556 grad_norm_d: 65.6982
======> Epoch: 689
Train Epoch: 690 [42.31%] G-Loss: 30.9967 D-Loss: 2.2202 Loss-g-fm: 7.2141 Loss-g-mel: 18.6581 Loss-g-dur: 1.4427 Loss-g-kl: 1.1492 lr: 0.0002 grad_norm_g: 1052.6766 grad_norm_d: 65.3024
======> Epoch: 690
Train Epoch: 691 [38.46%] G-Loss: 31.5831 D-Loss: 2.4803 Loss-g-fm: 6.6806 Loss-g-mel: 19.1999 Loss-g-dur: 1.4877 Loss-g-kl: 1.3932 lr: 0.0002 grad_norm_g: 731.6793 grad_norm_d: 60.6330
======> Epoch: 691
Train Epoch: 692 [34.62%] G-Loss: 28.8452 D-Loss: 2.4027 Loss-g-fm: 6.4203 Loss-g-mel: 16.7976 Loss-g-dur: 1.2540 Loss-g-kl: 1.3236 lr: 0.0002 grad_norm_g: 1223.0822 grad_norm_d: 85.7760
======> Epoch: 692
Train Epoch: 693 [30.77%] G-Loss: 30.9873 D-Loss: 2.2099 Loss-g-fm: 6.7089 Loss-g-mel: 18.6262 Loss-g-dur: 1.6693 Loss-g-kl: 1.4355 lr: 0.0002 grad_norm_g: 762.7015 grad_norm_d: 39.9160
Saving model and optimizer state at iteration 693 to /ZFS4T/tts/data/VITS/model_saved/G_18000.pth
Saving model and optimizer state at iteration 693 to /ZFS4T/tts/data/VITS/model_saved/D_18000.pth
======> Epoch: 693
Train Epoch: 694 [26.92%] G-Loss: 28.7318 D-Loss: 2.5792 Loss-g-fm: 5.8148 Loss-g-mel: 17.9930 Loss-g-dur: 1.5042 Loss-g-kl: 1.3062 lr: 0.0002 grad_norm_g: 806.9125 grad_norm_d: 48.9352
======> Epoch: 694
Train Epoch: 695 [23.08%] G-Loss: 24.4125 D-Loss: 2.3314 Loss-g-fm: 6.9977 Loss-g-mel: 12.4669 Loss-g-dur: 1.0233 Loss-g-kl: 1.3431 lr: 0.0002 grad_norm_g: 282.8733 grad_norm_d: 29.1690
======> Epoch: 695
Train Epoch: 696 [19.23%] G-Loss: 25.2297 D-Loss: 2.2580 Loss-g-fm: 7.9203 Loss-g-mel: 12.2456 Loss-g-dur: 1.0589 Loss-g-kl: 1.2635 lr: 0.0002 grad_norm_g: 1291.8064 grad_norm_d: 40.5709
======> Epoch: 696
Train Epoch: 697 [15.38%] G-Loss: 30.5335 D-Loss: 2.3457 Loss-g-fm: 6.2738 Loss-g-mel: 19.3910 Loss-g-dur: 1.4884 Loss-g-kl: 1.1598 lr: 0.0002 grad_norm_g: 125.6983 grad_norm_d: 22.9987
======> Epoch: 697
Train Epoch: 698 [11.54%] G-Loss: 32.1769 D-Loss: 2.3530 Loss-g-fm: 7.5060 Loss-g-mel: 19.2572 Loss-g-dur: 1.4976 Loss-g-kl: 1.1775 lr: 0.0002 grad_norm_g: 1275.1419 grad_norm_d: 72.6004
======> Epoch: 698
Train Epoch: 699 [7.69%] G-Loss: 31.5270 D-Loss: 2.3707 Loss-g-fm: 7.0792 Loss-g-mel: 18.4734 Loss-g-dur: 1.5175 Loss-g-kl: 1.5605 lr: 0.0002 grad_norm_g: 1011.1158 grad_norm_d: 66.1863
======> Epoch: 699
Train Epoch: 700 [3.85%] G-Loss: 30.7962 D-Loss: 2.2456 Loss-g-fm: 7.1840 Loss-g-mel: 18.5981 Loss-g-dur: 1.4657 Loss-g-kl: 1.1430 lr: 0.0002 grad_norm_g: 926.2742 grad_norm_d: 86.4978
======> Epoch: 700
Train Epoch: 701 [0.00%] G-Loss: 30.9483 D-Loss: 2.4566 Loss-g-fm: 6.5468 Loss-g-mel: 19.2617 Loss-g-dur: 1.5368 Loss-g-kl: 1.3513 lr: 0.0002 grad_norm_g: 323.4790 grad_norm_d: 57.1116
Train Epoch: 701 [96.15%] G-Loss: 30.6479 D-Loss: 2.2881 Loss-g-fm: 6.4250 Loss-g-mel: 19.0732 Loss-g-dur: 1.5483 Loss-g-kl: 1.4949 lr: 0.0002 grad_norm_g: 386.3707 grad_norm_d: 16.8660
======> Epoch: 701
Train Epoch: 702 [92.31%] G-Loss: 24.2879 D-Loss: 2.3480 Loss-g-fm: 7.6042 Loss-g-mel: 11.7269 Loss-g-dur: 0.9682 Loss-g-kl: 1.3884 lr: 0.0002 grad_norm_g: 642.1583 grad_norm_d: 35.1923
======> Epoch: 702
Train Epoch: 703 [88.46%] G-Loss: 29.7807 D-Loss: 2.3848 Loss-g-fm: 6.0370 Loss-g-mel: 18.5712 Loss-g-dur: 1.4411 Loss-g-kl: 1.4001 lr: 0.0002 grad_norm_g: 169.9716 grad_norm_d: 26.7452
======> Epoch: 703
Train Epoch: 704 [84.62%] G-Loss: 30.3467 D-Loss: 2.2634 Loss-g-fm: 6.8890 Loss-g-mel: 18.2260 Loss-g-dur: 1.4812 Loss-g-kl: 1.1779 lr: 0.0002 grad_norm_g: 870.4005 grad_norm_d: 98.8744
======> Epoch: 704
Train Epoch: 705 [80.77%] G-Loss: 31.3290 D-Loss: 2.3879 Loss-g-fm: 6.4601 Loss-g-mel: 18.9233 Loss-g-dur: 1.6020 Loss-g-kl: 1.4729 lr: 0.0002 grad_norm_g: 686.3257 grad_norm_d: 82.8675
======> Epoch: 705
Train Epoch: 706 [76.92%] G-Loss: 28.2212 D-Loss: 2.4640 Loss-g-fm: 5.5780 Loss-g-mel: 17.8867 Loss-g-dur: 1.4847 Loss-g-kl: 1.0204 lr: 0.0002 grad_norm_g: 114.6926 grad_norm_d: 16.5858
======> Epoch: 706
Train Epoch: 707 [73.08%] G-Loss: 28.0135 D-Loss: 2.2285 Loss-g-fm: 6.5926 Loss-g-mel: 16.4546 Loss-g-dur: 1.2702 Loss-g-kl: 1.2279 lr: 0.0002 grad_norm_g: 607.0252 grad_norm_d: 61.0222
======> Epoch: 707
Train Epoch: 708 [69.23%] G-Loss: 30.8748 D-Loss: 2.2159 Loss-g-fm: 7.0985 Loss-g-mel: 18.3979 Loss-g-dur: 1.4738 Loss-g-kl: 1.2073 lr: 0.0002 grad_norm_g: 942.2159 grad_norm_d: 39.3051
======> Epoch: 708
Train Epoch: 709 [65.38%] G-Loss: 28.8720 D-Loss: 2.4759 Loss-g-fm: 6.4874 Loss-g-mel: 17.0068 Loss-g-dur: 1.3217 Loss-g-kl: 1.2482 lr: 0.0002 grad_norm_g: 765.5378 grad_norm_d: 44.5611
======> Epoch: 709
Train Epoch: 710 [61.54%] G-Loss: 32.3234 D-Loss: 2.4729 Loss-g-fm: 7.1233 Loss-g-mel: 19.1562 Loss-g-dur: 1.5218 Loss-g-kl: 1.5033 lr: 0.0002 grad_norm_g: 351.5587 grad_norm_d: 18.7776
======> Epoch: 710
Train Epoch: 711 [57.69%] G-Loss: 29.8920 D-Loss: 2.2955 Loss-g-fm: 6.7112 Loss-g-mel: 18.1120 Loss-g-dur: 1.4942 Loss-g-kl: 1.2825 lr: 0.0002 grad_norm_g: 764.6621 grad_norm_d: 52.1278
======> Epoch: 711
Train Epoch: 712 [53.85%] G-Loss: 30.4304 D-Loss: 2.2652 Loss-g-fm: 6.8462 Loss-g-mel: 18.3296 Loss-g-dur: 1.4984 Loss-g-kl: 1.2774 lr: 0.0002 grad_norm_g: 982.9858 grad_norm_d: 76.9911
======> Epoch: 712
Train Epoch: 713 [50.00%] G-Loss: 30.5303 D-Loss: 2.5063 Loss-g-fm: 6.3965 Loss-g-mel: 19.3498 Loss-g-dur: 1.4503 Loss-g-kl: 1.2321 lr: 0.0002 grad_norm_g: 489.1778 grad_norm_d: 75.0398
======> Epoch: 713
Train Epoch: 714 [46.15%] G-Loss: 30.1125 D-Loss: 2.3159 Loss-g-fm: 6.3222 Loss-g-mel: 18.5853 Loss-g-dur: 1.5835 Loss-g-kl: 1.3532 lr: 0.0002 grad_norm_g: 396.2678 grad_norm_d: 20.7030
======> Epoch: 714
Train Epoch: 715 [42.31%] G-Loss: 30.3255 D-Loss: 2.3739 Loss-g-fm: 6.4406 Loss-g-mel: 18.6873 Loss-g-dur: 1.4937 Loss-g-kl: 1.2656 lr: 0.0002 grad_norm_g: 374.9119 grad_norm_d: 17.5483
======> Epoch: 715
Train Epoch: 716 [38.46%] G-Loss: 30.1513 D-Loss: 2.3241 Loss-g-fm: 6.6934 Loss-g-mel: 18.2471 Loss-g-dur: 1.4541 Loss-g-kl: 1.3608 lr: 0.0002 grad_norm_g: 933.8474 grad_norm_d: 71.1583
======> Epoch: 716
Train Epoch: 717 [34.62%] G-Loss: 29.0670 D-Loss: 2.5067 Loss-g-fm: 5.7263 Loss-g-mel: 18.2442 Loss-g-dur: 1.6911 Loss-g-kl: 1.4458 lr: 0.0002 grad_norm_g: 212.6007 grad_norm_d: 9.5816
======> Epoch: 717
Train Epoch: 718 [30.77%] G-Loss: 29.9217 D-Loss: 2.4165 Loss-g-fm: 6.3687 Loss-g-mel: 18.3149 Loss-g-dur: 1.4403 Loss-g-kl: 1.3454 lr: 0.0002 grad_norm_g: 1008.5458 grad_norm_d: 96.6623
======> Epoch: 718
Train Epoch: 719 [26.92%] G-Loss: 29.7469 D-Loss: 2.3956 Loss-g-fm: 6.1258 Loss-g-mel: 18.6198 Loss-g-dur: 1.4887 Loss-g-kl: 1.3839 lr: 0.0002 grad_norm_g: 262.4503 grad_norm_d: 23.7179
======> Epoch: 719
Train Epoch: 720 [23.08%] G-Loss: 23.8522 D-Loss: 2.3482 Loss-g-fm: 6.8452 Loss-g-mel: 12.0721 Loss-g-dur: 0.9786 Loss-g-kl: 1.2261 lr: 0.0002 grad_norm_g: 1118.8256 grad_norm_d: 61.5282
======> Epoch: 720
Train Epoch: 721 [19.23%] G-Loss: 32.0802 D-Loss: 2.3560 Loss-g-fm: 7.0985 Loss-g-mel: 19.4206 Loss-g-dur: 1.6582 Loss-g-kl: 1.4329 lr: 0.0002 grad_norm_g: 776.6072 grad_norm_d: 49.6166
======> Epoch: 721
Train Epoch: 722 [15.38%] G-Loss: 30.3797 D-Loss: 2.3976 Loss-g-fm: 6.4908 Loss-g-mel: 18.5351 Loss-g-dur: 1.5389 Loss-g-kl: 1.4531 lr: 0.0002 grad_norm_g: 785.7234 grad_norm_d: 48.7754
======> Epoch: 722
Train Epoch: 723 [11.54%] G-Loss: 30.7150 D-Loss: 2.5060 Loss-g-fm: 6.4511 Loss-g-mel: 18.6318 Loss-g-dur: 1.5946 Loss-g-kl: 1.3105 lr: 0.0002 grad_norm_g: 85.2925 grad_norm_d: 50.5656
======> Epoch: 723
Train Epoch: 724 [7.69%] G-Loss: 31.8361 D-Loss: 2.2735 Loss-g-fm: 7.5210 Loss-g-mel: 18.6346 Loss-g-dur: 1.5593 Loss-g-kl: 1.4456 lr: 0.0002 grad_norm_g: 897.1686 grad_norm_d: 46.1902
======> Epoch: 724
Train Epoch: 725 [3.85%] G-Loss: 31.0840 D-Loss: 2.4904 Loss-g-fm: 6.5357 Loss-g-mel: 19.1692 Loss-g-dur: 1.4511 Loss-g-kl: 1.3968 lr: 0.0002 grad_norm_g: 233.7824 grad_norm_d: 28.4766
======> Epoch: 725
Train Epoch: 726 [0.00%] G-Loss: 22.6993 D-Loss: 2.3174 Loss-g-fm: 6.5260 Loss-g-mel: 11.9806 Loss-g-dur: 0.9831 Loss-g-kl: 1.3221 lr: 0.0002 grad_norm_g: 248.9914 grad_norm_d: 7.3322
Train Epoch: 726 [96.15%] G-Loss: 28.1032 D-Loss: 2.5740 Loss-g-fm: 5.7091 Loss-g-mel: 17.7694 Loss-g-dur: 1.4292 Loss-g-kl: 0.8839 lr: 0.0002 grad_norm_g: 368.2289 grad_norm_d: 55.7532
======> Epoch: 726
Train Epoch: 727 [92.31%] G-Loss: 30.1100 D-Loss: 2.2576 Loss-g-fm: 6.9689 Loss-g-mel: 18.3247 Loss-g-dur: 1.4491 Loss-g-kl: 1.1495 lr: 0.0002 grad_norm_g: 1019.3305 grad_norm_d: 91.5486
======> Epoch: 727
Train Epoch: 728 [88.46%] G-Loss: 23.8860 D-Loss: 2.2465 Loss-g-fm: 7.3161 Loss-g-mel: 11.8279 Loss-g-dur: 0.9797 Loss-g-kl: 1.2274 lr: 0.0002 grad_norm_g: 701.7259 grad_norm_d: 20.4075
======> Epoch: 728
Train Epoch: 729 [84.62%] G-Loss: 33.0716 D-Loss: 2.1716 Loss-g-fm: 7.6097 Loss-g-mel: 19.9959 Loss-g-dur: 1.6355 Loss-g-kl: 1.4013 lr: 0.0002 grad_norm_g: 978.6282 grad_norm_d: 70.7629
======> Epoch: 729
Train Epoch: 730 [80.77%] G-Loss: 31.7253 D-Loss: 2.2904 Loss-g-fm: 6.7644 Loss-g-mel: 19.0999 Loss-g-dur: 1.7001 Loss-g-kl: 1.5908 lr: 0.0002 grad_norm_g: 633.7391 grad_norm_d: 30.5849
======> Epoch: 730
Train Epoch: 731 [76.92%] G-Loss: 29.3604 D-Loss: 2.3436 Loss-g-fm: 6.3349 Loss-g-mel: 17.9332 Loss-g-dur: 1.4370 Loss-g-kl: 1.3163 lr: 0.0002 grad_norm_g: 862.2277 grad_norm_d: 69.4799
======> Epoch: 731
Train Epoch: 732 [73.08%] G-Loss: 30.4040 D-Loss: 2.3497 Loss-g-fm: 6.7409 Loss-g-mel: 18.5381 Loss-g-dur: 1.4576 Loss-g-kl: 1.4588 lr: 0.0002 grad_norm_g: 1025.8000 grad_norm_d: 120.8851
======> Epoch: 732
Train Epoch: 733 [69.23%] G-Loss: 24.2091 D-Loss: 2.3492 Loss-g-fm: 7.4650 Loss-g-mel: 11.7160 Loss-g-dur: 0.9548 Loss-g-kl: 1.3325 lr: 0.0002 grad_norm_g: 1154.4657 grad_norm_d: 42.7563
======> Epoch: 733
Train Epoch: 734 [65.38%] G-Loss: 30.4211 D-Loss: 2.3716 Loss-g-fm: 7.2057 Loss-g-mel: 17.5210 Loss-g-dur: 1.2539 Loss-g-kl: 1.4419 lr: 0.0002 grad_norm_g: 1100.1337 grad_norm_d: 104.8487
======> Epoch: 734
Train Epoch: 735 [61.54%] G-Loss: 29.6923 D-Loss: 2.4411 Loss-g-fm: 6.1689 Loss-g-mel: 18.6052 Loss-g-dur: 1.4476 Loss-g-kl: 1.1393 lr: 0.0002 grad_norm_g: 653.8813 grad_norm_d: 71.9320
======> Epoch: 735
Train Epoch: 736 [57.69%] G-Loss: 23.6629 D-Loss: 2.3404 Loss-g-fm: 6.5964 Loss-g-mel: 12.2098 Loss-g-dur: 1.1119 Loss-g-kl: 1.2193 lr: 0.0002 grad_norm_g: 1051.0489 grad_norm_d: 58.5015
======> Epoch: 736
Train Epoch: 737 [53.85%] G-Loss: 31.1713 D-Loss: 2.3207 Loss-g-fm: 7.1258 Loss-g-mel: 18.7576 Loss-g-dur: 1.4500 Loss-g-kl: 1.3589 lr: 0.0002 grad_norm_g: 731.7241 grad_norm_d: 65.9420
======> Epoch: 737
Train Epoch: 738 [50.00%] G-Loss: 32.9043 D-Loss: 2.4098 Loss-g-fm: 7.6474 Loss-g-mel: 19.5872 Loss-g-dur: 1.5531 Loss-g-kl: 1.5254 lr: 0.0002 grad_norm_g: 1025.5582 grad_norm_d: 66.8709
======> Epoch: 738
Train Epoch: 739 [46.15%] G-Loss: 32.6496 D-Loss: 2.2482 Loss-g-fm: 7.4987 Loss-g-mel: 19.5855 Loss-g-dur: 1.5502 Loss-g-kl: 1.3362 lr: 0.0002 grad_norm_g: 881.5653 grad_norm_d: 64.1350
======> Epoch: 739
Train Epoch: 740 [42.31%] G-Loss: 32.1495 D-Loss: 2.3423 Loss-g-fm: 7.5911 Loss-g-mel: 19.0574 Loss-g-dur: 1.7004 Loss-g-kl: 1.4513 lr: 0.0002 grad_norm_g: 700.6373 grad_norm_d: 59.3999
======> Epoch: 740
Train Epoch: 741 [38.46%] G-Loss: 31.0377 D-Loss: 2.5007 Loss-g-fm: 6.5339 Loss-g-mel: 19.1314 Loss-g-dur: 1.5768 Loss-g-kl: 1.4078 lr: 0.0002 grad_norm_g: 200.0435 grad_norm_d: 69.5381
======> Epoch: 741
Train Epoch: 742 [34.62%] G-Loss: 30.9932 D-Loss: 2.3335 Loss-g-fm: 6.6528 Loss-g-mel: 18.7651 Loss-g-dur: 1.6326 Loss-g-kl: 1.5264 lr: 0.0002 grad_norm_g: 779.6632 grad_norm_d: 58.7528
======> Epoch: 742
Train Epoch: 743 [30.77%] G-Loss: 24.4984 D-Loss: 2.1651 Loss-g-fm: 7.9968 Loss-g-mel: 11.6926 Loss-g-dur: 0.9416 Loss-g-kl: 1.1502 lr: 0.0002 grad_norm_g: 1247.6795 grad_norm_d: 42.8779
======> Epoch: 743
Train Epoch: 744 [26.92%] G-Loss: 31.0018 D-Loss: 2.4705 Loss-g-fm: 6.2915 Loss-g-mel: 19.1346 Loss-g-dur: 1.5260 Loss-g-kl: 1.4117 lr: 0.0002 grad_norm_g: 667.9677 grad_norm_d: 40.6862
======> Epoch: 744
Train Epoch: 745 [23.08%] G-Loss: 28.5786 D-Loss: 2.3339 Loss-g-fm: 6.5024 Loss-g-mel: 17.2041 Loss-g-dur: 1.4257 Loss-g-kl: 1.1694 lr: 0.0002 grad_norm_g: 972.9526 grad_norm_d: 81.0441
======> Epoch: 745
Train Epoch: 746 [19.23%] G-Loss: 29.3418 D-Loss: 2.3727 Loss-g-fm: 6.0726 Loss-g-mel: 18.1052 Loss-g-dur: 1.4835 Loss-g-kl: 1.3292 lr: 0.0002 grad_norm_g: 684.0628 grad_norm_d: 23.4581
======> Epoch: 746
Train Epoch: 747 [15.38%] G-Loss: 31.5477 D-Loss: 2.4315 Loss-g-fm: 6.7842 Loss-g-mel: 19.1729 Loss-g-dur: 1.5156 Loss-g-kl: 1.3739 lr: 0.0002 grad_norm_g: 384.7567 grad_norm_d: 35.5600
======> Epoch: 747
Train Epoch: 748 [11.54%] G-Loss: 28.5758 D-Loss: 2.4205 Loss-g-fm: 6.2461 Loss-g-mel: 17.5795 Loss-g-dur: 1.4254 Loss-g-kl: 1.0133 lr: 0.0002 grad_norm_g: 879.5560 grad_norm_d: 93.0266
======> Epoch: 748
Train Epoch: 749 [7.69%] G-Loss: 29.9072 D-Loss: 2.1879 Loss-g-fm: 7.8281 Loss-g-mel: 17.1583 Loss-g-dur: 1.2680 Loss-g-kl: 1.2913 lr: 0.0002 grad_norm_g: 889.3941 grad_norm_d: 73.0385
======> Epoch: 749
Train Epoch: 750 [3.85%] G-Loss: 31.0298 D-Loss: 2.4759 Loss-g-fm: 6.6480 Loss-g-mel: 19.5704 Loss-g-dur: 1.5410 Loss-g-kl: 1.3287 lr: 0.0002 grad_norm_g: 576.2815 grad_norm_d: 33.9673
======> Epoch: 750
Train Epoch: 751 [0.00%] G-Loss: 31.6079 D-Loss: 2.3279 Loss-g-fm: 7.2493 Loss-g-mel: 19.2784 Loss-g-dur: 1.4912 Loss-g-kl: 1.1515 lr: 0.0002 grad_norm_g: 1068.5321 grad_norm_d: 81.4448
Train Epoch: 751 [96.15%] G-Loss: 30.0014 D-Loss: 2.3491 Loss-g-fm: 7.0049 Loss-g-mel: 18.1424 Loss-g-dur: 1.4449 Loss-g-kl: 1.1642 lr: 0.0002 grad_norm_g: 1154.8415 grad_norm_d: 111.5648
======> Epoch: 751
Train Epoch: 752 [92.31%] G-Loss: 31.9614 D-Loss: 2.2095 Loss-g-fm: 7.2137 Loss-g-mel: 19.3877 Loss-g-dur: 1.5699 Loss-g-kl: 1.2260 lr: 0.0002 grad_norm_g: 685.9869 grad_norm_d: 95.3855
======> Epoch: 752
Train Epoch: 753 [88.46%] G-Loss: 29.0589 D-Loss: 2.4939 Loss-g-fm: 5.9720 Loss-g-mel: 18.1017 Loss-g-dur: 1.4503 Loss-g-kl: 1.4275 lr: 0.0002 grad_norm_g: 563.0160 grad_norm_d: 63.4156
======> Epoch: 753
Train Epoch: 754 [84.62%] G-Loss: 30.1008 D-Loss: 2.3914 Loss-g-fm: 6.4020 Loss-g-mel: 18.4701 Loss-g-dur: 1.4906 Loss-g-kl: 1.1034 lr: 0.0002 grad_norm_g: 649.6477 grad_norm_d: 62.2137
======> Epoch: 754
Train Epoch: 755 [80.77%] G-Loss: 30.7024 D-Loss: 2.4377 Loss-g-fm: 6.8330 Loss-g-mel: 18.6545 Loss-g-dur: 1.5520 Loss-g-kl: 1.3640 lr: 0.0002 grad_norm_g: 292.8520 grad_norm_d: 10.8288
======> Epoch: 755
Train Epoch: 756 [76.92%] G-Loss: 24.9543 D-Loss: 2.1733 Loss-g-fm: 7.5476 Loss-g-mel: 12.4317 Loss-g-dur: 1.0302 Loss-g-kl: 1.4512 lr: 0.0002 grad_norm_g: 293.9047 grad_norm_d: 21.2493
======> Epoch: 756
Train Epoch: 757 [73.08%] G-Loss: 30.8437 D-Loss: 2.2985 Loss-g-fm: 6.8472 Loss-g-mel: 19.1213 Loss-g-dur: 1.4597 Loss-g-kl: 1.1089 lr: 0.0002 grad_norm_g: 859.7471 grad_norm_d: 70.7851
======> Epoch: 757
Train Epoch: 758 [69.23%] G-Loss: 30.4072 D-Loss: 2.2704 Loss-g-fm: 6.8646 Loss-g-mel: 18.3850 Loss-g-dur: 1.6253 Loss-g-kl: 1.3049 lr: 0.0002 grad_norm_g: 421.8922 grad_norm_d: 41.7390
======> Epoch: 758
Train Epoch: 759 [65.38%] G-Loss: 30.3064 D-Loss: 2.2744 Loss-g-fm: 6.9614 Loss-g-mel: 18.1273 Loss-g-dur: 1.4618 Loss-g-kl: 1.2702 lr: 0.0002 grad_norm_g: 935.6562 grad_norm_d: 80.2984
======> Epoch: 759
Train Epoch: 760 [61.54%] G-Loss: 31.4138 D-Loss: 2.3902 Loss-g-fm: 6.9965 Loss-g-mel: 19.0996 Loss-g-dur: 1.5371 Loss-g-kl: 1.4851 lr: 0.0002 grad_norm_g: 501.0603 grad_norm_d: 70.8537
======> Epoch: 760
Train Epoch: 761 [57.69%] G-Loss: 30.2508 D-Loss: 2.3022 Loss-g-fm: 6.6641 Loss-g-mel: 18.6148 Loss-g-dur: 1.4670 Loss-g-kl: 1.1663 lr: 0.0002 grad_norm_g: 542.1599 grad_norm_d: 31.4844
======> Epoch: 761
Train Epoch: 762 [53.85%] G-Loss: 31.4988 D-Loss: 2.4373 Loss-g-fm: 6.9720 Loss-g-mel: 18.9052 Loss-g-dur: 1.5366 Loss-g-kl: 1.4648 lr: 0.0002 grad_norm_g: 476.7913 grad_norm_d: 22.6014
======> Epoch: 762
Train Epoch: 763 [50.00%] G-Loss: 23.9849 D-Loss: 2.4586 Loss-g-fm: 7.4482 Loss-g-mel: 11.3883 Loss-g-dur: 0.9472 Loss-g-kl: 1.2011 lr: 0.0002 grad_norm_g: 1226.7935 grad_norm_d: 45.7249
======> Epoch: 763
Train Epoch: 764 [46.15%] G-Loss: 29.2783 D-Loss: 2.3921 Loss-g-fm: 5.9949 Loss-g-mel: 18.2309 Loss-g-dur: 1.4596 Loss-g-kl: 1.1181 lr: 0.0002 grad_norm_g: 842.7485 grad_norm_d: 64.1076
======> Epoch: 764
Train Epoch: 765 [42.31%] G-Loss: 27.4796 D-Loss: 2.4791 Loss-g-fm: 5.4195 Loss-g-mel: 17.1503 Loss-g-dur: 1.5100 Loss-g-kl: 1.2164 lr: 0.0002 grad_norm_g: 561.6001 grad_norm_d: 80.3323
======> Epoch: 765
Train Epoch: 766 [38.46%] G-Loss: 30.4426 D-Loss: 2.4173 Loss-g-fm: 6.4497 Loss-g-mel: 18.6722 Loss-g-dur: 1.4888 Loss-g-kl: 1.2608 lr: 0.0002 grad_norm_g: 793.0352 grad_norm_d: 88.6298
======> Epoch: 766
Train Epoch: 767 [34.62%] G-Loss: 32.5655 D-Loss: 2.2828 Loss-g-fm: 7.4639 Loss-g-mel: 19.7430 Loss-g-dur: 1.5245 Loss-g-kl: 1.3879 lr: 0.0002 grad_norm_g: 802.1781 grad_norm_d: 59.9912
======> Epoch: 767
Train Epoch: 768 [30.77%] G-Loss: 23.0936 D-Loss: 2.5441 Loss-g-fm: 6.2128 Loss-g-mel: 11.9271 Loss-g-dur: 1.0813 Loss-g-kl: 1.2519 lr: 0.0002 grad_norm_g: 511.5301 grad_norm_d: 29.2880
======> Epoch: 768
Train Epoch: 769 [26.92%] G-Loss: 29.8752 D-Loss: 2.3514 Loss-g-fm: 6.5745 Loss-g-mel: 17.9914 Loss-g-dur: 1.4474 Loss-g-kl: 1.4174 lr: 0.0002 grad_norm_g: 886.0359 grad_norm_d: 89.0600
======> Epoch: 769
Train Epoch: 770 [23.08%] G-Loss: 32.4733 D-Loss: 2.4516 Loss-g-fm: 7.2827 Loss-g-mel: 19.3726 Loss-g-dur: 1.5455 Loss-g-kl: 1.3733 lr: 0.0002 grad_norm_g: 1039.7407 grad_norm_d: 104.1384
Saving model and optimizer state at iteration 770 to /ZFS4T/tts/data/VITS/model_saved/G_20000.pth
Saving model and optimizer state at iteration 770 to /ZFS4T/tts/data/VITS/model_saved/D_20000.pth
======> Epoch: 770
Train Epoch: 771 [19.23%] G-Loss: 29.4366 D-Loss: 2.3140 Loss-g-fm: 6.1169 Loss-g-mel: 18.0964 Loss-g-dur: 1.5163 Loss-g-kl: 1.2081 lr: 0.0002 grad_norm_g: 986.4432 grad_norm_d: 78.3270
======> Epoch: 771
Train Epoch: 772 [15.38%] G-Loss: 30.8297 D-Loss: 2.4243 Loss-g-fm: 6.8671 Loss-g-mel: 18.7909 Loss-g-dur: 1.4461 Loss-g-kl: 1.4349 lr: 0.0002 grad_norm_g: 638.6478 grad_norm_d: 45.6399
======> Epoch: 772
Train Epoch: 773 [11.54%] G-Loss: 24.0508 D-Loss: 2.2030 Loss-g-fm: 7.4856 Loss-g-mel: 11.4512 Loss-g-dur: 0.9609 Loss-g-kl: 1.2928 lr: 0.0002 grad_norm_g: 1157.5505 grad_norm_d: 47.3318
======> Epoch: 773
Train Epoch: 774 [7.69%] G-Loss: 30.3873 D-Loss: 2.3706 Loss-g-fm: 6.5873 Loss-g-mel: 18.6035 Loss-g-dur: 1.4399 Loss-g-kl: 1.3520 lr: 0.0002 grad_norm_g: 184.7430 grad_norm_d: 26.2920
======> Epoch: 774
Train Epoch: 775 [3.85%] G-Loss: 31.3392 D-Loss: 2.3193 Loss-g-fm: 6.9520 Loss-g-mel: 18.9210 Loss-g-dur: 1.4795 Loss-g-kl: 1.4959 lr: 0.0002 grad_norm_g: 475.4192 grad_norm_d: 14.2300
======> Epoch: 775
Train Epoch: 776 [0.00%] G-Loss: 31.5313 D-Loss: 2.2349 Loss-g-fm: 7.2382 Loss-g-mel: 18.7206 Loss-g-dur: 1.4363 Loss-g-kl: 1.1859 lr: 0.0002 grad_norm_g: 1097.2740 grad_norm_d: 62.1059
Train Epoch: 776 [96.15%] G-Loss: 24.3278 D-Loss: 2.2348 Loss-g-fm: 7.8190 Loss-g-mel: 11.3366 Loss-g-dur: 0.9697 Loss-g-kl: 1.3685 lr: 0.0002 grad_norm_g: 1340.6289 grad_norm_d: 76.3469
======> Epoch: 776
Train Epoch: 777 [92.31%] G-Loss: 31.6674 D-Loss: 2.3750 Loss-g-fm: 7.6887 Loss-g-mel: 18.8867 Loss-g-dur: 1.5822 Loss-g-kl: 1.3585 lr: 0.0002 grad_norm_g: 984.0537 grad_norm_d: 73.7335
======> Epoch: 777
Train Epoch: 778 [88.46%] G-Loss: 31.0733 D-Loss: 2.4178 Loss-g-fm: 7.0850 Loss-g-mel: 19.1553 Loss-g-dur: 1.4342 Loss-g-kl: 1.1553 lr: 0.0002 grad_norm_g: 722.5944 grad_norm_d: 35.7646
======> Epoch: 778
Train Epoch: 779 [84.62%] G-Loss: 30.9667 D-Loss: 2.3814 Loss-g-fm: 7.2025 Loss-g-mel: 18.7116 Loss-g-dur: 1.4439 Loss-g-kl: 1.3765 lr: 0.0002 grad_norm_g: 187.6580 grad_norm_d: 12.2679
======> Epoch: 779
Train Epoch: 780 [80.77%] G-Loss: 30.3200 D-Loss: 2.3523 Loss-g-fm: 6.6166 Loss-g-mel: 18.7848 Loss-g-dur: 1.4300 Loss-g-kl: 1.1706 lr: 0.0002 grad_norm_g: 536.5757 grad_norm_d: 17.6574
======> Epoch: 780
Train Epoch: 781 [76.92%] G-Loss: 32.4002 D-Loss: 2.3240 Loss-g-fm: 7.7262 Loss-g-mel: 19.2469 Loss-g-dur: 1.4358 Loss-g-kl: 1.2957 lr: 0.0002 grad_norm_g: 1031.2243 grad_norm_d: 113.7026
======> Epoch: 781
Train Epoch: 782 [73.08%] G-Loss: 30.8769 D-Loss: 2.2342 Loss-g-fm: 7.0820 Loss-g-mel: 18.5354 Loss-g-dur: 1.5604 Loss-g-kl: 1.4297 lr: 0.0002 grad_norm_g: 861.9887 grad_norm_d: 86.7448
======> Epoch: 782
Train Epoch: 783 [69.23%] G-Loss: 32.5873 D-Loss: 2.2660 Loss-g-fm: 7.4187 Loss-g-mel: 19.5872 Loss-g-dur: 1.4981 Loss-g-kl: 1.3723 lr: 0.0002 grad_norm_g: 720.4699 grad_norm_d: 48.1644
======> Epoch: 783
Train Epoch: 784 [65.38%] G-Loss: 30.6630 D-Loss: 2.3251 Loss-g-fm: 7.0402 Loss-g-mel: 18.2001 Loss-g-dur: 1.5434 Loss-g-kl: 1.4859 lr: 0.0002 grad_norm_g: 371.4766 grad_norm_d: 14.6854
======> Epoch: 784
Train Epoch: 785 [61.54%] G-Loss: 31.5584 D-Loss: 2.4037 Loss-g-fm: 7.2998 Loss-g-mel: 19.0034 Loss-g-dur: 1.5878 Loss-g-kl: 1.2909 lr: 0.0002 grad_norm_g: 491.8238 grad_norm_d: 47.1792
======> Epoch: 785
Train Epoch: 786 [57.69%] G-Loss: 30.7206 D-Loss: 2.3310 Loss-g-fm: 6.8616 Loss-g-mel: 18.5993 Loss-g-dur: 1.4235 Loss-g-kl: 1.2440 lr: 0.0002 grad_norm_g: 923.9187 grad_norm_d: 57.8278
======> Epoch: 786
Train Epoch: 787 [53.85%] G-Loss: 29.1037 D-Loss: 2.2929 Loss-g-fm: 6.4788 Loss-g-mel: 17.7651 Loss-g-dur: 1.4690 Loss-g-kl: 1.2079 lr: 0.0002 grad_norm_g: 874.6649 grad_norm_d: 83.4369
======> Epoch: 787
Train Epoch: 788 [50.00%] G-Loss: 30.5346 D-Loss: 2.2512 Loss-g-fm: 7.1737 Loss-g-mel: 17.9113 Loss-g-dur: 1.5305 Loss-g-kl: 1.5068 lr: 0.0002 grad_norm_g: 680.9872 grad_norm_d: 19.2180
======> Epoch: 788
Train Epoch: 789 [46.15%] G-Loss: 29.7927 D-Loss: 2.5261 Loss-g-fm: 6.3313 Loss-g-mel: 17.9460 Loss-g-dur: 1.5650 Loss-g-kl: 1.3690 lr: 0.0002 grad_norm_g: 889.0892 grad_norm_d: 61.6941
======> Epoch: 789
Train Epoch: 790 [42.31%] G-Loss: 31.5290 D-Loss: 2.5371 Loss-g-fm: 7.0070 Loss-g-mel: 19.1295 Loss-g-dur: 1.4549 Loss-g-kl: 1.3957 lr: 0.0002 grad_norm_g: 436.0806 grad_norm_d: 156.0433
======> Epoch: 790
Train Epoch: 791 [38.46%] G-Loss: 30.0994 D-Loss: 2.6600 Loss-g-fm: 6.2016 Loss-g-mel: 18.5773 Loss-g-dur: 1.5231 Loss-g-kl: 1.3901 lr: 0.0002 grad_norm_g: 445.8235 grad_norm_d: 120.7144
======> Epoch: 791
Train Epoch: 792 [34.62%] G-Loss: 28.3824 D-Loss: 2.5779 Loss-g-fm: 5.5597 Loss-g-mel: 17.4980 Loss-g-dur: 1.4616 Loss-g-kl: 1.2384 lr: 0.0002 grad_norm_g: 512.8649 grad_norm_d: 52.8969
======> Epoch: 792
Train Epoch: 793 [30.77%] G-Loss: 22.2442 D-Loss: 2.3055 Loss-g-fm: 6.6129 Loss-g-mel: 10.6992 Loss-g-dur: 0.9600 Loss-g-kl: 1.2168 lr: 0.0002 grad_norm_g: 1020.4949 grad_norm_d: 39.1221
======> Epoch: 793
Train Epoch: 794 [26.92%] G-Loss: 31.4066 D-Loss: 2.2929 Loss-g-fm: 7.3633 Loss-g-mel: 18.8442 Loss-g-dur: 1.5632 Loss-g-kl: 1.4936 lr: 0.0002 grad_norm_g: 597.5196 grad_norm_d: 62.0033
======> Epoch: 794
Train Epoch: 795 [23.08%] G-Loss: 30.3197 D-Loss: 2.3035 Loss-g-fm: 6.9870 Loss-g-mel: 17.7820 Loss-g-dur: 1.4826 Loss-g-kl: 1.5033 lr: 0.0002 grad_norm_g: 748.3251 grad_norm_d: 60.1939
======> Epoch: 795
Train Epoch: 796 [19.23%] G-Loss: 29.0472 D-Loss: 2.3952 Loss-g-fm: 6.2630 Loss-g-mel: 17.6412 Loss-g-dur: 1.4413 Loss-g-kl: 1.3727 lr: 0.0002 grad_norm_g: 493.3243 grad_norm_d: 23.9464
======> Epoch: 796
Train Epoch: 797 [15.38%] G-Loss: 32.6722 D-Loss: 2.2673 Loss-g-fm: 7.4183 Loss-g-mel: 19.6514 Loss-g-dur: 1.6201 Loss-g-kl: 1.4319 lr: 0.0002 grad_norm_g: 145.0462 grad_norm_d: 16.0858
======> Epoch: 797
Train Epoch: 798 [11.54%] G-Loss: 32.5861 D-Loss: 2.1830 Loss-g-fm: 7.7998 Loss-g-mel: 19.1708 Loss-g-dur: 1.4777 Loss-g-kl: 1.1936 lr: 0.0002 grad_norm_g: 950.4000 grad_norm_d: 73.8306
======> Epoch: 798
Train Epoch: 799 [7.69%] G-Loss: 30.4558 D-Loss: 2.2631 Loss-g-fm: 6.8023 Loss-g-mel: 18.5507 Loss-g-dur: 1.3788 Loss-g-kl: 1.2184 lr: 0.0002 grad_norm_g: 761.9187 grad_norm_d: 81.5243
======> Epoch: 799
Train Epoch: 800 [3.85%] G-Loss: 30.5266 D-Loss: 2.2567 Loss-g-fm: 7.3612 Loss-g-mel: 18.1138 Loss-g-dur: 1.4565 Loss-g-kl: 1.0492 lr: 0.0002 grad_norm_g: 844.3624 grad_norm_d: 82.8852
======> Epoch: 800
Train Epoch: 801 [0.00%] G-Loss: 32.4741 D-Loss: 2.7570 Loss-g-fm: 7.2173 Loss-g-mel: 19.7286 Loss-g-dur: 1.5096 Loss-g-kl: 1.2958 lr: 0.0002 grad_norm_g: 407.6652 grad_norm_d: 114.3495
Train Epoch: 801 [96.15%] G-Loss: 21.6291 D-Loss: 2.5531 Loss-g-fm: 6.1952 Loss-g-mel: 11.2705 Loss-g-dur: 0.8961 Loss-g-kl: 1.3700 lr: 0.0002 grad_norm_g: 430.4069 grad_norm_d: 56.0849
======> Epoch: 801
Train Epoch: 802 [92.31%] G-Loss: 32.3151 D-Loss: 2.2454 Loss-g-fm: 7.6613 Loss-g-mel: 19.2137 Loss-g-dur: 1.4450 Loss-g-kl: 1.4313 lr: 0.0002 grad_norm_g: 142.7505 grad_norm_d: 10.3325
======> Epoch: 802
Train Epoch: 803 [88.46%] G-Loss: 27.6176 D-Loss: 2.5827 Loss-g-fm: 5.5194 Loss-g-mel: 17.2568 Loss-g-dur: 1.4474 Loss-g-kl: 1.2864 lr: 0.0002 grad_norm_g: 79.5574 grad_norm_d: 9.4224
======> Epoch: 803
Train Epoch: 804 [84.62%] G-Loss: 32.2545 D-Loss: 2.2837 Loss-g-fm: 7.5572 Loss-g-mel: 19.2226 Loss-g-dur: 1.6386 Loss-g-kl: 1.4824 lr: 0.0002 grad_norm_g: 144.4584 grad_norm_d: 10.1994
======> Epoch: 804
Train Epoch: 805 [80.77%] G-Loss: 30.1107 D-Loss: 2.3618 Loss-g-fm: 6.6763 Loss-g-mel: 18.0264 Loss-g-dur: 1.6266 Loss-g-kl: 1.4147 lr: 0.0002 grad_norm_g: 162.5919 grad_norm_d: 11.8756
======> Epoch: 805
Train Epoch: 806 [76.92%] G-Loss: 31.2243 D-Loss: 2.3147 Loss-g-fm: 7.1438 Loss-g-mel: 18.9350 Loss-g-dur: 1.5004 Loss-g-kl: 1.4357 lr: 0.0002 grad_norm_g: 505.5857 grad_norm_d: 39.7203
======> Epoch: 806
Train Epoch: 807 [73.08%] G-Loss: 28.8774 D-Loss: 2.3957 Loss-g-fm: 6.3575 Loss-g-mel: 17.6417 Loss-g-dur: 1.3789 Loss-g-kl: 1.0737 lr: 0.0002 grad_norm_g: 230.2587 grad_norm_d: 17.9545
======> Epoch: 807
Train Epoch: 808 [69.23%] G-Loss: 30.8189 D-Loss: 2.3831 Loss-g-fm: 7.0845 Loss-g-mel: 18.8349 Loss-g-dur: 1.4433 Loss-g-kl: 1.2129 lr: 0.0002 grad_norm_g: 539.4128 grad_norm_d: 23.7843
======> Epoch: 808
Train Epoch: 809 [65.38%] G-Loss: 30.3728 D-Loss: 2.3835 Loss-g-fm: 7.0958 Loss-g-mel: 17.9089 Loss-g-dur: 1.4601 Loss-g-kl: 1.3972 lr: 0.0002 grad_norm_g: 841.1534 grad_norm_d: 61.8286
======> Epoch: 809
Train Epoch: 810 [61.54%] G-Loss: 33.2355 D-Loss: 2.1997 Loss-g-fm: 8.6388 Loss-g-mel: 19.2946 Loss-g-dur: 1.5107 Loss-g-kl: 1.3518 lr: 0.0002 grad_norm_g: 672.3408 grad_norm_d: 39.6921
======> Epoch: 810
Train Epoch: 811 [57.69%] G-Loss: 30.9296 D-Loss: 2.4028 Loss-g-fm: 6.8116 Loss-g-mel: 18.8794 Loss-g-dur: 1.6452 Loss-g-kl: 1.3775 lr: 0.0002 grad_norm_g: 430.6563 grad_norm_d: 48.1916
======> Epoch: 811
Train Epoch: 812 [53.85%] G-Loss: 31.7300 D-Loss: 2.2792 Loss-g-fm: 7.3589 Loss-g-mel: 18.7665 Loss-g-dur: 1.4564 Loss-g-kl: 1.3859 lr: 0.0002 grad_norm_g: 429.6980 grad_norm_d: 31.4271
======> Epoch: 812
Train Epoch: 813 [50.00%] G-Loss: 23.3236 D-Loss: 2.4210 Loss-g-fm: 6.5222 Loss-g-mel: 11.9598 Loss-g-dur: 0.9353 Loss-g-kl: 1.3402 lr: 0.0002 grad_norm_g: 515.5269 grad_norm_d: 30.6817
======> Epoch: 813
Train Epoch: 814 [46.15%] G-Loss: 29.9793 D-Loss: 2.3224 Loss-g-fm: 6.8059 Loss-g-mel: 18.0784 Loss-g-dur: 1.4468 Loss-g-kl: 1.2073 lr: 0.0002 grad_norm_g: 814.6588 grad_norm_d: 42.7604
======> Epoch: 814
Train Epoch: 815 [42.31%] G-Loss: 32.1438 D-Loss: 2.2551 Loss-g-fm: 7.3483 Loss-g-mel: 19.3225 Loss-g-dur: 1.4727 Loss-g-kl: 1.3721 lr: 0.0002 grad_norm_g: 791.7968 grad_norm_d: 79.6730
======> Epoch: 815
Train Epoch: 816 [38.46%] G-Loss: 31.5530 D-Loss: 2.3164 Loss-g-fm: 7.1684 Loss-g-mel: 18.9133 Loss-g-dur: 1.4707 Loss-g-kl: 1.4117 lr: 0.0002 grad_norm_g: 594.4990 grad_norm_d: 69.4289
======> Epoch: 816
Train Epoch: 817 [34.62%] G-Loss: 30.1185 D-Loss: 2.3690 Loss-g-fm: 6.6083 Loss-g-mel: 18.3282 Loss-g-dur: 1.4571 Loss-g-kl: 1.2901 lr: 0.0002 grad_norm_g: 821.5245 grad_norm_d: 67.5517
======> Epoch: 817
Train Epoch: 818 [30.77%] G-Loss: 31.7957 D-Loss: 2.3306 Loss-g-fm: 7.9026 Loss-g-mel: 18.3981 Loss-g-dur: 1.5934 Loss-g-kl: 1.4372 lr: 0.0002 grad_norm_g: 851.6664 grad_norm_d: 43.3358
======> Epoch: 818
Train Epoch: 819 [26.92%] G-Loss: 30.3277 D-Loss: 2.3871 Loss-g-fm: 7.2210 Loss-g-mel: 18.1963 Loss-g-dur: 1.4813 Loss-g-kl: 1.3057 lr: 0.0002 grad_norm_g: 391.5124 grad_norm_d: 23.7712
======> Epoch: 819
Train Epoch: 820 [23.08%] G-Loss: 30.7931 D-Loss: 2.2955 Loss-g-fm: 7.3926 Loss-g-mel: 18.0053 Loss-g-dur: 1.4973 Loss-g-kl: 1.3089 lr: 0.0002 grad_norm_g: 854.5774 grad_norm_d: 73.9644
======> Epoch: 820
Train Epoch: 821 [19.23%] G-Loss: 29.0416 D-Loss: 2.8683 Loss-g-fm: 5.6196 Loss-g-mel: 17.9433 Loss-g-dur: 1.3885 Loss-g-kl: 1.4785 lr: 0.0002 grad_norm_g: 59.0814 grad_norm_d: 92.3796
======> Epoch: 821
Train Epoch: 822 [15.38%] G-Loss: 29.9908 D-Loss: 2.4158 Loss-g-fm: 6.7196 Loss-g-mel: 18.2189 Loss-g-dur: 1.3757 Loss-g-kl: 1.1417 lr: 0.0002 grad_norm_g: 380.5121 grad_norm_d: 65.3503
======> Epoch: 822
Train Epoch: 823 [11.54%] G-Loss: 30.4203 D-Loss: 2.2612 Loss-g-fm: 6.9234 Loss-g-mel: 18.2823 Loss-g-dur: 1.4366 Loss-g-kl: 1.2078 lr: 0.0002 grad_norm_g: 206.7866 grad_norm_d: 34.2281
======> Epoch: 823
Train Epoch: 824 [7.69%] G-Loss: 22.6248 D-Loss: 2.3527 Loss-g-fm: 6.6773 Loss-g-mel: 11.5094 Loss-g-dur: 0.9678 Loss-g-kl: 1.3322 lr: 0.0002 grad_norm_g: 655.9454 grad_norm_d: 34.0260
======> Epoch: 824
Train Epoch: 825 [3.85%] G-Loss: 30.4833 D-Loss: 2.3091 Loss-g-fm: 7.1518 Loss-g-mel: 18.3861 Loss-g-dur: 1.4307 Loss-g-kl: 1.1323 lr: 0.0002 grad_norm_g: 541.5614 grad_norm_d: 30.9225
======> Epoch: 825
Train Epoch: 826 [0.00%] G-Loss: 31.8235 D-Loss: 2.3739 Loss-g-fm: 7.2408 Loss-g-mel: 19.1189 Loss-g-dur: 1.6683 Loss-g-kl: 1.4094 lr: 0.0002 grad_norm_g: 99.5493 grad_norm_d: 26.3822
Train Epoch: 826 [96.15%] G-Loss: 28.7737 D-Loss: 2.4493 Loss-g-fm: 6.1868 Loss-g-mel: 17.6331 Loss-g-dur: 1.4338 Loss-g-kl: 1.2622 lr: 0.0002 grad_norm_g: 204.4266 grad_norm_d: 52.6022
======> Epoch: 826
Train Epoch: 827 [92.31%] G-Loss: 29.0622 D-Loss: 2.2924 Loss-g-fm: 6.4179 Loss-g-mel: 17.9544 Loss-g-dur: 1.4216 Loss-g-kl: 0.8416 lr: 0.0002 grad_norm_g: 594.9212 grad_norm_d: 17.3047
======> Epoch: 827
Train Epoch: 828 [88.46%] G-Loss: 31.2836 D-Loss: 2.4572 Loss-g-fm: 7.2615 Loss-g-mel: 18.8130 Loss-g-dur: 1.4212 Loss-g-kl: 1.2376 lr: 0.0002 grad_norm_g: 666.5070 grad_norm_d: 17.6418
======> Epoch: 828
Train Epoch: 829 [84.62%] G-Loss: 29.8519 D-Loss: 2.3628 Loss-g-fm: 6.7202 Loss-g-mel: 18.0434 Loss-g-dur: 1.4278 Loss-g-kl: 1.1456 lr: 0.0002 grad_norm_g: 399.1684 grad_norm_d: 62.9185
======> Epoch: 829
Train Epoch: 830 [80.77%] G-Loss: 29.4673 D-Loss: 2.4406 Loss-g-fm: 6.5846 Loss-g-mel: 17.7688 Loss-g-dur: 1.5012 Loss-g-kl: 1.1975 lr: 0.0002 grad_norm_g: 292.1354 grad_norm_d: 21.3036
======> Epoch: 830
Train Epoch: 831 [76.92%] G-Loss: 29.9667 D-Loss: 2.2870 Loss-g-fm: 6.6812 Loss-g-mel: 18.3911 Loss-g-dur: 1.4538 Loss-g-kl: 1.0916 lr: 0.0002 grad_norm_g: 103.2161 grad_norm_d: 22.4058
======> Epoch: 831
Train Epoch: 832 [73.08%] G-Loss: 24.0912 D-Loss: 2.3332 Loss-g-fm: 7.4938 Loss-g-mel: 11.6716 Loss-g-dur: 1.0230 Loss-g-kl: 1.4281 lr: 0.0002 grad_norm_g: 728.4760 grad_norm_d: 62.7411
======> Epoch: 832
Train Epoch: 833 [69.23%] G-Loss: 30.1355 D-Loss: 2.2342 Loss-g-fm: 7.1956 Loss-g-mel: 17.6822 Loss-g-dur: 1.4180 Loss-g-kl: 1.1830 lr: 0.0002 grad_norm_g: 821.7290 grad_norm_d: 56.0230
======> Epoch: 833
Train Epoch: 834 [65.38%] G-Loss: 32.7376 D-Loss: 2.1990 Loss-g-fm: 8.4263 Loss-g-mel: 18.6189 Loss-g-dur: 1.6465 Loss-g-kl: 1.4660 lr: 0.0002 grad_norm_g: 990.1034 grad_norm_d: 72.5046
======> Epoch: 834
Train Epoch: 835 [61.54%] G-Loss: 24.6856 D-Loss: 2.2184 Loss-g-fm: 7.9922 Loss-g-mel: 11.7000 Loss-g-dur: 0.9274 Loss-g-kl: 1.3397 lr: 0.0002 grad_norm_g: 979.7972 grad_norm_d: 30.9033
======> Epoch: 835
Train Epoch: 836 [57.69%] G-Loss: 34.2622 D-Loss: 2.1823 Loss-g-fm: 8.8045 Loss-g-mel: 19.9340 Loss-g-dur: 1.5344 Loss-g-kl: 1.4804 lr: 0.0002 grad_norm_g: 991.1719 grad_norm_d: 50.1254
======> Epoch: 836
Train Epoch: 837 [53.85%] G-Loss: 24.1827 D-Loss: 2.0997 Loss-g-fm: 7.7087 Loss-g-mel: 11.3194 Loss-g-dur: 0.9704 Loss-g-kl: 1.3479 lr: 0.0002 grad_norm_g: 1028.8338 grad_norm_d: 42.8168
======> Epoch: 837
Train Epoch: 838 [50.00%] G-Loss: 29.4899 D-Loss: 2.4969 Loss-g-fm: 6.3213 Loss-g-mel: 17.9477 Loss-g-dur: 1.4695 Loss-g-kl: 1.4196 lr: 0.0002 grad_norm_g: 593.5480 grad_norm_d: 85.6080
======> Epoch: 838
Train Epoch: 839 [46.15%] G-Loss: 33.6304 D-Loss: 2.2386 Loss-g-fm: 7.8256 Loss-g-mel: 19.9538 Loss-g-dur: 1.6576 Loss-g-kl: 1.7386 lr: 0.0002 grad_norm_g: 634.9745 grad_norm_d: 27.9377
======> Epoch: 839
Train Epoch: 840 [42.31%] G-Loss: 31.9007 D-Loss: 2.3219 Loss-g-fm: 7.2959 Loss-g-mel: 18.9208 Loss-g-dur: 1.6271 Loss-g-kl: 1.5524 lr: 0.0002 grad_norm_g: 797.5036 grad_norm_d: 78.5706
======> Epoch: 840
Train Epoch: 841 [38.46%] G-Loss: 31.9925 D-Loss: 2.5002 Loss-g-fm: 7.4987 Loss-g-mel: 19.1158 Loss-g-dur: 1.5152 Loss-g-kl: 1.4257 lr: 0.0002 grad_norm_g: 259.4705 grad_norm_d: 8.7729
======> Epoch: 841
Train Epoch: 842 [34.62%] G-Loss: 30.4106 D-Loss: 2.2772 Loss-g-fm: 7.2180 Loss-g-mel: 18.3500 Loss-g-dur: 1.4394 Loss-g-kl: 1.2044 lr: 0.0002 grad_norm_g: 174.9479 grad_norm_d: 22.7822
======> Epoch: 842
Train Epoch: 843 [30.77%] G-Loss: 32.2261 D-Loss: 2.2874 Loss-g-fm: 7.6743 Loss-g-mel: 19.1297 Loss-g-dur: 1.4762 Loss-g-kl: 1.4366 lr: 0.0002 grad_norm_g: 873.0024 grad_norm_d: 61.5050
======> Epoch: 843
Train Epoch: 844 [26.92%] G-Loss: 32.7898 D-Loss: 2.4604 Loss-g-fm: 7.9270 Loss-g-mel: 19.3532 Loss-g-dur: 1.4972 Loss-g-kl: 1.5221 lr: 0.0002 grad_norm_g: 487.3771 grad_norm_d: 33.5578
======> Epoch: 844
Train Epoch: 845 [23.08%] G-Loss: 29.0516 D-Loss: 2.4169 Loss-g-fm: 6.0285 Loss-g-mel: 18.0622 Loss-g-dur: 1.3869 Loss-g-kl: 1.1129 lr: 0.0002 grad_norm_g: 530.8273 grad_norm_d: 52.7935
======> Epoch: 845
Train Epoch: 846 [19.23%] G-Loss: 30.4596 D-Loss: 2.2963 Loss-g-fm: 7.1361 Loss-g-mel: 18.2179 Loss-g-dur: 1.4206 Loss-g-kl: 1.1969 lr: 0.0002 grad_norm_g: 934.5313 grad_norm_d: 76.4642
======> Epoch: 846
Train Epoch: 847 [15.38%] G-Loss: 32.2300 D-Loss: 2.3326 Loss-g-fm: 7.8260 Loss-g-mel: 19.0533 Loss-g-dur: 1.4972 Loss-g-kl: 1.4095 lr: 0.0002 grad_norm_g: 760.9135 grad_norm_d: 48.3957
terminate called without an active exception
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f44c239b820>
Traceback (most recent call last):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 831536) is killed by signal: Aborted. 
Saving model and optimizer state at iteration 847 to /ZFS4T/tts/data/VITS/model_saved/G_22000.pth
Saving model and optimizer state at iteration 847 to /ZFS4T/tts/data/VITS/model_saved/D_22000.pth
======> Epoch: 847
Train Epoch: 848 [11.54%] G-Loss: 31.1235 D-Loss: 2.2953 Loss-g-fm: 7.4557 Loss-g-mel: 18.4112 Loss-g-dur: 1.5104 Loss-g-kl: 1.2279 lr: 0.0002 grad_norm_g: 922.9855 grad_norm_d: 66.3074
======> Epoch: 848
Train Epoch: 849 [7.69%] G-Loss: 31.3894 D-Loss: 2.2904 Loss-g-fm: 7.5248 Loss-g-mel: 18.8314 Loss-g-dur: 1.4237 Loss-g-kl: 1.1062 lr: 0.0002 grad_norm_g: 338.2879 grad_norm_d: 45.1503
======> Epoch: 849
Train Epoch: 850 [3.85%] G-Loss: 29.6128 D-Loss: 2.3649 Loss-g-fm: 6.5797 Loss-g-mel: 18.3254 Loss-g-dur: 1.4019 Loss-g-kl: 1.1078 lr: 0.0002 grad_norm_g: 802.5174 grad_norm_d: 93.7510
======> Epoch: 850
Train Epoch: 851 [0.00%] G-Loss: 31.6002 D-Loss: 2.4097 Loss-g-fm: 7.1713 Loss-g-mel: 18.8109 Loss-g-dur: 1.6515 Loss-g-kl: 1.4757 lr: 0.0002 grad_norm_g: 554.2222 grad_norm_d: 18.5649
Train Epoch: 851 [96.15%] G-Loss: 30.9621 D-Loss: 2.2331 Loss-g-fm: 7.5649 Loss-g-mel: 18.2469 Loss-g-dur: 1.3956 Loss-g-kl: 1.2383 lr: 0.0002 grad_norm_g: 923.0517 grad_norm_d: 50.0152
======> Epoch: 851
Train Epoch: 852 [92.31%] G-Loss: 31.6928 D-Loss: 2.3814 Loss-g-fm: 7.5933 Loss-g-mel: 19.0559 Loss-g-dur: 1.4049 Loss-g-kl: 1.4315 lr: 0.0002 grad_norm_g: 857.5961 grad_norm_d: 56.8833
======> Epoch: 852
Train Epoch: 853 [88.46%] G-Loss: 32.3864 D-Loss: 2.3609 Loss-g-fm: 7.6609 Loss-g-mel: 19.3347 Loss-g-dur: 1.5372 Loss-g-kl: 1.3642 lr: 0.0002 grad_norm_g: 583.5437 grad_norm_d: 32.0784
======> Epoch: 853
Train Epoch: 854 [84.62%] G-Loss: 30.4573 D-Loss: 2.3952 Loss-g-fm: 6.8316 Loss-g-mel: 18.5061 Loss-g-dur: 1.4422 Loss-g-kl: 1.2336 lr: 0.0002 grad_norm_g: 528.7718 grad_norm_d: 53.5736
======> Epoch: 854
Train Epoch: 855 [80.77%] G-Loss: 30.3790 D-Loss: 2.3172 Loss-g-fm: 7.1492 Loss-g-mel: 17.9992 Loss-g-dur: 1.3915 Loss-g-kl: 1.2685 lr: 0.0002 grad_norm_g: 758.2058 grad_norm_d: 61.1978
======> Epoch: 855
Train Epoch: 856 [76.92%] G-Loss: 30.5290 D-Loss: 2.2957 Loss-g-fm: 8.3085 Loss-g-mel: 17.0105 Loss-g-dur: 1.2293 Loss-g-kl: 1.3424 lr: 0.0002 grad_norm_g: 548.6340 grad_norm_d: 112.9685
======> Epoch: 856
Train Epoch: 857 [73.08%] G-Loss: 30.7816 D-Loss: 2.3403 Loss-g-fm: 7.3880 Loss-g-mel: 18.0048 Loss-g-dur: 1.5183 Loss-g-kl: 1.4730 lr: 0.0002 grad_norm_g: 599.7814 grad_norm_d: 53.5261
======> Epoch: 857
Train Epoch: 858 [69.23%] G-Loss: 31.8571 D-Loss: 2.2084 Loss-g-fm: 9.5234 Loss-g-mel: 17.3421 Loss-g-dur: 1.2201 Loss-g-kl: 1.3711 lr: 0.0002 grad_norm_g: 180.3033 grad_norm_d: 13.9865
======> Epoch: 858
Train Epoch: 859 [65.38%] G-Loss: 29.9827 D-Loss: 2.3862 Loss-g-fm: 6.8260 Loss-g-mel: 18.5778 Loss-g-dur: 1.3793 Loss-g-kl: 1.2048 lr: 0.0002 grad_norm_g: 380.2829 grad_norm_d: 36.0077
======> Epoch: 859
Train Epoch: 860 [61.54%] G-Loss: 30.4919 D-Loss: 2.3720 Loss-g-fm: 7.0481 Loss-g-mel: 18.2433 Loss-g-dur: 1.3679 Loss-g-kl: 1.3322 lr: 0.0002 grad_norm_g: 643.7871 grad_norm_d: 61.9195
======> Epoch: 860
Train Epoch: 861 [57.69%] G-Loss: 32.8791 D-Loss: 2.3891 Loss-g-fm: 8.2050 Loss-g-mel: 19.0343 Loss-g-dur: 1.6370 Loss-g-kl: 1.3994 lr: 0.0002 grad_norm_g: 926.1857 grad_norm_d: 53.0668
======> Epoch: 861
Train Epoch: 862 [53.85%] G-Loss: 29.0536 D-Loss: 2.4876 Loss-g-fm: 6.1919 Loss-g-mel: 17.7459 Loss-g-dur: 1.4104 Loss-g-kl: 1.2125 lr: 0.0002 grad_norm_g: 726.6651 grad_norm_d: 49.6720
======> Epoch: 862
Train Epoch: 863 [50.00%] G-Loss: 31.0363 D-Loss: 2.3143 Loss-g-fm: 7.4314 Loss-g-mel: 18.4625 Loss-g-dur: 1.4203 Loss-g-kl: 1.2631 lr: 0.0002 grad_norm_g: 610.0102 grad_norm_d: 64.2807
======> Epoch: 863
Train Epoch: 864 [46.15%] G-Loss: 30.4057 D-Loss: 2.2399 Loss-g-fm: 7.0387 Loss-g-mel: 18.1476 Loss-g-dur: 1.4105 Loss-g-kl: 1.3285 lr: 0.0002 grad_norm_g: 189.0459 grad_norm_d: 28.6321
======> Epoch: 864
Train Epoch: 865 [42.31%] G-Loss: 31.9822 D-Loss: 2.5245 Loss-g-fm: 7.2423 Loss-g-mel: 19.2142 Loss-g-dur: 1.5629 Loss-g-kl: 1.3250 lr: 0.0002 grad_norm_g: 226.6555 grad_norm_d: 22.7116
======> Epoch: 865
Train Epoch: 866 [38.46%] G-Loss: 30.0125 D-Loss: 2.2681 Loss-g-fm: 6.7908 Loss-g-mel: 17.9719 Loss-g-dur: 1.4398 Loss-g-kl: 1.2154 lr: 0.0002 grad_norm_g: 749.6510 grad_norm_d: 74.6388
======> Epoch: 866
Train Epoch: 867 [34.62%] G-Loss: 30.3986 D-Loss: 2.3494 Loss-g-fm: 7.4932 Loss-g-mel: 18.0866 Loss-g-dur: 1.3661 Loss-g-kl: 1.2261 lr: 0.0002 grad_norm_g: 834.6187 grad_norm_d: 79.3442
======> Epoch: 867
Train Epoch: 868 [30.77%] G-Loss: 30.3312 D-Loss: 2.2344 Loss-g-fm: 7.1672 Loss-g-mel: 17.7137 Loss-g-dur: 1.3938 Loss-g-kl: 1.4987 lr: 0.0002 grad_norm_g: 885.2542 grad_norm_d: 67.4154
======> Epoch: 868
Train Epoch: 869 [26.92%] G-Loss: 28.0324 D-Loss: 2.6632 Loss-g-fm: 6.2311 Loss-g-mel: 16.9064 Loss-g-dur: 1.3545 Loss-g-kl: 1.2830 lr: 0.0002 grad_norm_g: 200.4191 grad_norm_d: 95.4023
======> Epoch: 869
Train Epoch: 870 [23.08%] G-Loss: 29.9780 D-Loss: 2.4239 Loss-g-fm: 6.8754 Loss-g-mel: 18.0329 Loss-g-dur: 1.4238 Loss-g-kl: 1.2790 lr: 0.0002 grad_norm_g: 358.3182 grad_norm_d: 36.6452
======> Epoch: 870
Train Epoch: 871 [19.23%] G-Loss: 29.7270 D-Loss: 2.3692 Loss-g-fm: 6.7014 Loss-g-mel: 18.0673 Loss-g-dur: 1.4532 Loss-g-kl: 1.1767 lr: 0.0002 grad_norm_g: 160.9642 grad_norm_d: 16.6604
======> Epoch: 871
Train Epoch: 872 [15.38%] G-Loss: 29.9619 D-Loss: 2.4089 Loss-g-fm: 6.6426 Loss-g-mel: 18.1724 Loss-g-dur: 1.4294 Loss-g-kl: 1.2548 lr: 0.0002 grad_norm_g: 187.9550 grad_norm_d: 10.4975
======> Epoch: 872
Train Epoch: 873 [11.54%] G-Loss: 32.3849 D-Loss: 2.2285 Loss-g-fm: 7.9725 Loss-g-mel: 19.0401 Loss-g-dur: 1.5212 Loss-g-kl: 1.4191 lr: 0.0002 grad_norm_g: 468.4227 grad_norm_d: 50.4854
======> Epoch: 873
Train Epoch: 874 [7.69%] G-Loss: 28.9152 D-Loss: 2.3612 Loss-g-fm: 6.6960 Loss-g-mel: 17.5420 Loss-g-dur: 1.3839 Loss-g-kl: 0.9706 lr: 0.0002 grad_norm_g: 618.8349 grad_norm_d: 57.5589
======> Epoch: 874
Train Epoch: 875 [3.85%] G-Loss: 31.0062 D-Loss: 2.3015 Loss-g-fm: 7.3071 Loss-g-mel: 18.5884 Loss-g-dur: 1.5361 Loss-g-kl: 1.1148 lr: 0.0002 grad_norm_g: 412.1743 grad_norm_d: 40.5320
======> Epoch: 875
Train Epoch: 876 [0.00%] G-Loss: 31.7770 D-Loss: 2.2638 Loss-g-fm: 7.9470 Loss-g-mel: 18.5953 Loss-g-dur: 1.4606 Loss-g-kl: 1.3685 lr: 0.0002 grad_norm_g: 707.8475 grad_norm_d: 34.0664
Train Epoch: 876 [96.15%] G-Loss: 30.0080 D-Loss: 2.5090 Loss-g-fm: 6.9031 Loss-g-mel: 17.9554 Loss-g-dur: 1.3823 Loss-g-kl: 0.9668 lr: 0.0002 grad_norm_g: 717.5549 grad_norm_d: 106.2706
======> Epoch: 876
Train Epoch: 877 [92.31%] G-Loss: 29.6850 D-Loss: 2.4263 Loss-g-fm: 6.5548 Loss-g-mel: 18.1527 Loss-g-dur: 1.3571 Loss-g-kl: 1.3311 lr: 0.0002 grad_norm_g: 115.5525 grad_norm_d: 21.0425
======> Epoch: 877
Train Epoch: 878 [88.46%] G-Loss: 34.2185 D-Loss: 2.2487 Loss-g-fm: 8.9243 Loss-g-mel: 19.7913 Loss-g-dur: 1.5288 Loss-g-kl: 1.4858 lr: 0.0002 grad_norm_g: 521.3832 grad_norm_d: 32.2119
======> Epoch: 878
Train Epoch: 879 [84.62%] G-Loss: 29.0822 D-Loss: 2.2530 Loss-g-fm: 6.6669 Loss-g-mel: 17.3057 Loss-g-dur: 1.3976 Loss-g-kl: 1.1463 lr: 0.0002 grad_norm_g: 843.1177 grad_norm_d: 52.3193
======> Epoch: 879
Train Epoch: 880 [80.77%] G-Loss: 30.6983 D-Loss: 2.4547 Loss-g-fm: 6.9547 Loss-g-mel: 18.3993 Loss-g-dur: 1.3788 Loss-g-kl: 1.2753 lr: 0.0002 grad_norm_g: 590.4012 grad_norm_d: 43.6040
======> Epoch: 880
Train Epoch: 881 [76.92%] G-Loss: 28.6154 D-Loss: 2.5551 Loss-g-fm: 6.0639 Loss-g-mel: 17.5055 Loss-g-dur: 1.5355 Loss-g-kl: 1.2452 lr: 0.0002 grad_norm_g: 656.8763 grad_norm_d: 47.6021
======> Epoch: 881
Train Epoch: 882 [73.08%] G-Loss: 31.8321 D-Loss: 2.3162 Loss-g-fm: 7.6128 Loss-g-mel: 18.9993 Loss-g-dur: 1.5230 Loss-g-kl: 1.2624 lr: 0.0002 grad_norm_g: 443.2083 grad_norm_d: 52.9563
======> Epoch: 882
Train Epoch: 883 [69.23%] G-Loss: 30.4794 D-Loss: 2.3621 Loss-g-fm: 6.9209 Loss-g-mel: 18.2285 Loss-g-dur: 1.3917 Loss-g-kl: 1.1556 lr: 0.0002 grad_norm_g: 823.1688 grad_norm_d: 84.9361
======> Epoch: 883
Train Epoch: 884 [65.38%] G-Loss: 32.8030 D-Loss: 2.2808 Loss-g-fm: 8.1683 Loss-g-mel: 19.1992 Loss-g-dur: 1.5549 Loss-g-kl: 1.5189 lr: 0.0002 grad_norm_g: 606.3098 grad_norm_d: 46.1014
======> Epoch: 884
Train Epoch: 885 [61.54%] G-Loss: 33.0259 D-Loss: 2.2271 Loss-g-fm: 8.3731 Loss-g-mel: 19.1177 Loss-g-dur: 1.4361 Loss-g-kl: 1.0096 lr: 0.0002 grad_norm_g: 937.1101 grad_norm_d: 44.9478
======> Epoch: 885
Train Epoch: 886 [57.69%] G-Loss: 29.3898 D-Loss: 2.4230 Loss-g-fm: 6.8885 Loss-g-mel: 17.3646 Loss-g-dur: 1.3811 Loss-g-kl: 1.2436 lr: 0.0002 grad_norm_g: 763.7876 grad_norm_d: 61.5852
======> Epoch: 886
Train Epoch: 887 [53.85%] G-Loss: 30.5500 D-Loss: 2.2972 Loss-g-fm: 6.7913 Loss-g-mel: 18.5830 Loss-g-dur: 1.4067 Loss-g-kl: 1.2255 lr: 0.0002 grad_norm_g: 463.1785 grad_norm_d: 45.2033
======> Epoch: 887
Train Epoch: 888 [50.00%] G-Loss: 30.8335 D-Loss: 2.1969 Loss-g-fm: 7.1491 Loss-g-mel: 18.0820 Loss-g-dur: 1.5237 Loss-g-kl: 1.4498 lr: 0.0002 grad_norm_g: 952.5501 grad_norm_d: 54.2416
======> Epoch: 888
Train Epoch: 889 [46.15%] G-Loss: 31.1843 D-Loss: 2.3295 Loss-g-fm: 7.5089 Loss-g-mel: 18.2226 Loss-g-dur: 1.4013 Loss-g-kl: 1.3165 lr: 0.0002 grad_norm_g: 712.7120 grad_norm_d: 37.4645
======> Epoch: 889
Train Epoch: 890 [42.31%] G-Loss: 31.1226 D-Loss: 2.3320 Loss-g-fm: 7.3070 Loss-g-mel: 18.8662 Loss-g-dur: 1.3954 Loss-g-kl: 1.0962 lr: 0.0002 grad_norm_g: 334.1311 grad_norm_d: 13.2206
======> Epoch: 890
Train Epoch: 891 [38.46%] G-Loss: 31.0517 D-Loss: 2.3184 Loss-g-fm: 7.7595 Loss-g-mel: 18.2255 Loss-g-dur: 1.3431 Loss-g-kl: 1.2390 lr: 0.0002 grad_norm_g: 864.0127 grad_norm_d: 66.4346
======> Epoch: 891
Train Epoch: 892 [34.62%] G-Loss: 24.2319 D-Loss: 2.2059 Loss-g-fm: 7.6790 Loss-g-mel: 11.7068 Loss-g-dur: 0.9648 Loss-g-kl: 1.3436 lr: 0.0002 grad_norm_g: 775.0110 grad_norm_d: 65.9883
======> Epoch: 892
Train Epoch: 893 [30.77%] G-Loss: 28.3625 D-Loss: 2.2589 Loss-g-fm: 6.3842 Loss-g-mel: 17.1029 Loss-g-dur: 1.3703 Loss-g-kl: 1.0025 lr: 0.0002 grad_norm_g: 753.4812 grad_norm_d: 68.3587
======> Epoch: 893
Train Epoch: 894 [26.92%] G-Loss: 31.3941 D-Loss: 2.6023 Loss-g-fm: 6.9362 Loss-g-mel: 18.7901 Loss-g-dur: 1.5473 Loss-g-kl: 1.1827 lr: 0.0002 grad_norm_g: 197.1984 grad_norm_d: 28.2374
======> Epoch: 894
Train Epoch: 895 [23.08%] G-Loss: 33.8578 D-Loss: 2.1840 Loss-g-fm: 8.3749 Loss-g-mel: 19.4598 Loss-g-dur: 1.7664 Loss-g-kl: 1.5380 lr: 0.0002 grad_norm_g: 967.6998 grad_norm_d: 59.2655
======> Epoch: 895
Train Epoch: 896 [19.23%] G-Loss: 28.5465 D-Loss: 2.2395 Loss-g-fm: 7.8435 Loss-g-mel: 15.7503 Loss-g-dur: 1.2091 Loss-g-kl: 1.3129 lr: 0.0002 grad_norm_g: 776.0109 grad_norm_d: 110.0268
======> Epoch: 896
Train Epoch: 897 [15.38%] G-Loss: 30.5125 D-Loss: 2.4366 Loss-g-fm: 6.8690 Loss-g-mel: 18.7355 Loss-g-dur: 1.3805 Loss-g-kl: 1.3528 lr: 0.0002 grad_norm_g: 541.4248 grad_norm_d: 46.2359
======> Epoch: 897
Train Epoch: 898 [11.54%] G-Loss: 29.9761 D-Loss: 2.3764 Loss-g-fm: 7.3812 Loss-g-mel: 17.5640 Loss-g-dur: 1.3796 Loss-g-kl: 1.1643 lr: 0.0002 grad_norm_g: 1019.4148 grad_norm_d: 76.0045
======> Epoch: 898
Train Epoch: 899 [7.69%] G-Loss: 30.2193 D-Loss: 2.3394 Loss-g-fm: 7.1475 Loss-g-mel: 18.0618 Loss-g-dur: 1.3764 Loss-g-kl: 1.1695 lr: 0.0002 grad_norm_g: 703.8688 grad_norm_d: 41.3478
======> Epoch: 899
Train Epoch: 900 [3.85%] G-Loss: 31.0139 D-Loss: 2.3764 Loss-g-fm: 7.6504 Loss-g-mel: 18.2343 Loss-g-dur: 1.3855 Loss-g-kl: 1.3092 lr: 0.0002 grad_norm_g: 118.7549 grad_norm_d: 18.6987
======> Epoch: 900
Train Epoch: 901 [0.00%] G-Loss: 30.6967 D-Loss: 2.2578 Loss-g-fm: 7.2872 Loss-g-mel: 18.0949 Loss-g-dur: 1.4261 Loss-g-kl: 1.3575 lr: 0.0002 grad_norm_g: 105.3365 grad_norm_d: 26.7234
Train Epoch: 901 [96.15%] G-Loss: 30.1823 D-Loss: 2.3500 Loss-g-fm: 7.3766 Loss-g-mel: 17.4303 Loss-g-dur: 1.3626 Loss-g-kl: 1.3758 lr: 0.0002 grad_norm_g: 584.4814 grad_norm_d: 23.0632
======> Epoch: 901
Train Epoch: 902 [92.31%] G-Loss: 34.2588 D-Loss: 2.5898 Loss-g-fm: 8.7752 Loss-g-mel: 19.4849 Loss-g-dur: 1.5073 Loss-g-kl: 1.5576 lr: 0.0002 grad_norm_g: 935.2959 grad_norm_d: 109.4181
======> Epoch: 902
Train Epoch: 903 [88.46%] G-Loss: 30.5898 D-Loss: 2.4055 Loss-g-fm: 7.1570 Loss-g-mel: 18.3545 Loss-g-dur: 1.4173 Loss-g-kl: 1.3027 lr: 0.0002 grad_norm_g: 663.3116 grad_norm_d: 58.0673
======> Epoch: 903
Train Epoch: 904 [84.62%] G-Loss: 32.7858 D-Loss: 2.2954 Loss-g-fm: 7.7278 Loss-g-mel: 19.3909 Loss-g-dur: 1.4695 Loss-g-kl: 1.3139 lr: 0.0002 grad_norm_g: 375.5632 grad_norm_d: 67.8180
======> Epoch: 904
Train Epoch: 905 [80.77%] G-Loss: 29.8573 D-Loss: 2.6288 Loss-g-fm: 6.3589 Loss-g-mel: 17.7432 Loss-g-dur: 1.4829 Loss-g-kl: 1.4119 lr: 0.0002 grad_norm_g: 723.7242 grad_norm_d: 38.1548
======> Epoch: 905
Train Epoch: 906 [76.92%] G-Loss: 29.9908 D-Loss: 2.3831 Loss-g-fm: 6.7370 Loss-g-mel: 18.3533 Loss-g-dur: 1.4190 Loss-g-kl: 1.3527 lr: 0.0002 grad_norm_g: 156.2462 grad_norm_d: 15.1305
======> Epoch: 906
Train Epoch: 907 [73.08%] G-Loss: 29.7312 D-Loss: 2.4908 Loss-g-fm: 6.4391 Loss-g-mel: 18.1249 Loss-g-dur: 1.4866 Loss-g-kl: 1.3796 lr: 0.0002 grad_norm_g: 133.7383 grad_norm_d: 27.6840
======> Epoch: 907
Train Epoch: 908 [69.23%] G-Loss: 29.5455 D-Loss: 2.3433 Loss-g-fm: 6.9340 Loss-g-mel: 18.0113 Loss-g-dur: 1.4364 Loss-g-kl: 1.0366 lr: 0.0002 grad_norm_g: 709.9050 grad_norm_d: 56.8773
======> Epoch: 908
Train Epoch: 909 [65.38%] G-Loss: 32.0698 D-Loss: 2.3443 Loss-g-fm: 7.6544 Loss-g-mel: 19.2424 Loss-g-dur: 1.4629 Loss-g-kl: 1.3836 lr: 0.0002 grad_norm_g: 960.3577 grad_norm_d: 57.0988
======> Epoch: 909
Train Epoch: 910 [61.54%] G-Loss: 30.3409 D-Loss: 2.3373 Loss-g-fm: 7.2551 Loss-g-mel: 18.0161 Loss-g-dur: 1.4422 Loss-g-kl: 1.3835 lr: 0.0002 grad_norm_g: 834.3428 grad_norm_d: 66.0469
======> Epoch: 910
Train Epoch: 911 [57.69%] G-Loss: 32.2521 D-Loss: 2.4638 Loss-g-fm: 7.5166 Loss-g-mel: 19.2428 Loss-g-dur: 1.5401 Loss-g-kl: 1.4608 lr: 0.0002 grad_norm_g: 631.1168 grad_norm_d: 69.3200
======> Epoch: 911
Train Epoch: 912 [53.85%] G-Loss: 30.2995 D-Loss: 2.2698 Loss-g-fm: 7.3826 Loss-g-mel: 17.7619 Loss-g-dur: 1.3389 Loss-g-kl: 1.3129 lr: 0.0002 grad_norm_g: 858.5898 grad_norm_d: 69.6905
======> Epoch: 912
Train Epoch: 913 [50.00%] G-Loss: 29.5004 D-Loss: 2.4172 Loss-g-fm: 6.8812 Loss-g-mel: 17.7101 Loss-g-dur: 1.3812 Loss-g-kl: 1.3067 lr: 0.0002 grad_norm_g: 735.7202 grad_norm_d: 61.9810
======> Epoch: 913
Train Epoch: 914 [46.15%] G-Loss: 31.7737 D-Loss: 2.5252 Loss-g-fm: 8.0709 Loss-g-mel: 18.7991 Loss-g-dur: 1.5222 Loss-g-kl: 1.3742 lr: 0.0002 grad_norm_g: 860.6147 grad_norm_d: 99.4676
======> Epoch: 914
Train Epoch: 915 [42.31%] G-Loss: 30.5333 D-Loss: 2.1735 Loss-g-fm: 7.2188 Loss-g-mel: 17.9641 Loss-g-dur: 1.3380 Loss-g-kl: 1.2324 lr: 0.0002 grad_norm_g: 530.1232 grad_norm_d: 104.5788
======> Epoch: 915
Train Epoch: 916 [38.46%] G-Loss: 29.4799 D-Loss: 2.3542 Loss-g-fm: 6.5054 Loss-g-mel: 17.7446 Loss-g-dur: 1.3949 Loss-g-kl: 1.3217 lr: 0.0002 grad_norm_g: 755.9796 grad_norm_d: 76.9300
======> Epoch: 916
Train Epoch: 917 [34.62%] G-Loss: 32.6529 D-Loss: 2.2798 Loss-g-fm: 8.0576 Loss-g-mel: 19.1788 Loss-g-dur: 1.4741 Loss-g-kl: 1.5227 lr: 0.0002 grad_norm_g: 918.1134 grad_norm_d: 72.1590
======> Epoch: 917
Train Epoch: 918 [30.77%] G-Loss: 31.0260 D-Loss: 2.3528 Loss-g-fm: 7.3380 Loss-g-mel: 18.3137 Loss-g-dur: 1.5865 Loss-g-kl: 1.4809 lr: 0.0002 grad_norm_g: 803.8479 grad_norm_d: 61.6502
======> Epoch: 918
Train Epoch: 919 [26.92%] G-Loss: 32.9397 D-Loss: 2.3016 Loss-g-fm: 7.9865 Loss-g-mel: 19.7191 Loss-g-dur: 1.4692 Loss-g-kl: 1.3444 lr: 0.0002 grad_norm_g: 738.5848 grad_norm_d: 63.5898
======> Epoch: 919
Train Epoch: 920 [23.08%] G-Loss: 32.4565 D-Loss: 2.3176 Loss-g-fm: 7.6948 Loss-g-mel: 19.2900 Loss-g-dur: 1.4697 Loss-g-kl: 1.5238 lr: 0.0002 grad_norm_g: 691.2708 grad_norm_d: 48.3639
======> Epoch: 920
Train Epoch: 921 [19.23%] G-Loss: 29.7420 D-Loss: 2.3314 Loss-g-fm: 8.1685 Loss-g-mel: 16.4222 Loss-g-dur: 1.2246 Loss-g-kl: 1.4430 lr: 0.0002 grad_norm_g: 695.1796 grad_norm_d: 49.8307
======> Epoch: 921
Train Epoch: 922 [15.38%] G-Loss: 31.7645 D-Loss: 2.3508 Loss-g-fm: 7.4537 Loss-g-mel: 19.1869 Loss-g-dur: 1.4553 Loss-g-kl: 1.2957 lr: 0.0002 grad_norm_g: 145.4707 grad_norm_d: 10.4085
======> Epoch: 922
Train Epoch: 923 [11.54%] G-Loss: 31.2553 D-Loss: 2.2440 Loss-g-fm: 7.3852 Loss-g-mel: 18.8995 Loss-g-dur: 1.3456 Loss-g-kl: 1.1388 lr: 0.0002 grad_norm_g: 863.2814 grad_norm_d: 82.4458
======> Epoch: 923
Train Epoch: 924 [7.69%] G-Loss: 31.2952 D-Loss: 2.4175 Loss-g-fm: 7.4447 Loss-g-mel: 18.4265 Loss-g-dur: 1.5093 Loss-g-kl: 1.4131 lr: 0.0002 grad_norm_g: 214.5031 grad_norm_d: 10.8463
Saving model and optimizer state at iteration 924 to /ZFS4T/tts/data/VITS/model_saved/G_24000.pth
Saving model and optimizer state at iteration 924 to /ZFS4T/tts/data/VITS/model_saved/D_24000.pth
======> Epoch: 924
Train Epoch: 925 [3.85%] G-Loss: 31.5334 D-Loss: 2.5403 Loss-g-fm: 7.6822 Loss-g-mel: 18.5812 Loss-g-dur: 1.3842 Loss-g-kl: 1.2658 lr: 0.0002 grad_norm_g: 661.1863 grad_norm_d: 144.7804
======> Epoch: 925
Train Epoch: 926 [0.00%] G-Loss: 32.0879 D-Loss: 2.3817 Loss-g-fm: 7.9308 Loss-g-mel: 18.6008 Loss-g-dur: 1.5770 Loss-g-kl: 1.5113 lr: 0.0002 grad_norm_g: 609.4062 grad_norm_d: 49.7869
Train Epoch: 926 [96.15%] G-Loss: 28.6462 D-Loss: 2.4057 Loss-g-fm: 6.2786 Loss-g-mel: 17.4402 Loss-g-dur: 1.3645 Loss-g-kl: 1.1454 lr: 0.0002 grad_norm_g: 663.5409 grad_norm_d: 60.1117
======> Epoch: 926
Train Epoch: 927 [92.31%] G-Loss: 31.8979 D-Loss: 2.2576 Loss-g-fm: 7.9420 Loss-g-mel: 19.1600 Loss-g-dur: 1.5496 Loss-g-kl: 1.4057 lr: 0.0002 grad_norm_g: 496.2140 grad_norm_d: 44.1451
======> Epoch: 927
Train Epoch: 928 [88.46%] G-Loss: 30.8589 D-Loss: 2.2485 Loss-g-fm: 7.5305 Loss-g-mel: 18.2222 Loss-g-dur: 1.4274 Loss-g-kl: 1.1013 lr: 0.0002 grad_norm_g: 411.1498 grad_norm_d: 54.9584
======> Epoch: 928
Train Epoch: 929 [84.62%] G-Loss: 30.2953 D-Loss: 2.2844 Loss-g-fm: 7.2187 Loss-g-mel: 18.0391 Loss-g-dur: 1.3862 Loss-g-kl: 1.2923 lr: 0.0002 grad_norm_g: 176.6455 grad_norm_d: 9.3440
======> Epoch: 929
Train Epoch: 930 [80.77%] G-Loss: 29.1601 D-Loss: 2.2176 Loss-g-fm: 8.1670 Loss-g-mel: 15.9596 Loss-g-dur: 1.2576 Loss-g-kl: 1.2233 lr: 0.0002 grad_norm_g: 948.3068 grad_norm_d: 72.0833
======> Epoch: 930
Train Epoch: 931 [76.92%] G-Loss: 23.2831 D-Loss: 2.2798 Loss-g-fm: 7.0002 Loss-g-mel: 11.0773 Loss-g-dur: 0.9736 Loss-g-kl: 1.3097 lr: 0.0002 grad_norm_g: 776.1535 grad_norm_d: 29.8751
======> Epoch: 931
Train Epoch: 932 [73.08%] G-Loss: 32.2622 D-Loss: 2.3632 Loss-g-fm: 8.1292 Loss-g-mel: 18.6372 Loss-g-dur: 1.5201 Loss-g-kl: 1.3241 lr: 0.0002 grad_norm_g: 853.0419 grad_norm_d: 71.9624
======> Epoch: 932
Train Epoch: 933 [69.23%] G-Loss: 31.7523 D-Loss: 2.3280 Loss-g-fm: 8.0236 Loss-g-mel: 18.4231 Loss-g-dur: 1.4783 Loss-g-kl: 1.4013 lr: 0.0002 grad_norm_g: 137.7330 grad_norm_d: 27.9794
======> Epoch: 933
Train Epoch: 934 [65.38%] G-Loss: 31.2048 D-Loss: 2.3227 Loss-g-fm: 7.6962 Loss-g-mel: 18.5275 Loss-g-dur: 1.4178 Loss-g-kl: 1.1310 lr: 0.0002 grad_norm_g: 693.5517 grad_norm_d: 62.2512
======> Epoch: 934
Train Epoch: 935 [61.54%] G-Loss: 23.7721 D-Loss: 2.2649 Loss-g-fm: 7.3476 Loss-g-mel: 11.7427 Loss-g-dur: 0.8741 Loss-g-kl: 1.3415 lr: 0.0002 grad_norm_g: 438.3261 grad_norm_d: 35.6749
======> Epoch: 935
Train Epoch: 936 [57.69%] G-Loss: 30.8267 D-Loss: 2.5694 Loss-g-fm: 7.3114 Loss-g-mel: 18.3522 Loss-g-dur: 1.3556 Loss-g-kl: 1.1402 lr: 0.0002 grad_norm_g: 461.5494 grad_norm_d: 111.9091
======> Epoch: 936
Train Epoch: 937 [53.85%] G-Loss: 29.9651 D-Loss: 2.3462 Loss-g-fm: 6.7275 Loss-g-mel: 17.6456 Loss-g-dur: 1.3624 Loss-g-kl: 1.2961 lr: 0.0002 grad_norm_g: 484.8717 grad_norm_d: 26.4713
======> Epoch: 937
Train Epoch: 938 [50.00%] G-Loss: 31.9048 D-Loss: 2.4325 Loss-g-fm: 7.5476 Loss-g-mel: 18.5029 Loss-g-dur: 1.5458 Loss-g-kl: 1.3811 lr: 0.0002 grad_norm_g: 668.2234 grad_norm_d: 62.1160
======> Epoch: 938
Train Epoch: 939 [46.15%] G-Loss: 24.1423 D-Loss: 2.2687 Loss-g-fm: 7.4667 Loss-g-mel: 11.6496 Loss-g-dur: 0.9960 Loss-g-kl: 1.4124 lr: 0.0002 grad_norm_g: 936.7227 grad_norm_d: 48.8338
======> Epoch: 939
Train Epoch: 940 [42.31%] G-Loss: 32.8756 D-Loss: 2.2634 Loss-g-fm: 8.3735 Loss-g-mel: 18.9852 Loss-g-dur: 1.5872 Loss-g-kl: 1.5187 lr: 0.0002 grad_norm_g: 226.6414 grad_norm_d: 32.7252
======> Epoch: 940
Train Epoch: 941 [38.46%] G-Loss: 31.5715 D-Loss: 2.3280 Loss-g-fm: 7.4418 Loss-g-mel: 18.9809 Loss-g-dur: 1.5175 Loss-g-kl: 1.5492 lr: 0.0002 grad_norm_g: 453.2626 grad_norm_d: 29.5186
======> Epoch: 941
Train Epoch: 942 [34.62%] G-Loss: 30.2138 D-Loss: 2.3681 Loss-g-fm: 7.1536 Loss-g-mel: 18.2044 Loss-g-dur: 1.3538 Loss-g-kl: 1.0690 lr: 0.0002 grad_norm_g: 548.2894 grad_norm_d: 32.2319
======> Epoch: 942
Train Epoch: 943 [30.77%] G-Loss: 31.0982 D-Loss: 2.4961 Loss-g-fm: 7.6259 Loss-g-mel: 18.0275 Loss-g-dur: 1.4051 Loss-g-kl: 1.3800 lr: 0.0002 grad_norm_g: 695.2409 grad_norm_d: 78.2960
======> Epoch: 943
Train Epoch: 944 [26.92%] G-Loss: 31.9700 D-Loss: 2.3411 Loss-g-fm: 7.8628 Loss-g-mel: 18.4914 Loss-g-dur: 1.5688 Loss-g-kl: 1.5085 lr: 0.0002 grad_norm_g: 760.7945 grad_norm_d: 81.4952
======> Epoch: 944
Train Epoch: 945 [23.08%] G-Loss: 30.7045 D-Loss: 2.2249 Loss-g-fm: 7.5980 Loss-g-mel: 17.8034 Loss-g-dur: 1.3605 Loss-g-kl: 1.3528 lr: 0.0002 grad_norm_g: 410.9688 grad_norm_d: 8.9929
======> Epoch: 945
Train Epoch: 946 [19.23%] G-Loss: 33.9231 D-Loss: 2.1269 Loss-g-fm: 9.2064 Loss-g-mel: 19.2769 Loss-g-dur: 1.4635 Loss-g-kl: 1.4190 lr: 0.0002 grad_norm_g: 828.0817 grad_norm_d: 49.4560
======> Epoch: 946
Train Epoch: 947 [15.38%] G-Loss: 32.2716 D-Loss: 2.3654 Loss-g-fm: 7.8517 Loss-g-mel: 18.9045 Loss-g-dur: 1.4943 Loss-g-kl: 1.5717 lr: 0.0002 grad_norm_g: 631.1723 grad_norm_d: 59.1420
======> Epoch: 947
Train Epoch: 948 [11.54%] G-Loss: 30.8682 D-Loss: 2.3594 Loss-g-fm: 6.8927 Loss-g-mel: 18.5369 Loss-g-dur: 1.3809 Loss-g-kl: 1.5117 lr: 0.0002 grad_norm_g: 579.2287 grad_norm_d: 52.1465
======> Epoch: 948
Train Epoch: 949 [7.69%] G-Loss: 30.4065 D-Loss: 2.2166 Loss-g-fm: 7.2915 Loss-g-mel: 18.0377 Loss-g-dur: 1.3792 Loss-g-kl: 1.1766 lr: 0.0002 grad_norm_g: 914.4535 grad_norm_d: 55.2077
======> Epoch: 949
Train Epoch: 950 [3.85%] G-Loss: 32.8929 D-Loss: 2.2512 Loss-g-fm: 8.3590 Loss-g-mel: 19.3371 Loss-g-dur: 1.3443 Loss-g-kl: 1.3362 lr: 0.0002 grad_norm_g: 251.9881 grad_norm_d: 12.6879
======> Epoch: 950
Train Epoch: 951 [0.00%] G-Loss: 29.8866 D-Loss: 2.4103 Loss-g-fm: 7.0580 Loss-g-mel: 17.5838 Loss-g-dur: 1.3617 Loss-g-kl: 1.5073 lr: 0.0002 grad_norm_g: 355.5902 grad_norm_d: 16.1891
Train Epoch: 951 [96.15%] G-Loss: 33.6491 D-Loss: 2.2758 Loss-g-fm: 8.8107 Loss-g-mel: 19.0333 Loss-g-dur: 1.4819 Loss-g-kl: 1.4850 lr: 0.0002 grad_norm_g: 940.7602 grad_norm_d: 71.0831
======> Epoch: 951
Train Epoch: 952 [92.31%] G-Loss: 31.4117 D-Loss: 2.4883 Loss-g-fm: 7.6363 Loss-g-mel: 18.3949 Loss-g-dur: 1.3819 Loss-g-kl: 1.4074 lr: 0.0002 grad_norm_g: 104.9032 grad_norm_d: 89.4551
======> Epoch: 952
Train Epoch: 953 [88.46%] G-Loss: 32.4029 D-Loss: 2.2570 Loss-g-fm: 8.1558 Loss-g-mel: 19.0267 Loss-g-dur: 1.5197 Loss-g-kl: 1.4575 lr: 0.0002 grad_norm_g: 598.1016 grad_norm_d: 48.1775
======> Epoch: 953
Train Epoch: 954 [84.62%] G-Loss: 31.0121 D-Loss: 2.2241 Loss-g-fm: 7.7736 Loss-g-mel: 18.4031 Loss-g-dur: 1.3729 Loss-g-kl: 1.0264 lr: 0.0002 grad_norm_g: 710.5175 grad_norm_d: 47.6699
======> Epoch: 954
Train Epoch: 955 [80.77%] G-Loss: 33.6083 D-Loss: 2.3044 Loss-g-fm: 8.4185 Loss-g-mel: 19.0670 Loss-g-dur: 1.5452 Loss-g-kl: 1.3646 lr: 0.0002 grad_norm_g: 732.0946 grad_norm_d: 56.7683
======> Epoch: 955
Train Epoch: 956 [76.92%] G-Loss: 31.4288 D-Loss: 2.4460 Loss-g-fm: 7.4291 Loss-g-mel: 18.3456 Loss-g-dur: 1.4993 Loss-g-kl: 1.3096 lr: 0.0002 grad_norm_g: 175.8209 grad_norm_d: 29.0555
======> Epoch: 956
Train Epoch: 957 [73.08%] G-Loss: 29.3248 D-Loss: 2.4387 Loss-g-fm: 6.5206 Loss-g-mel: 18.0125 Loss-g-dur: 1.3879 Loss-g-kl: 1.2690 lr: 0.0002 grad_norm_g: 614.6163 grad_norm_d: 19.6324
======> Epoch: 957
Train Epoch: 958 [69.23%] G-Loss: 30.3583 D-Loss: 2.3645 Loss-g-fm: 7.4008 Loss-g-mel: 17.9594 Loss-g-dur: 1.3475 Loss-g-kl: 1.2758 lr: 0.0002 grad_norm_g: 1005.8242 grad_norm_d: 49.3407
======> Epoch: 958
Train Epoch: 959 [65.38%] G-Loss: 29.9678 D-Loss: 2.2507 Loss-g-fm: 7.2045 Loss-g-mel: 17.6415 Loss-g-dur: 1.3642 Loss-g-kl: 1.2097 lr: 0.0002 grad_norm_g: 581.9817 grad_norm_d: 38.9061
======> Epoch: 959
Train Epoch: 960 [61.54%] G-Loss: 23.6704 D-Loss: 2.2344 Loss-g-fm: 7.5980 Loss-g-mel: 11.3520 Loss-g-dur: 0.8962 Loss-g-kl: 1.3843 lr: 0.0002 grad_norm_g: 1170.6444 grad_norm_d: 57.1121
======> Epoch: 960
Train Epoch: 961 [57.69%] G-Loss: 31.4328 D-Loss: 2.2074 Loss-g-fm: 7.8269 Loss-g-mel: 18.5624 Loss-g-dur: 1.3896 Loss-g-kl: 1.1570 lr: 0.0002 grad_norm_g: 702.7062 grad_norm_d: 65.2520
======> Epoch: 961
Train Epoch: 962 [53.85%] G-Loss: 23.9675 D-Loss: 2.2764 Loss-g-fm: 7.8091 Loss-g-mel: 10.8632 Loss-g-dur: 0.9585 Loss-g-kl: 1.3803 lr: 0.0002 grad_norm_g: 387.5555 grad_norm_d: 35.8453
======> Epoch: 962
Train Epoch: 963 [50.00%] G-Loss: 31.3652 D-Loss: 2.3191 Loss-g-fm: 7.6410 Loss-g-mel: 18.4191 Loss-g-dur: 1.4008 Loss-g-kl: 1.2151 lr: 0.0002 grad_norm_g: 308.5290 grad_norm_d: 44.1596
======> Epoch: 963
Train Epoch: 964 [46.15%] G-Loss: 31.2090 D-Loss: 2.2499 Loss-g-fm: 7.3265 Loss-g-mel: 19.0393 Loss-g-dur: 1.3511 Loss-g-kl: 1.2590 lr: 0.0002 grad_norm_g: 237.2379 grad_norm_d: 19.9602
======> Epoch: 964
Train Epoch: 965 [42.31%] G-Loss: 31.1475 D-Loss: 2.2748 Loss-g-fm: 7.5620 Loss-g-mel: 18.4432 Loss-g-dur: 1.4550 Loss-g-kl: 1.3842 lr: 0.0002 grad_norm_g: 294.2693 grad_norm_d: 13.2787
======> Epoch: 965
Train Epoch: 966 [38.46%] G-Loss: 29.5044 D-Loss: 2.4900 Loss-g-fm: 6.8170 Loss-g-mel: 17.7852 Loss-g-dur: 1.3919 Loss-g-kl: 1.1977 lr: 0.0002 grad_norm_g: 40.2873 grad_norm_d: 11.6189
======> Epoch: 966
Train Epoch: 967 [34.62%] G-Loss: 33.1497 D-Loss: 2.2584 Loss-g-fm: 8.8658 Loss-g-mel: 18.6904 Loss-g-dur: 1.4634 Loss-g-kl: 1.3811 lr: 0.0002 grad_norm_g: 458.3412 grad_norm_d: 20.7011
======> Epoch: 967
Train Epoch: 968 [30.77%] G-Loss: 30.4880 D-Loss: 2.3775 Loss-g-fm: 7.2821 Loss-g-mel: 18.3765 Loss-g-dur: 1.3261 Loss-g-kl: 1.1015 lr: 0.0002 grad_norm_g: 751.8274 grad_norm_d: 60.0269
======> Epoch: 968
Train Epoch: 969 [26.92%] G-Loss: 30.7227 D-Loss: 2.4058 Loss-g-fm: 7.1248 Loss-g-mel: 18.2432 Loss-g-dur: 1.4726 Loss-g-kl: 1.3416 lr: 0.0002 grad_norm_g: 428.9106 grad_norm_d: 82.7874
======> Epoch: 969
Train Epoch: 970 [23.08%] G-Loss: 23.8580 D-Loss: 2.2729 Loss-g-fm: 7.7426 Loss-g-mel: 11.1017 Loss-g-dur: 0.9381 Loss-g-kl: 1.3046 lr: 0.0002 grad_norm_g: 191.9967 grad_norm_d: 21.2625
======> Epoch: 970
Train Epoch: 971 [19.23%] G-Loss: 31.7301 D-Loss: 2.2686 Loss-g-fm: 7.7987 Loss-g-mel: 18.2211 Loss-g-dur: 1.4950 Loss-g-kl: 1.5150 lr: 0.0002 grad_norm_g: 631.3346 grad_norm_d: 65.1202
======> Epoch: 971
Train Epoch: 972 [15.38%] G-Loss: 30.9968 D-Loss: 2.3874 Loss-g-fm: 7.5018 Loss-g-mel: 18.3346 Loss-g-dur: 1.3994 Loss-g-kl: 1.1962 lr: 0.0002 grad_norm_g: 819.0662 grad_norm_d: 55.3120
======> Epoch: 972
Train Epoch: 973 [11.54%] G-Loss: 33.0183 D-Loss: 2.4068 Loss-g-fm: 7.9709 Loss-g-mel: 19.5669 Loss-g-dur: 1.3839 Loss-g-kl: 1.4739 lr: 0.0002 grad_norm_g: 753.7989 grad_norm_d: 48.3492
======> Epoch: 973
Train Epoch: 974 [7.69%] G-Loss: 30.5815 D-Loss: 2.2835 Loss-g-fm: 7.5048 Loss-g-mel: 18.0012 Loss-g-dur: 1.3425 Loss-g-kl: 1.3800 lr: 0.0002 grad_norm_g: 163.6456 grad_norm_d: 5.3579
======> Epoch: 974
Train Epoch: 975 [3.85%] G-Loss: 32.5510 D-Loss: 2.1571 Loss-g-fm: 8.6457 Loss-g-mel: 18.7016 Loss-g-dur: 1.3580 Loss-g-kl: 1.2408 lr: 0.0002 grad_norm_g: 919.5235 grad_norm_d: 84.0105
======> Epoch: 975
Train Epoch: 976 [0.00%] G-Loss: 30.2172 D-Loss: 2.2885 Loss-g-fm: 7.3059 Loss-g-mel: 18.2148 Loss-g-dur: 1.3835 Loss-g-kl: 1.0232 lr: 0.0002 grad_norm_g: 545.0874 grad_norm_d: 60.6257
Train Epoch: 976 [96.15%] G-Loss: 28.9791 D-Loss: 2.2122 Loss-g-fm: 8.1068 Loss-g-mel: 15.7339 Loss-g-dur: 1.1931 Loss-g-kl: 1.3769 lr: 0.0002 grad_norm_g: 721.7761 grad_norm_d: 80.6141
======> Epoch: 976
Train Epoch: 977 [92.31%] G-Loss: 30.0879 D-Loss: 2.2518 Loss-g-fm: 7.2304 Loss-g-mel: 17.5607 Loss-g-dur: 1.3606 Loss-g-kl: 1.2841 lr: 0.0002 grad_norm_g: 757.7849 grad_norm_d: 65.7675
======> Epoch: 977
Train Epoch: 978 [88.46%] G-Loss: 32.1189 D-Loss: 2.3011 Loss-g-fm: 8.7422 Loss-g-mel: 18.4127 Loss-g-dur: 1.3393 Loss-g-kl: 1.1690 lr: 0.0002 grad_norm_g: 1044.2062 grad_norm_d: 83.5316
======> Epoch: 978
Train Epoch: 979 [84.62%] G-Loss: 30.8562 D-Loss: 2.3056 Loss-g-fm: 7.9574 Loss-g-mel: 17.8437 Loss-g-dur: 1.3789 Loss-g-kl: 1.2503 lr: 0.0002 grad_norm_g: 876.9346 grad_norm_d: 67.8283
======> Epoch: 979
Train Epoch: 980 [80.77%] G-Loss: 30.6156 D-Loss: 2.2887 Loss-g-fm: 7.8827 Loss-g-mel: 17.4757 Loss-g-dur: 1.3283 Loss-g-kl: 1.2665 lr: 0.0002 grad_norm_g: 434.2984 grad_norm_d: 23.5836
======> Epoch: 980
Train Epoch: 981 [76.92%] G-Loss: 30.6484 D-Loss: 2.2419 Loss-g-fm: 7.8055 Loss-g-mel: 17.9546 Loss-g-dur: 1.3527 Loss-g-kl: 1.1560 lr: 0.0002 grad_norm_g: 750.5660 grad_norm_d: 57.2294
======> Epoch: 981
Train Epoch: 982 [73.08%] G-Loss: 33.0330 D-Loss: 2.2885 Loss-g-fm: 8.3128 Loss-g-mel: 19.3513 Loss-g-dur: 1.4612 Loss-g-kl: 1.5275 lr: 0.0002 grad_norm_g: 639.3151 grad_norm_d: 26.7696
======> Epoch: 982
Train Epoch: 983 [69.23%] G-Loss: 23.5001 D-Loss: 2.2478 Loss-g-fm: 7.2279 Loss-g-mel: 11.2017 Loss-g-dur: 0.8842 Loss-g-kl: 1.3576 lr: 0.0002 grad_norm_g: 1014.7997 grad_norm_d: 37.8694
======> Epoch: 983
Train Epoch: 984 [65.38%] G-Loss: 30.7189 D-Loss: 2.2727 Loss-g-fm: 7.3842 Loss-g-mel: 18.1387 Loss-g-dur: 1.3583 Loss-g-kl: 1.3599 lr: 0.0002 grad_norm_g: 427.3357 grad_norm_d: 18.3079
======> Epoch: 984
Train Epoch: 985 [61.54%] G-Loss: 30.2894 D-Loss: 2.5726 Loss-g-fm: 7.0302 Loss-g-mel: 18.1325 Loss-g-dur: 1.3543 Loss-g-kl: 1.1627 lr: 0.0002 grad_norm_g: 529.3079 grad_norm_d: 120.4228
======> Epoch: 985
Train Epoch: 986 [57.69%] G-Loss: 31.5735 D-Loss: 2.3304 Loss-g-fm: 8.0891 Loss-g-mel: 18.6104 Loss-g-dur: 1.3522 Loss-g-kl: 1.2238 lr: 0.0002 grad_norm_g: 476.7940 grad_norm_d: 81.8117
======> Epoch: 986
Train Epoch: 987 [53.85%] G-Loss: 30.9740 D-Loss: 2.1748 Loss-g-fm: 9.1899 Loss-g-mel: 16.7474 Loss-g-dur: 1.2080 Loss-g-kl: 1.3263 lr: 0.0002 grad_norm_g: 190.1909 grad_norm_d: 8.5603
======> Epoch: 987
Train Epoch: 988 [50.00%] G-Loss: 22.8463 D-Loss: 2.3484 Loss-g-fm: 6.8910 Loss-g-mel: 10.7320 Loss-g-dur: 0.9552 Loss-g-kl: 1.3629 lr: 0.0002 grad_norm_g: 206.2932 grad_norm_d: 15.8769
======> Epoch: 988
Train Epoch: 989 [46.15%] G-Loss: 22.4632 D-Loss: 2.2037 Loss-g-fm: 7.5718 Loss-g-mel: 10.2240 Loss-g-dur: 0.9036 Loss-g-kl: 1.2172 lr: 0.0002 grad_norm_g: 112.7334 grad_norm_d: 15.0604
======> Epoch: 989
Train Epoch: 990 [42.31%] G-Loss: 32.9332 D-Loss: 2.4493 Loss-g-fm: 8.6373 Loss-g-mel: 19.0276 Loss-g-dur: 1.4238 Loss-g-kl: 1.3997 lr: 0.0002 grad_norm_g: 520.4257 grad_norm_d: 29.9851
======> Epoch: 990
Train Epoch: 991 [38.46%] G-Loss: 27.7395 D-Loss: 2.3116 Loss-g-fm: 7.2024 Loss-g-mel: 15.3808 Loss-g-dur: 1.2405 Loss-g-kl: 1.2885 lr: 0.0002 grad_norm_g: 415.4748 grad_norm_d: 10.5255
======> Epoch: 991
Train Epoch: 992 [34.62%] G-Loss: 31.5000 D-Loss: 2.2663 Loss-g-fm: 7.6005 Loss-g-mel: 18.7268 Loss-g-dur: 1.3382 Loss-g-kl: 1.2934 lr: 0.0002 grad_norm_g: 581.2841 grad_norm_d: 36.7194
======> Epoch: 992
Train Epoch: 993 [30.77%] G-Loss: 32.7403 D-Loss: 2.1452 Loss-g-fm: 8.6060 Loss-g-mel: 18.4289 Loss-g-dur: 1.3574 Loss-g-kl: 1.3728 lr: 0.0002 grad_norm_g: 843.5849 grad_norm_d: 64.9853
======> Epoch: 993
Train Epoch: 994 [26.92%] G-Loss: 32.2682 D-Loss: 2.3255 Loss-g-fm: 8.3662 Loss-g-mel: 18.4806 Loss-g-dur: 1.3262 Loss-g-kl: 1.2623 lr: 0.0002 grad_norm_g: 770.8491 grad_norm_d: 104.0587
======> Epoch: 994
Train Epoch: 995 [23.08%] G-Loss: 23.8407 D-Loss: 2.0429 Loss-g-fm: 8.1393 Loss-g-mel: 10.5513 Loss-g-dur: 0.9320 Loss-g-kl: 1.3442 lr: 0.0002 grad_norm_g: 1041.6970 grad_norm_d: 32.8811
======> Epoch: 995
Train Epoch: 996 [19.23%] G-Loss: 31.0981 D-Loss: 2.2575 Loss-g-fm: 7.6767 Loss-g-mel: 17.8836 Loss-g-dur: 1.4222 Loss-g-kl: 1.3653 lr: 0.0002 grad_norm_g: 756.8477 grad_norm_d: 64.4987
======> Epoch: 996
Train Epoch: 997 [15.38%] G-Loss: 32.1920 D-Loss: 2.2095 Loss-g-fm: 8.4135 Loss-g-mel: 18.2711 Loss-g-dur: 1.5391 Loss-g-kl: 1.5211 lr: 0.0002 grad_norm_g: 650.3531 grad_norm_d: 51.4378
======> Epoch: 997
Train Epoch: 998 [11.54%] G-Loss: 31.9335 D-Loss: 2.3917 Loss-g-fm: 8.0957 Loss-g-mel: 18.4601 Loss-g-dur: 1.4623 Loss-g-kl: 1.4371 lr: 0.0002 grad_norm_g: 431.0932 grad_norm_d: 63.2548
======> Epoch: 998
Train Epoch: 999 [7.69%] G-Loss: 31.1091 D-Loss: 2.3313 Loss-g-fm: 8.0671 Loss-g-mel: 17.9135 Loss-g-dur: 1.3391 Loss-g-kl: 1.2734 lr: 0.0002 grad_norm_g: 456.4814 grad_norm_d: 72.3206
======> Epoch: 999
Train Epoch: 1000 [3.85%] G-Loss: 31.4646 D-Loss: 2.3499 Loss-g-fm: 7.8536 Loss-g-mel: 18.3534 Loss-g-dur: 1.4736 Loss-g-kl: 1.1460 lr: 0.0002 grad_norm_g: 771.4403 grad_norm_d: 66.5881
======> Epoch: 1000
Train Epoch: 1001 [0.00%] G-Loss: 32.7636 D-Loss: 2.2880 Loss-g-fm: 8.4116 Loss-g-mel: 18.8477 Loss-g-dur: 1.5203 Loss-g-kl: 1.4355 lr: 0.0002 grad_norm_g: 648.9860 grad_norm_d: 42.1011
Saving model and optimizer state at iteration 1001 to /ZFS4T/tts/data/VITS/model_saved/G_26000.pth
Saving model and optimizer state at iteration 1001 to /ZFS4T/tts/data/VITS/model_saved/D_26000.pth
Train Epoch: 1001 [96.15%] G-Loss: 31.7057 D-Loss: 2.1850 Loss-g-fm: 7.9699 Loss-g-mel: 18.1037 Loss-g-dur: 1.5164 Loss-g-kl: 1.5391 lr: 0.0002 grad_norm_g: 291.5626 grad_norm_d: 37.4264
======> Epoch: 1001
Train Epoch: 1002 [92.31%] G-Loss: 32.1255 D-Loss: 2.3922 Loss-g-fm: 7.7302 Loss-g-mel: 19.1451 Loss-g-dur: 1.5101 Loss-g-kl: 1.2658 lr: 0.0002 grad_norm_g: 784.4012 grad_norm_d: 57.3630
======> Epoch: 1002
Train Epoch: 1003 [88.46%] G-Loss: 33.3767 D-Loss: 2.3104 Loss-g-fm: 9.0196 Loss-g-mel: 18.7171 Loss-g-dur: 1.5036 Loss-g-kl: 1.4809 lr: 0.0002 grad_norm_g: 838.1733 grad_norm_d: 77.4539
======> Epoch: 1003
Train Epoch: 1004 [84.62%] G-Loss: 30.4399 D-Loss: 2.3481 Loss-g-fm: 7.1345 Loss-g-mel: 18.2749 Loss-g-dur: 1.3457 Loss-g-kl: 1.2020 lr: 0.0002 grad_norm_g: 533.6520 grad_norm_d: 19.9484
======> Epoch: 1004
Train Epoch: 1005 [80.77%] G-Loss: 29.0443 D-Loss: 2.5636 Loss-g-fm: 6.4152 Loss-g-mel: 17.5071 Loss-g-dur: 1.4077 Loss-g-kl: 1.3219 lr: 0.0002 grad_norm_g: 226.2380 grad_norm_d: 13.1121
======> Epoch: 1005
Train Epoch: 1006 [76.92%] G-Loss: 30.5999 D-Loss: 2.2982 Loss-g-fm: 8.9089 Loss-g-mel: 16.6074 Loss-g-dur: 1.2423 Loss-g-kl: 1.2347 lr: 0.0002 grad_norm_g: 823.1979 grad_norm_d: 65.8638
======> Epoch: 1006
Train Epoch: 1007 [73.08%] G-Loss: 28.9555 D-Loss: 2.3266 Loss-g-fm: 7.6071 Loss-g-mel: 16.3423 Loss-g-dur: 1.2664 Loss-g-kl: 1.4088 lr: 0.0002 grad_norm_g: 520.1767 grad_norm_d: 21.3961
======> Epoch: 1007
Train Epoch: 1008 [69.23%] G-Loss: 31.6467 D-Loss: 2.3678 Loss-g-fm: 7.4574 Loss-g-mel: 18.7492 Loss-g-dur: 1.3783 Loss-g-kl: 1.2905 lr: 0.0002 grad_norm_g: 463.5764 grad_norm_d: 84.7691
======> Epoch: 1008
Train Epoch: 1009 [65.38%] G-Loss: 30.6916 D-Loss: 2.3182 Loss-g-fm: 7.4967 Loss-g-mel: 18.0038 Loss-g-dur: 1.4575 Loss-g-kl: 1.3895 lr: 0.0002 grad_norm_g: 867.5472 grad_norm_d: 77.5738
======> Epoch: 1009
Train Epoch: 1010 [61.54%] G-Loss: 31.6894 D-Loss: 2.1616 Loss-g-fm: 8.3871 Loss-g-mel: 18.3573 Loss-g-dur: 1.3826 Loss-g-kl: 1.1499 lr: 0.0002 grad_norm_g: 713.0611 grad_norm_d: 58.3390
======> Epoch: 1010
Train Epoch: 1011 [57.69%] G-Loss: 23.6177 D-Loss: 2.2556 Loss-g-fm: 7.3730 Loss-g-mel: 11.2742 Loss-g-dur: 0.8649 Loss-g-kl: 1.4030 lr: 0.0002 grad_norm_g: 275.3525 grad_norm_d: 26.8015
======> Epoch: 1011
Train Epoch: 1012 [53.85%] G-Loss: 29.9808 D-Loss: 2.4126 Loss-g-fm: 7.5415 Loss-g-mel: 17.4751 Loss-g-dur: 1.3543 Loss-g-kl: 1.2484 lr: 0.0002 grad_norm_g: 576.6080 grad_norm_d: 62.0633
======> Epoch: 1012
Train Epoch: 1013 [50.00%] G-Loss: 31.0901 D-Loss: 2.2262 Loss-g-fm: 7.6746 Loss-g-mel: 17.9269 Loss-g-dur: 1.5591 Loss-g-kl: 1.4465 lr: 0.0002 grad_norm_g: 213.6352 grad_norm_d: 4.9715
======> Epoch: 1013
Train Epoch: 1014 [46.15%] G-Loss: 30.8149 D-Loss: 2.3675 Loss-g-fm: 7.3827 Loss-g-mel: 18.4626 Loss-g-dur: 1.3474 Loss-g-kl: 1.2272 lr: 0.0002 grad_norm_g: 241.4655 grad_norm_d: 23.2518
======> Epoch: 1014
Train Epoch: 1015 [42.31%] G-Loss: 33.6599 D-Loss: 2.2145 Loss-g-fm: 8.8994 Loss-g-mel: 18.7008 Loss-g-dur: 1.4564 Loss-g-kl: 1.3723 lr: 0.0002 grad_norm_g: 236.2532 grad_norm_d: 21.6593
======> Epoch: 1015
Train Epoch: 1016 [38.46%] G-Loss: 31.4458 D-Loss: 2.3714 Loss-g-fm: 7.4822 Loss-g-mel: 18.5949 Loss-g-dur: 1.4944 Loss-g-kl: 1.2550 lr: 0.0002 grad_norm_g: 406.8343 grad_norm_d: 19.1138
======> Epoch: 1016
Train Epoch: 1017 [34.62%] G-Loss: 32.1728 D-Loss: 2.2756 Loss-g-fm: 8.2693 Loss-g-mel: 18.4551 Loss-g-dur: 1.4532 Loss-g-kl: 1.4866 lr: 0.0002 grad_norm_g: 251.2864 grad_norm_d: 4.2867
======> Epoch: 1017
Train Epoch: 1018 [30.77%] G-Loss: 30.8191 D-Loss: 2.3126 Loss-g-fm: 7.8283 Loss-g-mel: 18.0463 Loss-g-dur: 1.3497 Loss-g-kl: 1.1013 lr: 0.0002 grad_norm_g: 823.9401 grad_norm_d: 82.3714
======> Epoch: 1018
Train Epoch: 1019 [26.92%] G-Loss: 30.6581 D-Loss: 2.3800 Loss-g-fm: 7.4552 Loss-g-mel: 18.0941 Loss-g-dur: 1.3721 Loss-g-kl: 1.2820 lr: 0.0002 grad_norm_g: 619.2151 grad_norm_d: 77.2097
======> Epoch: 1019
Train Epoch: 1020 [23.08%] G-Loss: 30.9031 D-Loss: 2.2358 Loss-g-fm: 8.1872 Loss-g-mel: 17.7899 Loss-g-dur: 1.3633 Loss-g-kl: 1.2725 lr: 0.0002 grad_norm_g: 822.5767 grad_norm_d: 66.2936
======> Epoch: 1020
Train Epoch: 1021 [19.23%] G-Loss: 32.4852 D-Loss: 2.3495 Loss-g-fm: 8.8136 Loss-g-mel: 18.7059 Loss-g-dur: 1.3747 Loss-g-kl: 1.1047 lr: 0.0002 grad_norm_g: 651.8857 grad_norm_d: 65.7349
======> Epoch: 1021
Train Epoch: 1022 [15.38%] G-Loss: 29.1126 D-Loss: 2.4893 Loss-g-fm: 6.7177 Loss-g-mel: 17.2661 Loss-g-dur: 1.3802 Loss-g-kl: 1.2789 lr: 0.0002 grad_norm_g: 493.8617 grad_norm_d: 84.4485
======> Epoch: 1022
Train Epoch: 1023 [11.54%] G-Loss: 30.4301 D-Loss: 2.2232 Loss-g-fm: 7.5523 Loss-g-mel: 18.0178 Loss-g-dur: 1.3283 Loss-g-kl: 1.0708 lr: 0.0002 grad_norm_g: 725.0745 grad_norm_d: 71.2349
======> Epoch: 1023
Train Epoch: 1024 [7.69%] G-Loss: 29.5928 D-Loss: 2.3032 Loss-g-fm: 6.9697 Loss-g-mel: 17.6673 Loss-g-dur: 1.3083 Loss-g-kl: 1.0605 lr: 0.0002 grad_norm_g: 256.0935 grad_norm_d: 29.0459
======> Epoch: 1024
Train Epoch: 1025 [3.85%] G-Loss: 29.8401 D-Loss: 2.3422 Loss-g-fm: 6.8301 Loss-g-mel: 17.7775 Loss-g-dur: 1.3148 Loss-g-kl: 1.3669 lr: 0.0002 grad_norm_g: 154.4304 grad_norm_d: 13.2167
======> Epoch: 1025
Train Epoch: 1026 [0.00%] G-Loss: 24.3974 D-Loss: 2.1579 Loss-g-fm: 8.0551 Loss-g-mel: 10.6993 Loss-g-dur: 0.9842 Loss-g-kl: 1.4300 lr: 0.0002 grad_norm_g: 220.3291 grad_norm_d: 17.5164
Train Epoch: 1026 [96.15%] G-Loss: 29.6431 D-Loss: 2.3136 Loss-g-fm: 7.2809 Loss-g-mel: 17.5598 Loss-g-dur: 1.3725 Loss-g-kl: 1.1442 lr: 0.0002 grad_norm_g: 541.5898 grad_norm_d: 44.0036
======> Epoch: 1026
Train Epoch: 1027 [92.31%] G-Loss: 23.3763 D-Loss: 2.2506 Loss-g-fm: 7.4157 Loss-g-mel: 10.7878 Loss-g-dur: 0.9357 Loss-g-kl: 1.4833 lr: 0.0002 grad_norm_g: 1017.8387 grad_norm_d: 45.4921
======> Epoch: 1027
Train Epoch: 1028 [88.46%] G-Loss: 31.4007 D-Loss: 2.2626 Loss-g-fm: 8.3296 Loss-g-mel: 17.9640 Loss-g-dur: 1.4158 Loss-g-kl: 1.1816 lr: 0.0002 grad_norm_g: 701.6958 grad_norm_d: 63.9582
======> Epoch: 1028
Train Epoch: 1029 [84.62%] G-Loss: 31.5674 D-Loss: 2.3747 Loss-g-fm: 7.9034 Loss-g-mel: 18.5003 Loss-g-dur: 1.3654 Loss-g-kl: 1.3136 lr: 0.0002 grad_norm_g: 562.0801 grad_norm_d: 52.0646
======> Epoch: 1029
Train Epoch: 1030 [80.77%] G-Loss: 27.9704 D-Loss: 2.4182 Loss-g-fm: 6.2800 Loss-g-mel: 16.9193 Loss-g-dur: 1.3177 Loss-g-kl: 1.1413 lr: 0.0002 grad_norm_g: 705.6017 grad_norm_d: 80.2890
======> Epoch: 1030
Train Epoch: 1031 [76.92%] G-Loss: 31.5007 D-Loss: 2.3299 Loss-g-fm: 7.7374 Loss-g-mel: 18.0975 Loss-g-dur: 1.5157 Loss-g-kl: 1.4237 lr: 0.0002 grad_norm_g: 249.8347 grad_norm_d: 32.8786
======> Epoch: 1031
Train Epoch: 1032 [73.08%] G-Loss: 34.1746 D-Loss: 2.2776 Loss-g-fm: 9.3435 Loss-g-mel: 19.2602 Loss-g-dur: 1.5117 Loss-g-kl: 1.4958 lr: 0.0002 grad_norm_g: 225.7897 grad_norm_d: 14.7259
======> Epoch: 1032
Train Epoch: 1033 [69.23%] G-Loss: 32.5654 D-Loss: 2.3319 Loss-g-fm: 8.5790 Loss-g-mel: 18.6618 Loss-g-dur: 1.4558 Loss-g-kl: 1.5112 lr: 0.0002 grad_norm_g: 691.9972 grad_norm_d: 85.4772
======> Epoch: 1033
Train Epoch: 1034 [65.38%] G-Loss: 21.6163 D-Loss: 2.3987 Loss-g-fm: 6.0233 Loss-g-mel: 10.6998 Loss-g-dur: 0.8819 Loss-g-kl: 1.4030 lr: 0.0002 grad_norm_g: 246.5521 grad_norm_d: 21.3389
======> Epoch: 1034
Train Epoch: 1035 [61.54%] G-Loss: 31.3398 D-Loss: 2.3225 Loss-g-fm: 7.4950 Loss-g-mel: 18.5472 Loss-g-dur: 1.4261 Loss-g-kl: 1.2840 lr: 0.0002 grad_norm_g: 464.9243 grad_norm_d: 44.1651
======> Epoch: 1035
Train Epoch: 1036 [57.69%] G-Loss: 31.4616 D-Loss: 2.4728 Loss-g-fm: 7.7182 Loss-g-mel: 18.1774 Loss-g-dur: 1.4366 Loss-g-kl: 1.1627 lr: 0.0002 grad_norm_g: 750.5601 grad_norm_d: 87.4665
======> Epoch: 1036
Train Epoch: 1037 [53.85%] G-Loss: 32.3026 D-Loss: 2.3835 Loss-g-fm: 8.4977 Loss-g-mel: 18.4453 Loss-g-dur: 1.3763 Loss-g-kl: 1.3312 lr: 0.0002 grad_norm_g: 607.5832 grad_norm_d: 38.1608
======> Epoch: 1037
Train Epoch: 1038 [50.00%] G-Loss: 31.8882 D-Loss: 2.3488 Loss-g-fm: 7.9957 Loss-g-mel: 18.1503 Loss-g-dur: 1.3723 Loss-g-kl: 1.3084 lr: 0.0002 grad_norm_g: 721.1017 grad_norm_d: 66.1347
======> Epoch: 1038
Train Epoch: 1039 [46.15%] G-Loss: 31.2339 D-Loss: 2.3197 Loss-g-fm: 7.9027 Loss-g-mel: 18.2420 Loss-g-dur: 1.3199 Loss-g-kl: 1.2302 lr: 0.0002 grad_norm_g: 44.1770 grad_norm_d: 29.4823
======> Epoch: 1039
Train Epoch: 1040 [42.31%] G-Loss: 29.2792 D-Loss: 2.3906 Loss-g-fm: 7.0665 Loss-g-mel: 17.4627 Loss-g-dur: 1.3329 Loss-g-kl: 1.0788 lr: 0.0002 grad_norm_g: 388.2352 grad_norm_d: 58.2771
======> Epoch: 1040
Train Epoch: 1041 [38.46%] G-Loss: 33.5984 D-Loss: 2.3215 Loss-g-fm: 9.1670 Loss-g-mel: 18.6690 Loss-g-dur: 1.5712 Loss-g-kl: 1.4851 lr: 0.0002 grad_norm_g: 866.0556 grad_norm_d: 40.9214
======> Epoch: 1041
Train Epoch: 1042 [34.62%] G-Loss: 32.3941 D-Loss: 2.2009 Loss-g-fm: 8.5545 Loss-g-mel: 18.3653 Loss-g-dur: 1.4658 Loss-g-kl: 1.3277 lr: 0.0002 grad_norm_g: 803.7475 grad_norm_d: 44.0619
======> Epoch: 1042
Train Epoch: 1043 [30.77%] G-Loss: 21.8054 D-Loss: 2.3182 Loss-g-fm: 6.6969 Loss-g-mel: 10.3151 Loss-g-dur: 0.8411 Loss-g-kl: 1.3986 lr: 0.0002 grad_norm_g: 519.9533 grad_norm_d: 17.6096
======> Epoch: 1043
Train Epoch: 1044 [26.92%] G-Loss: 33.8421 D-Loss: 2.2885 Loss-g-fm: 8.8247 Loss-g-mel: 19.3242 Loss-g-dur: 1.4243 Loss-g-kl: 1.3986 lr: 0.0002 grad_norm_g: 1067.2230 grad_norm_d: 79.3739
======> Epoch: 1044
Train Epoch: 1045 [23.08%] G-Loss: 31.6697 D-Loss: 2.2522 Loss-g-fm: 8.2612 Loss-g-mel: 18.2681 Loss-g-dur: 1.4032 Loss-g-kl: 1.4206 lr: 0.0002 grad_norm_g: 857.9202 grad_norm_d: 48.9038
======> Epoch: 1045
Train Epoch: 1046 [19.23%] G-Loss: 31.0445 D-Loss: 2.4942 Loss-g-fm: 7.4945 Loss-g-mel: 18.0143 Loss-g-dur: 1.3583 Loss-g-kl: 1.1771 lr: 0.0002 grad_norm_g: 729.8936 grad_norm_d: 99.4096
======> Epoch: 1046
Train Epoch: 1047 [15.38%] G-Loss: 23.4358 D-Loss: 2.5162 Loss-g-fm: 7.0584 Loss-g-mel: 11.9375 Loss-g-dur: 0.9796 Loss-g-kl: 1.4261 lr: 0.0002 grad_norm_g: 700.0412 grad_norm_d: 71.3916
======> Epoch: 1047
Train Epoch: 1048 [11.54%] G-Loss: 29.8618 D-Loss: 2.3775 Loss-g-fm: 7.6148 Loss-g-mel: 16.9991 Loss-g-dur: 1.3239 Loss-g-kl: 1.3424 lr: 0.0002 grad_norm_g: 725.5771 grad_norm_d: 75.5198
======> Epoch: 1048
Train Epoch: 1049 [7.69%] G-Loss: 31.5824 D-Loss: 2.2477 Loss-g-fm: 8.4444 Loss-g-mel: 18.2105 Loss-g-dur: 1.3594 Loss-g-kl: 1.1262 lr: 0.0002 grad_norm_g: 504.3212 grad_norm_d: 35.0210
======> Epoch: 1049
Train Epoch: 1050 [3.85%] G-Loss: 30.6514 D-Loss: 2.4286 Loss-g-fm: 7.2160 Loss-g-mel: 18.3199 Loss-g-dur: 1.3835 Loss-g-kl: 1.3097 lr: 0.0002 grad_norm_g: 307.6943 grad_norm_d: 46.8258
======> Epoch: 1050
Train Epoch: 1051 [0.00%] G-Loss: 29.3969 D-Loss: 2.4161 Loss-g-fm: 6.9040 Loss-g-mel: 17.4960 Loss-g-dur: 1.3576 Loss-g-kl: 1.2112 lr: 0.0002 grad_norm_g: 507.5379 grad_norm_d: 53.0034
Train Epoch: 1051 [96.15%] G-Loss: 33.2448 D-Loss: 2.2334 Loss-g-fm: 8.6265 Loss-g-mel: 19.6022 Loss-g-dur: 1.4656 Loss-g-kl: 1.4210 lr: 0.0002 grad_norm_g: 498.4367 grad_norm_d: 57.9567
======> Epoch: 1051
Train Epoch: 1052 [92.31%] G-Loss: 33.7455 D-Loss: 2.2394 Loss-g-fm: 9.0147 Loss-g-mel: 19.4329 Loss-g-dur: 1.4824 Loss-g-kl: 1.2097 lr: 0.0002 grad_norm_g: 1057.0899 grad_norm_d: 59.5625
======> Epoch: 1052
Train Epoch: 1053 [88.46%] G-Loss: 33.4406 D-Loss: 2.2261 Loss-g-fm: 8.8594 Loss-g-mel: 19.1831 Loss-g-dur: 1.3542 Loss-g-kl: 1.3311 lr: 0.0002 grad_norm_g: 605.8171 grad_norm_d: 75.9709
======> Epoch: 1053
Train Epoch: 1054 [84.62%] G-Loss: 30.0992 D-Loss: 2.4282 Loss-g-fm: 6.7389 Loss-g-mel: 18.0289 Loss-g-dur: 1.4714 Loss-g-kl: 1.4854 lr: 0.0002 grad_norm_g: 123.9149 grad_norm_d: 23.6310
======> Epoch: 1054
Train Epoch: 1055 [80.77%] G-Loss: 32.7843 D-Loss: 2.2073 Loss-g-fm: 9.0988 Loss-g-mel: 18.3640 Loss-g-dur: 1.3524 Loss-g-kl: 1.0767 lr: 0.0002 grad_norm_g: 870.0054 grad_norm_d: 97.1243
======> Epoch: 1055
Train Epoch: 1056 [76.92%] G-Loss: 34.9978 D-Loss: 2.5364 Loss-g-fm: 9.0193 Loss-g-mel: 20.0379 Loss-g-dur: 1.5734 Loss-g-kl: 1.5009 lr: 0.0002 grad_norm_g: 685.8616 grad_norm_d: 56.4913
======> Epoch: 1056
Train Epoch: 1057 [73.08%] G-Loss: 32.3042 D-Loss: 2.3693 Loss-g-fm: 7.8614 Loss-g-mel: 18.6876 Loss-g-dur: 1.5521 Loss-g-kl: 1.6751 lr: 0.0002 grad_norm_g: 786.8019 grad_norm_d: 63.6396
======> Epoch: 1057
Train Epoch: 1058 [69.23%] G-Loss: 33.9882 D-Loss: 2.3405 Loss-g-fm: 8.9560 Loss-g-mel: 19.3107 Loss-g-dur: 1.4967 Loss-g-kl: 1.4987 lr: 0.0002 grad_norm_g: 916.6247 grad_norm_d: 51.8389
======> Epoch: 1058
Train Epoch: 1059 [65.38%] G-Loss: 32.8304 D-Loss: 2.3563 Loss-g-fm: 8.6851 Loss-g-mel: 18.9452 Loss-g-dur: 1.4926 Loss-g-kl: 1.2604 lr: 0.0002 grad_norm_g: 769.5909 grad_norm_d: 40.3998
======> Epoch: 1059
Train Epoch: 1060 [61.54%] G-Loss: 23.2492 D-Loss: 2.2488 Loss-g-fm: 7.8963 Loss-g-mel: 9.9845 Loss-g-dur: 0.9498 Loss-g-kl: 1.2714 lr: 0.0002 grad_norm_g: 663.2314 grad_norm_d: 33.8929
======> Epoch: 1060
Train Epoch: 1061 [57.69%] G-Loss: 30.4697 D-Loss: 2.3905 Loss-g-fm: 7.4494 Loss-g-mel: 17.8422 Loss-g-dur: 1.3557 Loss-g-kl: 1.1478 lr: 0.0002 grad_norm_g: 114.8687 grad_norm_d: 13.0814
======> Epoch: 1061
Train Epoch: 1062 [53.85%] G-Loss: 32.0606 D-Loss: 2.3259 Loss-g-fm: 8.7952 Loss-g-mel: 18.2159 Loss-g-dur: 1.3235 Loss-g-kl: 1.3104 lr: 0.0002 grad_norm_g: 795.0816 grad_norm_d: 79.7301
======> Epoch: 1062
Train Epoch: 1063 [50.00%] G-Loss: 32.6006 D-Loss: 2.2689 Loss-g-fm: 8.4458 Loss-g-mel: 18.8834 Loss-g-dur: 1.4893 Loss-g-kl: 1.3937 lr: 0.0002 grad_norm_g: 974.3141 grad_norm_d: 76.8152
======> Epoch: 1063
Train Epoch: 1064 [46.15%] G-Loss: 30.6214 D-Loss: 2.4798 Loss-g-fm: 7.7881 Loss-g-mel: 17.9170 Loss-g-dur: 1.3192 Loss-g-kl: 1.3551 lr: 0.0002 grad_norm_g: 507.4020 grad_norm_d: 50.1632
======> Epoch: 1064
Train Epoch: 1065 [42.31%] G-Loss: 33.2285 D-Loss: 2.2096 Loss-g-fm: 8.7468 Loss-g-mel: 19.0991 Loss-g-dur: 1.4202 Loss-g-kl: 1.3160 lr: 0.0002 grad_norm_g: 607.2551 grad_norm_d: 31.2038
======> Epoch: 1065
Train Epoch: 1066 [38.46%] G-Loss: 32.5576 D-Loss: 2.3198 Loss-g-fm: 8.5949 Loss-g-mel: 18.6971 Loss-g-dur: 1.4530 Loss-g-kl: 1.3870 lr: 0.0002 grad_norm_g: 301.8120 grad_norm_d: 10.9217
======> Epoch: 1066
Train Epoch: 1067 [34.62%] G-Loss: 30.5213 D-Loss: 2.5773 Loss-g-fm: 7.4780 Loss-g-mel: 18.2125 Loss-g-dur: 1.3704 Loss-g-kl: 1.2848 lr: 0.0002 grad_norm_g: 348.6271 grad_norm_d: 69.4344
======> Epoch: 1067
Train Epoch: 1068 [30.77%] G-Loss: 33.5312 D-Loss: 2.4038 Loss-g-fm: 8.6015 Loss-g-mel: 18.9414 Loss-g-dur: 1.5351 Loss-g-kl: 1.4353 lr: 0.0002 grad_norm_g: 399.7910 grad_norm_d: 20.7362
======> Epoch: 1068
Train Epoch: 1069 [26.92%] G-Loss: 31.6610 D-Loss: 2.3014 Loss-g-fm: 8.5643 Loss-g-mel: 18.0165 Loss-g-dur: 1.3755 Loss-g-kl: 1.0481 lr: 0.0002 grad_norm_g: 831.3620 grad_norm_d: 68.5219
======> Epoch: 1069
Train Epoch: 1070 [23.08%] G-Loss: 31.2796 D-Loss: 2.3135 Loss-g-fm: 7.8807 Loss-g-mel: 17.9548 Loss-g-dur: 1.5710 Loss-g-kl: 1.5139 lr: 0.0002 grad_norm_g: 276.0087 grad_norm_d: 10.5641
======> Epoch: 1070
Train Epoch: 1071 [19.23%] G-Loss: 30.3551 D-Loss: 2.4308 Loss-g-fm: 7.4300 Loss-g-mel: 17.5933 Loss-g-dur: 1.3641 Loss-g-kl: 1.2131 lr: 0.0002 grad_norm_g: 631.1204 grad_norm_d: 68.0341
======> Epoch: 1071
Train Epoch: 1072 [15.38%] G-Loss: 32.7382 D-Loss: 2.1499 Loss-g-fm: 9.1251 Loss-g-mel: 18.3817 Loss-g-dur: 1.3446 Loss-g-kl: 1.1713 lr: 0.0002 grad_norm_g: 718.0310 grad_norm_d: 78.4542
======> Epoch: 1072
Train Epoch: 1073 [11.54%] G-Loss: 31.8140 D-Loss: 2.1536 Loss-g-fm: 8.3552 Loss-g-mel: 18.1941 Loss-g-dur: 1.3275 Loss-g-kl: 1.3427 lr: 0.0002 grad_norm_g: 588.8529 grad_norm_d: 40.4736
======> Epoch: 1073
Train Epoch: 1074 [7.69%] G-Loss: 33.0275 D-Loss: 2.1553 Loss-g-fm: 8.7327 Loss-g-mel: 18.9858 Loss-g-dur: 1.5274 Loss-g-kl: 1.3045 lr: 0.0002 grad_norm_g: 588.6327 grad_norm_d: 36.5008
======> Epoch: 1074
Train Epoch: 1075 [3.85%] G-Loss: 33.8498 D-Loss: 2.5543 Loss-g-fm: 9.2765 Loss-g-mel: 19.2030 Loss-g-dur: 1.5373 Loss-g-kl: 1.2496 lr: 0.0002 grad_norm_g: 776.6603 grad_norm_d: 42.2599
======> Epoch: 1075
Train Epoch: 1076 [0.00%] G-Loss: 29.4723 D-Loss: 2.2228 Loss-g-fm: 8.6529 Loss-g-mel: 15.9449 Loss-g-dur: 1.1310 Loss-g-kl: 1.3737 lr: 0.0002 grad_norm_g: 867.0771 grad_norm_d: 75.0346
Train Epoch: 1076 [96.15%] G-Loss: 29.0306 D-Loss: 2.2995 Loss-g-fm: 6.5746 Loss-g-mel: 17.2246 Loss-g-dur: 1.3304 Loss-g-kl: 1.4166 lr: 0.0002 grad_norm_g: 762.0371 grad_norm_d: 100.9325
======> Epoch: 1076
Train Epoch: 1077 [92.31%] G-Loss: 30.8361 D-Loss: 2.3313 Loss-g-fm: 7.9916 Loss-g-mel: 17.5878 Loss-g-dur: 1.3541 Loss-g-kl: 1.4348 lr: 0.0002 grad_norm_g: 348.7156 grad_norm_d: 17.8540
Saving model and optimizer state at iteration 1077 to /ZFS4T/tts/data/VITS/model_saved/G_28000.pth
Saving model and optimizer state at iteration 1077 to /ZFS4T/tts/data/VITS/model_saved/D_28000.pth
======> Epoch: 1077
Train Epoch: 1078 [88.46%] G-Loss: 31.7447 D-Loss: 2.4246 Loss-g-fm: 8.0632 Loss-g-mel: 18.3941 Loss-g-dur: 1.3430 Loss-g-kl: 1.4092 lr: 0.0002 grad_norm_g: 543.8016 grad_norm_d: 105.8511
======> Epoch: 1078
Train Epoch: 1079 [84.62%] G-Loss: 22.3684 D-Loss: 2.1623 Loss-g-fm: 7.2926 Loss-g-mel: 10.3563 Loss-g-dur: 0.9087 Loss-g-kl: 1.4178 lr: 0.0002 grad_norm_g: 240.1446 grad_norm_d: 19.7020
======> Epoch: 1079
Train Epoch: 1080 [80.77%] G-Loss: 30.9045 D-Loss: 2.3669 Loss-g-fm: 7.5034 Loss-g-mel: 17.9322 Loss-g-dur: 1.3465 Loss-g-kl: 1.2915 lr: 0.0002 grad_norm_g: 499.1637 grad_norm_d: 51.4441
======> Epoch: 1080
Train Epoch: 1081 [76.92%] G-Loss: 30.3925 D-Loss: 2.3402 Loss-g-fm: 7.4000 Loss-g-mel: 17.7297 Loss-g-dur: 1.3399 Loss-g-kl: 1.3340 lr: 0.0002 grad_norm_g: 331.4752 grad_norm_d: 33.9786
======> Epoch: 1081
Train Epoch: 1082 [73.08%] G-Loss: 30.9015 D-Loss: 2.3023 Loss-g-fm: 7.8781 Loss-g-mel: 18.0186 Loss-g-dur: 1.3460 Loss-g-kl: 1.2182 lr: 0.0002 grad_norm_g: 408.1165 grad_norm_d: 35.2046
======> Epoch: 1082
Train Epoch: 1083 [69.23%] G-Loss: 31.1020 D-Loss: 2.3133 Loss-g-fm: 7.7689 Loss-g-mel: 17.9812 Loss-g-dur: 1.3261 Loss-g-kl: 1.3895 lr: 0.0002 grad_norm_g: 637.6376 grad_norm_d: 46.1858
======> Epoch: 1083
Train Epoch: 1084 [65.38%] G-Loss: 32.2186 D-Loss: 2.2461 Loss-g-fm: 8.4490 Loss-g-mel: 18.5417 Loss-g-dur: 1.3384 Loss-g-kl: 1.2781 lr: 0.0002 grad_norm_g: 823.2247 grad_norm_d: 48.0806
======> Epoch: 1084
Train Epoch: 1085 [61.54%] G-Loss: 32.7997 D-Loss: 2.1752 Loss-g-fm: 8.4824 Loss-g-mel: 18.8653 Loss-g-dur: 1.4462 Loss-g-kl: 1.3763 lr: 0.0002 grad_norm_g: 848.7134 grad_norm_d: 44.4332
======> Epoch: 1085
Train Epoch: 1086 [57.69%] G-Loss: 31.2768 D-Loss: 2.3054 Loss-g-fm: 8.2572 Loss-g-mel: 17.8453 Loss-g-dur: 1.3425 Loss-g-kl: 1.1472 lr: 0.0002 grad_norm_g: 899.0602 grad_norm_d: 91.4323
======> Epoch: 1086
Train Epoch: 1087 [53.85%] G-Loss: 28.1309 D-Loss: 2.3047 Loss-g-fm: 8.2586 Loss-g-mel: 14.8319 Loss-g-dur: 1.1690 Loss-g-kl: 1.3133 lr: 0.0002 grad_norm_g: 896.3550 grad_norm_d: 62.2373
======> Epoch: 1087
Train Epoch: 1088 [50.00%] G-Loss: 31.5258 D-Loss: 2.4790 Loss-g-fm: 7.7313 Loss-g-mel: 18.2915 Loss-g-dur: 1.4326 Loss-g-kl: 1.5486 lr: 0.0002 grad_norm_g: 571.6749 grad_norm_d: 33.2761
======> Epoch: 1088
Train Epoch: 1089 [46.15%] G-Loss: 33.1293 D-Loss: 2.1608 Loss-g-fm: 9.0614 Loss-g-mel: 18.5478 Loss-g-dur: 1.4506 Loss-g-kl: 1.3175 lr: 0.0002 grad_norm_g: 961.5811 grad_norm_d: 59.8291
======> Epoch: 1089
Train Epoch: 1090 [42.31%] G-Loss: 31.1902 D-Loss: 2.3331 Loss-g-fm: 8.3783 Loss-g-mel: 17.8063 Loss-g-dur: 1.3288 Loss-g-kl: 1.0509 lr: 0.0002 grad_norm_g: 670.3029 grad_norm_d: 64.3783
======> Epoch: 1090
Train Epoch: 1091 [38.46%] G-Loss: 32.9658 D-Loss: 2.3954 Loss-g-fm: 8.7804 Loss-g-mel: 18.8093 Loss-g-dur: 1.3398 Loss-g-kl: 1.4636 lr: 0.0002 grad_norm_g: 673.4212 grad_norm_d: 82.2394
======> Epoch: 1091
Train Epoch: 1092 [34.62%] G-Loss: 34.5805 D-Loss: 2.2111 Loss-g-fm: 9.3193 Loss-g-mel: 19.6015 Loss-g-dur: 1.4276 Loss-g-kl: 1.3744 lr: 0.0002 grad_norm_g: 534.0440 grad_norm_d: 37.0688
======> Epoch: 1092
Train Epoch: 1093 [30.77%] G-Loss: 32.7751 D-Loss: 2.2250 Loss-g-fm: 8.3213 Loss-g-mel: 18.5194 Loss-g-dur: 1.5463 Loss-g-kl: 1.5955 lr: 0.0002 grad_norm_g: 292.6283 grad_norm_d: 12.8630
======> Epoch: 1093
Train Epoch: 1094 [26.92%] G-Loss: 30.5176 D-Loss: 2.3954 Loss-g-fm: 7.7009 Loss-g-mel: 17.7884 Loss-g-dur: 1.4232 Loss-g-kl: 1.3288 lr: 0.0002 grad_norm_g: 114.0384 grad_norm_d: 22.7981
======> Epoch: 1094
Train Epoch: 1095 [23.08%] G-Loss: 23.7335 D-Loss: 1.9730 Loss-g-fm: 8.4962 Loss-g-mel: 10.0229 Loss-g-dur: 0.9409 Loss-g-kl: 1.4225 lr: 0.0002 grad_norm_g: 1221.5089 grad_norm_d: 45.8174
======> Epoch: 1095
Train Epoch: 1096 [19.23%] G-Loss: 28.9917 D-Loss: 2.2814 Loss-g-fm: 6.8700 Loss-g-mel: 17.2403 Loss-g-dur: 1.3396 Loss-g-kl: 1.1553 lr: 0.0002 grad_norm_g: 410.0927 grad_norm_d: 57.1591
======> Epoch: 1096
Train Epoch: 1097 [15.38%] G-Loss: 30.1710 D-Loss: 2.3652 Loss-g-fm: 7.2641 Loss-g-mel: 17.8297 Loss-g-dur: 1.3261 Loss-g-kl: 1.2516 lr: 0.0002 grad_norm_g: 689.3789 grad_norm_d: 72.2212
======> Epoch: 1097
Train Epoch: 1098 [11.54%] G-Loss: 33.6899 D-Loss: 2.3014 Loss-g-fm: 8.8918 Loss-g-mel: 19.3767 Loss-g-dur: 1.4660 Loss-g-kl: 1.2863 lr: 0.0002 grad_norm_g: 661.2346 grad_norm_d: 42.9795
======> Epoch: 1098
Train Epoch: 1099 [7.69%] G-Loss: 32.8338 D-Loss: 2.1922 Loss-g-fm: 8.2309 Loss-g-mel: 19.4151 Loss-g-dur: 1.3022 Loss-g-kl: 1.2905 lr: 0.0002 grad_norm_g: 531.3009 grad_norm_d: 24.0507
======> Epoch: 1099
Train Epoch: 1100 [3.85%] G-Loss: 31.3005 D-Loss: 2.4459 Loss-g-fm: 7.5310 Loss-g-mel: 18.1401 Loss-g-dur: 1.4450 Loss-g-kl: 1.3488 lr: 0.0002 grad_norm_g: 455.5724 grad_norm_d: 39.2229
======> Epoch: 1100
Train Epoch: 1101 [0.00%] G-Loss: 31.4759 D-Loss: 2.5611 Loss-g-fm: 7.8057 Loss-g-mel: 18.3373 Loss-g-dur: 1.3291 Loss-g-kl: 1.1936 lr: 0.0002 grad_norm_g: 452.6196 grad_norm_d: 98.2724
Train Epoch: 1101 [96.15%] G-Loss: 31.9693 D-Loss: 2.2657 Loss-g-fm: 8.5450 Loss-g-mel: 18.0525 Loss-g-dur: 1.3510 Loss-g-kl: 1.2869 lr: 0.0002 grad_norm_g: 772.6935 grad_norm_d: 54.5453
======> Epoch: 1101
Train Epoch: 1102 [92.31%] G-Loss: 31.9574 D-Loss: 2.3631 Loss-g-fm: 8.3851 Loss-g-mel: 18.0960 Loss-g-dur: 1.3753 Loss-g-kl: 1.1850 lr: 0.0002 grad_norm_g: 903.8806 grad_norm_d: 26.0056
======> Epoch: 1102
Train Epoch: 1103 [88.46%] G-Loss: 30.7388 D-Loss: 2.2508 Loss-g-fm: 7.8957 Loss-g-mel: 17.8817 Loss-g-dur: 1.3590 Loss-g-kl: 1.1555 lr: 0.0002 grad_norm_g: 534.0881 grad_norm_d: 18.9354
======> Epoch: 1103
Train Epoch: 1104 [84.62%] G-Loss: 32.7005 D-Loss: 2.2610 Loss-g-fm: 8.4888 Loss-g-mel: 19.1687 Loss-g-dur: 1.3524 Loss-g-kl: 1.2180 lr: 0.0002 grad_norm_g: 749.1089 grad_norm_d: 78.8065
======> Epoch: 1104
Train Epoch: 1105 [80.77%] G-Loss: 33.1674 D-Loss: 2.2102 Loss-g-fm: 8.8484 Loss-g-mel: 18.9098 Loss-g-dur: 1.4455 Loss-g-kl: 1.3436 lr: 0.0002 grad_norm_g: 958.6124 grad_norm_d: 82.9409
======> Epoch: 1105
Train Epoch: 1106 [76.92%] G-Loss: 30.6356 D-Loss: 2.3090 Loss-g-fm: 7.8446 Loss-g-mel: 17.8249 Loss-g-dur: 1.3288 Loss-g-kl: 1.3234 lr: 0.0002 grad_norm_g: 838.7600 grad_norm_d: 87.5802
======> Epoch: 1106
Train Epoch: 1107 [73.08%] G-Loss: 29.9387 D-Loss: 2.4151 Loss-g-fm: 7.4779 Loss-g-mel: 17.8033 Loss-g-dur: 1.3114 Loss-g-kl: 1.2663 lr: 0.0002 grad_norm_g: 470.0371 grad_norm_d: 45.7524
======> Epoch: 1107
Train Epoch: 1108 [69.23%] G-Loss: 30.7707 D-Loss: 2.4203 Loss-g-fm: 7.5202 Loss-g-mel: 17.8171 Loss-g-dur: 1.3510 Loss-g-kl: 1.3223 lr: 0.0002 grad_norm_g: 522.1312 grad_norm_d: 69.7762
======> Epoch: 1108
Train Epoch: 1109 [65.38%] G-Loss: 30.9482 D-Loss: 2.2935 Loss-g-fm: 8.1082 Loss-g-mel: 17.8170 Loss-g-dur: 1.3560 Loss-g-kl: 1.0424 lr: 0.0002 grad_norm_g: 811.7654 grad_norm_d: 71.1113
======> Epoch: 1109
Train Epoch: 1110 [61.54%] G-Loss: 29.7602 D-Loss: 2.3815 Loss-g-fm: 7.1772 Loss-g-mel: 17.4276 Loss-g-dur: 1.3599 Loss-g-kl: 1.3691 lr: 0.0002 grad_norm_g: 309.1402 grad_norm_d: 4.6050
======> Epoch: 1110
Train Epoch: 1111 [57.69%] G-Loss: 33.7318 D-Loss: 2.2650 Loss-g-fm: 9.1392 Loss-g-mel: 19.3593 Loss-g-dur: 1.3988 Loss-g-kl: 1.3933 lr: 0.0002 grad_norm_g: 484.9709 grad_norm_d: 29.8199
======> Epoch: 1111
Train Epoch: 1112 [53.85%] G-Loss: 31.1703 D-Loss: 2.1057 Loss-g-fm: 8.3705 Loss-g-mel: 17.7823 Loss-g-dur: 1.2916 Loss-g-kl: 1.1470 lr: 0.0002 grad_norm_g: 751.8966 grad_norm_d: 63.3912
======> Epoch: 1112
Train Epoch: 1113 [50.00%] G-Loss: 30.4920 D-Loss: 2.3121 Loss-g-fm: 7.9592 Loss-g-mel: 17.4016 Loss-g-dur: 1.3041 Loss-g-kl: 1.1869 lr: 0.0002 grad_norm_g: 449.6166 grad_norm_d: 43.3230
======> Epoch: 1113
Train Epoch: 1114 [46.15%] G-Loss: 29.4833 D-Loss: 2.3271 Loss-g-fm: 6.9457 Loss-g-mel: 17.5442 Loss-g-dur: 1.3257 Loss-g-kl: 1.1654 lr: 0.0002 grad_norm_g: 660.4227 grad_norm_d: 78.4873
======> Epoch: 1114
Train Epoch: 1115 [42.31%] G-Loss: 32.7249 D-Loss: 2.3254 Loss-g-fm: 8.2930 Loss-g-mel: 18.9079 Loss-g-dur: 1.4989 Loss-g-kl: 1.4444 lr: 0.0002 grad_norm_g: 869.7603 grad_norm_d: 53.8404
======> Epoch: 1115
Train Epoch: 1116 [38.46%] G-Loss: 31.4889 D-Loss: 2.2933 Loss-g-fm: 8.1795 Loss-g-mel: 18.1247 Loss-g-dur: 1.4472 Loss-g-kl: 1.4385 lr: 0.0002 grad_norm_g: 73.2396 grad_norm_d: 6.4871
======> Epoch: 1116
Train Epoch: 1117 [34.62%] G-Loss: 34.2043 D-Loss: 2.3427 Loss-g-fm: 9.6342 Loss-g-mel: 18.9357 Loss-g-dur: 1.4358 Loss-g-kl: 1.4888 lr: 0.0002 grad_norm_g: 983.4869 grad_norm_d: 36.4178
======> Epoch: 1117
Train Epoch: 1118 [30.77%] G-Loss: 31.5811 D-Loss: 2.3544 Loss-g-fm: 7.7179 Loss-g-mel: 18.1912 Loss-g-dur: 1.4802 Loss-g-kl: 1.6148 lr: 0.0002 grad_norm_g: 849.1504 grad_norm_d: 109.1775
======> Epoch: 1118
Train Epoch: 1119 [26.92%] G-Loss: 31.7037 D-Loss: 2.3334 Loss-g-fm: 8.1886 Loss-g-mel: 18.4726 Loss-g-dur: 1.3364 Loss-g-kl: 1.3873 lr: 0.0002 grad_norm_g: 605.9062 grad_norm_d: 29.9283
======> Epoch: 1119
Train Epoch: 1120 [23.08%] G-Loss: 31.9547 D-Loss: 2.2837 Loss-g-fm: 8.4296 Loss-g-mel: 18.1992 Loss-g-dur: 1.4146 Loss-g-kl: 1.3900 lr: 0.0002 grad_norm_g: 733.0935 grad_norm_d: 38.4431
======> Epoch: 1120
Train Epoch: 1121 [19.23%] G-Loss: 31.3106 D-Loss: 2.3487 Loss-g-fm: 7.9956 Loss-g-mel: 18.1864 Loss-g-dur: 1.3163 Loss-g-kl: 1.2959 lr: 0.0002 grad_norm_g: 646.1198 grad_norm_d: 38.1554
======> Epoch: 1121
Train Epoch: 1122 [15.38%] G-Loss: 33.2830 D-Loss: 2.4519 Loss-g-fm: 8.7521 Loss-g-mel: 18.7242 Loss-g-dur: 1.3889 Loss-g-kl: 1.3541 lr: 0.0002 grad_norm_g: 740.1619 grad_norm_d: 82.3421
======> Epoch: 1122
Train Epoch: 1123 [11.54%] G-Loss: 22.3064 D-Loss: 2.3560 Loss-g-fm: 7.2733 Loss-g-mel: 10.4807 Loss-g-dur: 0.8760 Loss-g-kl: 1.3399 lr: 0.0002 grad_norm_g: 264.9937 grad_norm_d: 56.1019
======> Epoch: 1123
Train Epoch: 1124 [7.69%] G-Loss: 30.9601 D-Loss: 2.3260 Loss-g-fm: 7.8771 Loss-g-mel: 18.0306 Loss-g-dur: 1.3631 Loss-g-kl: 1.2442 lr: 0.0002 grad_norm_g: 627.4431 grad_norm_d: 61.4440
======> Epoch: 1124
Train Epoch: 1125 [3.85%] G-Loss: 31.6877 D-Loss: 2.3082 Loss-g-fm: 8.5851 Loss-g-mel: 17.8899 Loss-g-dur: 1.3346 Loss-g-kl: 1.4114 lr: 0.0002 grad_norm_g: 607.9705 grad_norm_d: 49.9046
======> Epoch: 1125
Train Epoch: 1126 [0.00%] G-Loss: 33.0993 D-Loss: 2.1870 Loss-g-fm: 9.2562 Loss-g-mel: 18.2776 Loss-g-dur: 1.3134 Loss-g-kl: 1.2909 lr: 0.0002 grad_norm_g: 888.4684 grad_norm_d: 49.5646
Train Epoch: 1126 [96.15%] G-Loss: 30.6798 D-Loss: 2.4235 Loss-g-fm: 7.7379 Loss-g-mel: 17.6980 Loss-g-dur: 1.2979 Loss-g-kl: 1.2968 lr: 0.0002 grad_norm_g: 551.4297 grad_norm_d: 112.2185
======> Epoch: 1126
Train Epoch: 1127 [92.31%] G-Loss: 29.9366 D-Loss: 2.3678 Loss-g-fm: 7.5272 Loss-g-mel: 17.4450 Loss-g-dur: 1.3284 Loss-g-kl: 1.1522 lr: 0.0002 grad_norm_g: 416.6632 grad_norm_d: 61.9027
======> Epoch: 1127
Train Epoch: 1128 [88.46%] G-Loss: 33.6357 D-Loss: 2.1624 Loss-g-fm: 9.1609 Loss-g-mel: 18.6255 Loss-g-dur: 1.5335 Loss-g-kl: 1.5215 lr: 0.0002 grad_norm_g: 893.7720 grad_norm_d: 41.5756
======> Epoch: 1128
Train Epoch: 1129 [84.62%] G-Loss: 30.9865 D-Loss: 2.4059 Loss-g-fm: 7.8984 Loss-g-mel: 18.1373 Loss-g-dur: 1.3482 Loss-g-kl: 1.0996 lr: 0.0002 grad_norm_g: 130.8787 grad_norm_d: 43.3278
======> Epoch: 1129
Train Epoch: 1130 [80.77%] G-Loss: 34.6508 D-Loss: 2.1586 Loss-g-fm: 9.9286 Loss-g-mel: 19.4319 Loss-g-dur: 1.4138 Loss-g-kl: 1.3295 lr: 0.0002 grad_norm_g: 883.2283 grad_norm_d: 67.7384
======> Epoch: 1130
Train Epoch: 1131 [76.92%] G-Loss: 33.5411 D-Loss: 2.1642 Loss-g-fm: 9.1164 Loss-g-mel: 18.9435 Loss-g-dur: 1.4282 Loss-g-kl: 1.4239 lr: 0.0002 grad_norm_g: 355.8794 grad_norm_d: 68.8979
======> Epoch: 1131
Train Epoch: 1132 [73.08%] G-Loss: 31.0621 D-Loss: 2.3839 Loss-g-fm: 7.8268 Loss-g-mel: 18.1866 Loss-g-dur: 1.5050 Loss-g-kl: 1.4100 lr: 0.0002 grad_norm_g: 331.5230 grad_norm_d: 11.2390
======> Epoch: 1132
Train Epoch: 1133 [69.23%] G-Loss: 21.9567 D-Loss: 2.3005 Loss-g-fm: 7.1035 Loss-g-mel: 9.8526 Loss-g-dur: 0.7959 Loss-g-kl: 1.2509 lr: 0.0002 grad_norm_g: 911.9768 grad_norm_d: 42.3348
======> Epoch: 1133
Train Epoch: 1134 [65.38%] G-Loss: 31.8319 D-Loss: 2.2230 Loss-g-fm: 7.9563 Loss-g-mel: 18.0673 Loss-g-dur: 1.3693 Loss-g-kl: 1.4682 lr: 0.0002 grad_norm_g: 522.8649 grad_norm_d: 32.7021
======> Epoch: 1134
Train Epoch: 1135 [61.54%] G-Loss: 31.9871 D-Loss: 2.3994 Loss-g-fm: 8.0121 Loss-g-mel: 18.2529 Loss-g-dur: 1.4515 Loss-g-kl: 1.3307 lr: 0.0002 grad_norm_g: 858.4151 grad_norm_d: 30.4416
======> Epoch: 1135
Train Epoch: 1136 [57.69%] G-Loss: 32.7952 D-Loss: 2.2175 Loss-g-fm: 8.7058 Loss-g-mel: 18.6179 Loss-g-dur: 1.3874 Loss-g-kl: 1.3601 lr: 0.0002 grad_norm_g: 909.1109 grad_norm_d: 70.9365
======> Epoch: 1136
Train Epoch: 1137 [53.85%] G-Loss: 31.3675 D-Loss: 2.3809 Loss-g-fm: 8.1312 Loss-g-mel: 18.2994 Loss-g-dur: 1.2915 Loss-g-kl: 1.1712 lr: 0.0002 grad_norm_g: 335.1457 grad_norm_d: 37.7447
======> Epoch: 1137
Train Epoch: 1138 [50.00%] G-Loss: 21.6197 D-Loss: 2.4536 Loss-g-fm: 6.5955 Loss-g-mel: 10.2756 Loss-g-dur: 0.8796 Loss-g-kl: 1.4183 lr: 0.0002 grad_norm_g: 492.9145 grad_norm_d: 26.4227
======> Epoch: 1138
Train Epoch: 1139 [46.15%] G-Loss: 22.9936 D-Loss: 2.3064 Loss-g-fm: 7.4632 Loss-g-mel: 10.7077 Loss-g-dur: 0.8330 Loss-g-kl: 1.3064 lr: 0.0002 grad_norm_g: 864.4665 grad_norm_d: 31.4589
======> Epoch: 1139
Train Epoch: 1140 [42.31%] G-Loss: 31.5641 D-Loss: 2.3473 Loss-g-fm: 8.5194 Loss-g-mel: 18.1448 Loss-g-dur: 1.3745 Loss-g-kl: 1.1526 lr: 0.0002 grad_norm_g: 509.8960 grad_norm_d: 14.9122
======> Epoch: 1140
Train Epoch: 1141 [38.46%] G-Loss: 32.4311 D-Loss: 2.2278 Loss-g-fm: 8.5541 Loss-g-mel: 18.1866 Loss-g-dur: 1.3214 Loss-g-kl: 1.6381 lr: 0.0002 grad_norm_g: 913.3082 grad_norm_d: 53.9048
======> Epoch: 1141
Train Epoch: 1142 [34.62%] G-Loss: 22.5037 D-Loss: 2.3447 Loss-g-fm: 6.8878 Loss-g-mel: 10.6380 Loss-g-dur: 0.8569 Loss-g-kl: 1.4483 lr: 0.0002 grad_norm_g: 655.5207 grad_norm_d: 20.7829
======> Epoch: 1142
Train Epoch: 1143 [30.77%] G-Loss: 30.4322 D-Loss: 2.2793 Loss-g-fm: 7.9265 Loss-g-mel: 17.5626 Loss-g-dur: 1.3132 Loss-g-kl: 1.1947 lr: 0.0002 grad_norm_g: 760.1030 grad_norm_d: 45.8856
======> Epoch: 1143
Train Epoch: 1144 [26.92%] G-Loss: 32.0301 D-Loss: 2.1872 Loss-g-fm: 8.9749 Loss-g-mel: 17.8835 Loss-g-dur: 1.2948 Loss-g-kl: 1.2107 lr: 0.0002 grad_norm_g: 731.3054 grad_norm_d: 67.4579
======> Epoch: 1144
Train Epoch: 1145 [23.08%] G-Loss: 30.1686 D-Loss: 2.3422 Loss-g-fm: 7.7645 Loss-g-mel: 17.5598 Loss-g-dur: 1.2788 Loss-g-kl: 1.0842 lr: 0.0002 grad_norm_g: 622.9264 grad_norm_d: 32.6261
======> Epoch: 1145
Train Epoch: 1146 [19.23%] G-Loss: 25.8786 D-Loss: 2.2577 Loss-g-fm: 7.8097 Loss-g-mel: 13.1478 Loss-g-dur: 0.9256 Loss-g-kl: 1.5774 lr: 0.0002 grad_norm_g: 884.4410 grad_norm_d: 41.7475
======> Epoch: 1146
Train Epoch: 1147 [15.38%] G-Loss: 32.2610 D-Loss: 2.3220 Loss-g-fm: 8.2159 Loss-g-mel: 18.6350 Loss-g-dur: 1.4314 Loss-g-kl: 1.3700 lr: 0.0002 grad_norm_g: 783.5709 grad_norm_d: 76.7613
======> Epoch: 1147
Train Epoch: 1148 [11.54%] G-Loss: 30.3579 D-Loss: 2.3257 Loss-g-fm: 7.5113 Loss-g-mel: 17.9061 Loss-g-dur: 1.3650 Loss-g-kl: 1.2188 lr: 0.0002 grad_norm_g: 581.6993 grad_norm_d: 35.4465
======> Epoch: 1148
Train Epoch: 1149 [7.69%] G-Loss: 31.0558 D-Loss: 2.2656 Loss-g-fm: 7.9497 Loss-g-mel: 17.6241 Loss-g-dur: 1.2612 Loss-g-kl: 1.3205 lr: 0.0002 grad_norm_g: 825.2267 grad_norm_d: 111.6688
======> Epoch: 1149
Train Epoch: 1150 [3.85%] G-Loss: 31.9411 D-Loss: 2.3918 Loss-g-fm: 8.1419 Loss-g-mel: 18.3123 Loss-g-dur: 1.3904 Loss-g-kl: 1.4682 lr: 0.0002 grad_norm_g: 638.0024 grad_norm_d: 122.6472
======> Epoch: 1150
Train Epoch: 1151 [0.00%] G-Loss: 29.8834 D-Loss: 2.3595 Loss-g-fm: 7.4320 Loss-g-mel: 17.4103 Loss-g-dur: 1.3077 Loss-g-kl: 1.3620 lr: 0.0002 grad_norm_g: 351.1479 grad_norm_d: 21.7433
Train Epoch: 1151 [96.15%] G-Loss: 31.9940 D-Loss: 2.3465 Loss-g-fm: 8.1884 Loss-g-mel: 18.4601 Loss-g-dur: 1.3943 Loss-g-kl: 1.4524 lr: 0.0002 grad_norm_g: 614.7733 grad_norm_d: 59.1472
======> Epoch: 1151
Train Epoch: 1152 [92.31%] G-Loss: 32.6387 D-Loss: 2.2514 Loss-g-fm: 8.7996 Loss-g-mel: 18.7971 Loss-g-dur: 1.3544 Loss-g-kl: 1.2168 lr: 0.0002 grad_norm_g: 396.8963 grad_norm_d: 21.5493
======> Epoch: 1152
Train Epoch: 1153 [88.46%] G-Loss: 22.5118 D-Loss: 2.2611 Loss-g-fm: 6.9432 Loss-g-mel: 10.8474 Loss-g-dur: 0.8411 Loss-g-kl: 1.3863 lr: 0.0002 grad_norm_g: 699.0220 grad_norm_d: 28.8400
======> Epoch: 1153
Train Epoch: 1154 [84.62%] G-Loss: 30.8143 D-Loss: 2.3302 Loss-g-fm: 8.0665 Loss-g-mel: 17.6202 Loss-g-dur: 1.3365 Loss-g-kl: 1.3867 lr: 0.0002 grad_norm_g: 321.1097 grad_norm_d: 14.8309
Saving model and optimizer state at iteration 1154 to /ZFS4T/tts/data/VITS/model_saved/G_30000.pth
Saving model and optimizer state at iteration 1154 to /ZFS4T/tts/data/VITS/model_saved/D_30000.pth
======> Epoch: 1154
Train Epoch: 1155 [80.77%] G-Loss: 31.1542 D-Loss: 2.2678 Loss-g-fm: 8.2831 Loss-g-mel: 18.0114 Loss-g-dur: 1.3038 Loss-g-kl: 1.3688 lr: 0.0002 grad_norm_g: 199.5574 grad_norm_d: 10.9982
======> Epoch: 1155
Train Epoch: 1156 [76.92%] G-Loss: 32.3649 D-Loss: 2.2632 Loss-g-fm: 8.8839 Loss-g-mel: 17.8162 Loss-g-dur: 1.3826 Loss-g-kl: 1.6626 lr: 0.0002 grad_norm_g: 823.4526 grad_norm_d: 53.2987
======> Epoch: 1156
Train Epoch: 1157 [73.08%] G-Loss: 33.0128 D-Loss: 2.5259 Loss-g-fm: 8.6720 Loss-g-mel: 18.8788 Loss-g-dur: 1.6347 Loss-g-kl: 1.6221 lr: 0.0002 grad_norm_g: 679.1229 grad_norm_d: 52.0565
======> Epoch: 1157
Train Epoch: 1158 [69.23%] G-Loss: 32.5172 D-Loss: 2.3495 Loss-g-fm: 8.5246 Loss-g-mel: 18.4599 Loss-g-dur: 1.5087 Loss-g-kl: 1.4646 lr: 0.0002 grad_norm_g: 759.7425 grad_norm_d: 80.9882
======> Epoch: 1158
Train Epoch: 1159 [65.38%] G-Loss: 31.2726 D-Loss: 2.3697 Loss-g-fm: 8.1225 Loss-g-mel: 17.9777 Loss-g-dur: 1.3100 Loss-g-kl: 1.2972 lr: 0.0002 grad_norm_g: 415.5971 grad_norm_d: 87.3378
======> Epoch: 1159
Train Epoch: 1160 [61.54%] G-Loss: 31.0393 D-Loss: 2.2376 Loss-g-fm: 7.9285 Loss-g-mel: 18.0401 Loss-g-dur: 1.3231 Loss-g-kl: 1.2089 lr: 0.0002 grad_norm_g: 134.4864 grad_norm_d: 40.5962
======> Epoch: 1160
Train Epoch: 1161 [57.69%] G-Loss: 33.5697 D-Loss: 2.4044 Loss-g-fm: 8.6304 Loss-g-mel: 19.3997 Loss-g-dur: 1.4880 Loss-g-kl: 1.4872 lr: 0.0002 grad_norm_g: 85.9092 grad_norm_d: 11.2771
======> Epoch: 1161
Train Epoch: 1162 [53.85%] G-Loss: 32.5681 D-Loss: 2.2503 Loss-g-fm: 8.3438 Loss-g-mel: 18.6322 Loss-g-dur: 1.3749 Loss-g-kl: 1.2429 lr: 0.0002 grad_norm_g: 567.8599 grad_norm_d: 42.7775
======> Epoch: 1162
Train Epoch: 1163 [50.00%] G-Loss: 32.7975 D-Loss: 2.2210 Loss-g-fm: 8.9767 Loss-g-mel: 18.6168 Loss-g-dur: 1.4608 Loss-g-kl: 1.4752 lr: 0.0002 grad_norm_g: 533.8318 grad_norm_d: 42.2392
======> Epoch: 1163
Train Epoch: 1164 [46.15%] G-Loss: 31.6225 D-Loss: 2.2462 Loss-g-fm: 8.4962 Loss-g-mel: 18.1506 Loss-g-dur: 1.2957 Loss-g-kl: 1.2933 lr: 0.0002 grad_norm_g: 260.6751 grad_norm_d: 8.2857
======> Epoch: 1164
Train Epoch: 1165 [42.31%] G-Loss: 32.1111 D-Loss: 2.3736 Loss-g-fm: 8.1061 Loss-g-mel: 18.3186 Loss-g-dur: 1.3834 Loss-g-kl: 1.6001 lr: 0.0002 grad_norm_g: 686.1701 grad_norm_d: 49.7367
======> Epoch: 1165
Train Epoch: 1166 [38.46%] G-Loss: 35.0319 D-Loss: 2.4410 Loss-g-fm: 9.2892 Loss-g-mel: 19.5496 Loss-g-dur: 1.7065 Loss-g-kl: 1.6034 lr: 0.0002 grad_norm_g: 320.0656 grad_norm_d: 61.8937
======> Epoch: 1166
Train Epoch: 1167 [34.62%] G-Loss: 30.1222 D-Loss: 2.2683 Loss-g-fm: 7.6621 Loss-g-mel: 17.4065 Loss-g-dur: 1.3074 Loss-g-kl: 1.2339 lr: 0.0002 grad_norm_g: 720.6032 grad_norm_d: 68.2167
======> Epoch: 1167
Train Epoch: 1168 [30.77%] G-Loss: 34.4840 D-Loss: 2.1238 Loss-g-fm: 10.0155 Loss-g-mel: 18.7979 Loss-g-dur: 1.4200 Loss-g-kl: 1.4598 lr: 0.0002 grad_norm_g: 455.5118 grad_norm_d: 19.0725
======> Epoch: 1168
Train Epoch: 1169 [26.92%] G-Loss: 32.7109 D-Loss: 2.4532 Loss-g-fm: 8.6657 Loss-g-mel: 18.1825 Loss-g-dur: 1.3401 Loss-g-kl: 1.4574 lr: 0.0002 grad_norm_g: 861.8287 grad_norm_d: 90.0865
======> Epoch: 1169
Train Epoch: 1170 [23.08%] G-Loss: 35.7133 D-Loss: 2.1580 Loss-g-fm: 11.1751 Loss-g-mel: 18.8803 Loss-g-dur: 1.4521 Loss-g-kl: 1.5013 lr: 0.0002 grad_norm_g: 1033.5477 grad_norm_d: 44.6402
======> Epoch: 1170
Train Epoch: 1171 [19.23%] G-Loss: 31.6004 D-Loss: 2.2745 Loss-g-fm: 8.3894 Loss-g-mel: 18.0868 Loss-g-dur: 1.3338 Loss-g-kl: 1.3461 lr: 0.0002 grad_norm_g: 701.4136 grad_norm_d: 50.5962
======> Epoch: 1171
Train Epoch: 1172 [15.38%] G-Loss: 32.4714 D-Loss: 2.2273 Loss-g-fm: 8.6594 Loss-g-mel: 18.2600 Loss-g-dur: 1.4184 Loss-g-kl: 1.4286 lr: 0.0002 grad_norm_g: 489.8719 grad_norm_d: 14.3626
======> Epoch: 1172
Train Epoch: 1173 [11.54%] G-Loss: 30.7768 D-Loss: 2.1843 Loss-g-fm: 7.9602 Loss-g-mel: 17.6800 Loss-g-dur: 1.2799 Loss-g-kl: 1.2839 lr: 0.0002 grad_norm_g: 75.2663 grad_norm_d: 32.5594
======> Epoch: 1173
Train Epoch: 1174 [7.69%] G-Loss: 33.2247 D-Loss: 2.1755 Loss-g-fm: 9.6784 Loss-g-mel: 18.1648 Loss-g-dur: 1.4094 Loss-g-kl: 1.4469 lr: 0.0002 grad_norm_g: 925.5506 grad_norm_d: 41.0458
======> Epoch: 1174
Train Epoch: 1175 [3.85%] G-Loss: 23.0107 D-Loss: 2.3344 Loss-g-fm: 7.2815 Loss-g-mel: 10.7095 Loss-g-dur: 0.8561 Loss-g-kl: 1.2990 lr: 0.0002 grad_norm_g: 1258.8175 grad_norm_d: 47.3592
======> Epoch: 1175
Train Epoch: 1176 [0.00%] G-Loss: 21.4557 D-Loss: 2.3706 Loss-g-fm: 6.0332 Loss-g-mel: 10.8504 Loss-g-dur: 0.8012 Loss-g-kl: 1.3930 lr: 0.0002 grad_norm_g: 361.2393 grad_norm_d: 6.6392
Train Epoch: 1176 [96.15%] G-Loss: 31.7112 D-Loss: 2.3213 Loss-g-fm: 8.4446 Loss-g-mel: 17.7821 Loss-g-dur: 1.3654 Loss-g-kl: 1.3700 lr: 0.0002 grad_norm_g: 956.8417 grad_norm_d: 89.9166
======> Epoch: 1176
Train Epoch: 1177 [92.31%] G-Loss: 32.2149 D-Loss: 2.2802 Loss-g-fm: 8.2726 Loss-g-mel: 18.5380 Loss-g-dur: 1.3071 Loss-g-kl: 1.4467 lr: 0.0002 grad_norm_g: 859.8042 grad_norm_d: 91.0899
======> Epoch: 1177
Train Epoch: 1178 [88.46%] G-Loss: 31.2282 D-Loss: 2.3288 Loss-g-fm: 7.8155 Loss-g-mel: 18.3270 Loss-g-dur: 1.3204 Loss-g-kl: 1.2203 lr: 0.0002 grad_norm_g: 980.3686 grad_norm_d: 91.8225
======> Epoch: 1178
Train Epoch: 1179 [84.62%] G-Loss: 32.1711 D-Loss: 2.2207 Loss-g-fm: 8.9607 Loss-g-mel: 17.8657 Loss-g-dur: 1.3034 Loss-g-kl: 1.2974 lr: 0.0002 grad_norm_g: 934.9429 grad_norm_d: 62.8567
======> Epoch: 1179
Train Epoch: 1180 [80.77%] G-Loss: 32.8763 D-Loss: 2.3534 Loss-g-fm: 8.9732 Loss-g-mel: 18.5758 Loss-g-dur: 1.4338 Loss-g-kl: 1.3412 lr: 0.0002 grad_norm_g: 594.5786 grad_norm_d: 17.5794
======> Epoch: 1180
Train Epoch: 1181 [76.92%] G-Loss: 34.3666 D-Loss: 2.2114 Loss-g-fm: 9.6669 Loss-g-mel: 19.2505 Loss-g-dur: 1.4448 Loss-g-kl: 1.4487 lr: 0.0002 grad_norm_g: 204.3190 grad_norm_d: 35.2019
======> Epoch: 1181
Train Epoch: 1182 [73.08%] G-Loss: 22.4142 D-Loss: 2.3414 Loss-g-fm: 7.3796 Loss-g-mel: 10.1483 Loss-g-dur: 0.8965 Loss-g-kl: 1.4194 lr: 0.0002 grad_norm_g: 352.4903 grad_norm_d: 23.6393
======> Epoch: 1182
Train Epoch: 1183 [69.23%] G-Loss: 28.1730 D-Loss: 2.2223 Loss-g-fm: 8.1167 Loss-g-mel: 15.0477 Loss-g-dur: 1.0992 Loss-g-kl: 1.3557 lr: 0.0002 grad_norm_g: 910.2120 grad_norm_d: 80.3611
======> Epoch: 1183
Train Epoch: 1184 [65.38%] G-Loss: 33.4596 D-Loss: 2.3567 Loss-g-fm: 9.1141 Loss-g-mel: 18.8847 Loss-g-dur: 1.4160 Loss-g-kl: 1.4690 lr: 0.0002 grad_norm_g: 363.1656 grad_norm_d: 21.5149
======> Epoch: 1184
Train Epoch: 1185 [61.54%] G-Loss: 31.7889 D-Loss: 2.2947 Loss-g-fm: 8.4816 Loss-g-mel: 18.0006 Loss-g-dur: 1.3086 Loss-g-kl: 1.1933 lr: 0.0002 grad_norm_g: 929.5724 grad_norm_d: 67.7763
======> Epoch: 1185
Train Epoch: 1186 [57.69%] G-Loss: 30.3948 D-Loss: 2.4064 Loss-g-fm: 7.7388 Loss-g-mel: 17.7804 Loss-g-dur: 1.3236 Loss-g-kl: 1.0604 lr: 0.0002 grad_norm_g: 111.2336 grad_norm_d: 22.0332
======> Epoch: 1186
Train Epoch: 1187 [53.85%] G-Loss: 33.4793 D-Loss: 2.1754 Loss-g-fm: 9.4948 Loss-g-mel: 18.7050 Loss-g-dur: 1.3201 Loss-g-kl: 1.5042 lr: 0.0002 grad_norm_g: 914.6118 grad_norm_d: 67.7594
======> Epoch: 1187
Train Epoch: 1188 [50.00%] G-Loss: 32.8859 D-Loss: 2.3401 Loss-g-fm: 9.2650 Loss-g-mel: 18.3858 Loss-g-dur: 1.2863 Loss-g-kl: 1.4066 lr: 0.0002 grad_norm_g: 91.0486 grad_norm_d: 25.9690
======> Epoch: 1188
Train Epoch: 1189 [46.15%] G-Loss: 31.9099 D-Loss: 2.0797 Loss-g-fm: 8.7666 Loss-g-mel: 17.7501 Loss-g-dur: 1.3401 Loss-g-kl: 1.3682 lr: 0.0002 grad_norm_g: 732.2574 grad_norm_d: 43.9400
======> Epoch: 1189
Train Epoch: 1190 [42.31%] G-Loss: 33.1761 D-Loss: 2.1659 Loss-g-fm: 9.4548 Loss-g-mel: 18.2640 Loss-g-dur: 1.2738 Loss-g-kl: 1.3006 lr: 0.0002 grad_norm_g: 904.8687 grad_norm_d: 67.3515
======> Epoch: 1190
Train Epoch: 1191 [38.46%] G-Loss: 33.8915 D-Loss: 2.3056 Loss-g-fm: 9.1778 Loss-g-mel: 18.9570 Loss-g-dur: 1.3585 Loss-g-kl: 1.6249 lr: 0.0002 grad_norm_g: 1060.0408 grad_norm_d: 88.6485
======> Epoch: 1191
Train Epoch: 1192 [34.62%] G-Loss: 33.7636 D-Loss: 2.3786 Loss-g-fm: 9.2687 Loss-g-mel: 19.1182 Loss-g-dur: 1.4293 Loss-g-kl: 1.2929 lr: 0.0002 grad_norm_g: 651.7750 grad_norm_d: 105.5535
======> Epoch: 1192
Train Epoch: 1193 [30.77%] G-Loss: 29.9712 D-Loss: 2.3762 Loss-g-fm: 7.4807 Loss-g-mel: 17.6571 Loss-g-dur: 1.3365 Loss-g-kl: 1.2857 lr: 0.0002 grad_norm_g: 62.8388 grad_norm_d: 27.9810
======> Epoch: 1193
Train Epoch: 1194 [26.92%] G-Loss: 31.4656 D-Loss: 2.3138 Loss-g-fm: 8.5546 Loss-g-mel: 18.1527 Loss-g-dur: 1.2874 Loss-g-kl: 1.2738 lr: 0.0002 grad_norm_g: 505.3159 grad_norm_d: 42.8450
======> Epoch: 1194
Train Epoch: 1195 [23.08%] G-Loss: 31.6724 D-Loss: 2.2964 Loss-g-fm: 8.0143 Loss-g-mel: 18.2114 Loss-g-dur: 1.3967 Loss-g-kl: 1.1361 lr: 0.0002 grad_norm_g: 841.0111 grad_norm_d: 55.3372
======> Epoch: 1195
Train Epoch: 1196 [19.23%] G-Loss: 33.0413 D-Loss: 2.1204 Loss-g-fm: 9.5624 Loss-g-mel: 18.0008 Loss-g-dur: 1.3591 Loss-g-kl: 1.5063 lr: 0.0002 grad_norm_g: 670.9491 grad_norm_d: 37.2551
======> Epoch: 1196
Train Epoch: 1197 [15.38%] G-Loss: 31.9566 D-Loss: 2.3437 Loss-g-fm: 8.9829 Loss-g-mel: 17.8367 Loss-g-dur: 1.3373 Loss-g-kl: 1.2472 lr: 0.0002 grad_norm_g: 336.1489 grad_norm_d: 25.1949
======> Epoch: 1197
Train Epoch: 1198 [11.54%] G-Loss: 31.5084 D-Loss: 2.3181 Loss-g-fm: 8.4465 Loss-g-mel: 17.6904 Loss-g-dur: 1.3372 Loss-g-kl: 1.2704 lr: 0.0002 grad_norm_g: 665.0320 grad_norm_d: 73.9748
======> Epoch: 1198
Train Epoch: 1199 [7.69%] G-Loss: 29.2695 D-Loss: 2.4854 Loss-g-fm: 7.2286 Loss-g-mel: 17.2463 Loss-g-dur: 1.2699 Loss-g-kl: 1.1257 lr: 0.0002 grad_norm_g: 285.5111 grad_norm_d: 49.9330
======> Epoch: 1199
Train Epoch: 1200 [3.85%] G-Loss: 33.1440 D-Loss: 2.4320 Loss-g-fm: 8.4702 Loss-g-mel: 18.8929 Loss-g-dur: 1.4740 Loss-g-kl: 1.5363 lr: 0.0002 grad_norm_g: 254.8373 grad_norm_d: 35.0169
======> Epoch: 1200
Train Epoch: 1201 [0.00%] G-Loss: 33.2106 D-Loss: 2.2221 Loss-g-fm: 9.1510 Loss-g-mel: 18.7214 Loss-g-dur: 1.4312 Loss-g-kl: 1.3741 lr: 0.0002 grad_norm_g: 798.2904 grad_norm_d: 52.4259
Train Epoch: 1201 [96.15%] G-Loss: 21.9877 D-Loss: 2.3444 Loss-g-fm: 6.5350 Loss-g-mel: 10.6416 Loss-g-dur: 0.8441 Loss-g-kl: 1.2831 lr: 0.0002 grad_norm_g: 540.2379 grad_norm_d: 34.2666
======> Epoch: 1201
Train Epoch: 1202 [92.31%] G-Loss: 31.3962 D-Loss: 2.1952 Loss-g-fm: 8.4700 Loss-g-mel: 17.4922 Loss-g-dur: 1.2979 Loss-g-kl: 1.3938 lr: 0.0002 grad_norm_g: 219.2581 grad_norm_d: 26.3236
======> Epoch: 1202
Train Epoch: 1203 [88.46%] G-Loss: 31.8819 D-Loss: 2.2663 Loss-g-fm: 9.0162 Loss-g-mel: 17.4878 Loss-g-dur: 1.4376 Loss-g-kl: 1.5295 lr: 0.0002 grad_norm_g: 289.4483 grad_norm_d: 11.4737
======> Epoch: 1203
Train Epoch: 1204 [84.62%] G-Loss: 33.2096 D-Loss: 2.3154 Loss-g-fm: 9.3223 Loss-g-mel: 18.6135 Loss-g-dur: 1.3903 Loss-g-kl: 1.3552 lr: 0.0002 grad_norm_g: 676.9077 grad_norm_d: 42.1712
======> Epoch: 1204
Train Epoch: 1205 [80.77%] G-Loss: 33.0421 D-Loss: 2.1767 Loss-g-fm: 9.4191 Loss-g-mel: 18.6571 Loss-g-dur: 1.2767 Loss-g-kl: 1.2171 lr: 0.0002 grad_norm_g: 831.5118 grad_norm_d: 69.4910
======> Epoch: 1205
Train Epoch: 1206 [76.92%] G-Loss: 32.3891 D-Loss: 2.2396 Loss-g-fm: 8.4184 Loss-g-mel: 18.3222 Loss-g-dur: 1.4377 Loss-g-kl: 1.6230 lr: 0.0002 grad_norm_g: 385.7381 grad_norm_d: 43.8398
======> Epoch: 1206
Train Epoch: 1207 [73.08%] G-Loss: 30.5041 D-Loss: 2.4374 Loss-g-fm: 7.8831 Loss-g-mel: 17.5148 Loss-g-dur: 1.3469 Loss-g-kl: 1.0903 lr: 0.0002 grad_norm_g: 532.7270 grad_norm_d: 122.8017
======> Epoch: 1207
Train Epoch: 1208 [69.23%] G-Loss: 31.2446 D-Loss: 2.2877 Loss-g-fm: 8.3379 Loss-g-mel: 17.8415 Loss-g-dur: 1.3308 Loss-g-kl: 1.3043 lr: 0.0002 grad_norm_g: 724.4627 grad_norm_d: 84.7285
======> Epoch: 1208
Train Epoch: 1209 [65.38%] G-Loss: 31.6217 D-Loss: 2.3595 Loss-g-fm: 8.2016 Loss-g-mel: 17.5366 Loss-g-dur: 1.4406 Loss-g-kl: 1.4939 lr: 0.0002 grad_norm_g: 341.3953 grad_norm_d: 29.7320
======> Epoch: 1209
Train Epoch: 1210 [61.54%] G-Loss: 32.2979 D-Loss: 2.1770 Loss-g-fm: 8.9234 Loss-g-mel: 18.1986 Loss-g-dur: 1.3616 Loss-g-kl: 1.2890 lr: 0.0002 grad_norm_g: 462.0466 grad_norm_d: 22.2585
======> Epoch: 1210
Train Epoch: 1211 [57.69%] G-Loss: 33.4201 D-Loss: 2.2577 Loss-g-fm: 9.5843 Loss-g-mel: 18.2797 Loss-g-dur: 1.4120 Loss-g-kl: 1.4235 lr: 0.0002 grad_norm_g: 620.6468 grad_norm_d: 31.6573
======> Epoch: 1211
Train Epoch: 1212 [53.85%] G-Loss: 33.8474 D-Loss: 2.2811 Loss-g-fm: 9.8488 Loss-g-mel: 18.5562 Loss-g-dur: 1.3879 Loss-g-kl: 1.5308 lr: 0.0002 grad_norm_g: 446.2042 grad_norm_d: 42.4607
======> Epoch: 1212
Train Epoch: 1213 [50.00%] G-Loss: 31.3235 D-Loss: 2.2892 Loss-g-fm: 8.4734 Loss-g-mel: 17.7407 Loss-g-dur: 1.2351 Loss-g-kl: 1.3183 lr: 0.0002 grad_norm_g: 515.5547 grad_norm_d: 24.8627
======> Epoch: 1213
Train Epoch: 1214 [46.15%] G-Loss: 31.1129 D-Loss: 2.2962 Loss-g-fm: 8.5682 Loss-g-mel: 17.3857 Loss-g-dur: 1.3099 Loss-g-kl: 1.2972 lr: 0.0002 grad_norm_g: 616.8853 grad_norm_d: 50.9013
======> Epoch: 1214
Train Epoch: 1215 [42.31%] G-Loss: 34.3614 D-Loss: 2.2558 Loss-g-fm: 9.5759 Loss-g-mel: 19.5057 Loss-g-dur: 1.3688 Loss-g-kl: 1.6119 lr: 0.0002 grad_norm_g: 264.1508 grad_norm_d: 37.0090
======> Epoch: 1215
Train Epoch: 1216 [38.46%] G-Loss: 32.8948 D-Loss: 2.2750 Loss-g-fm: 8.7990 Loss-g-mel: 18.5239 Loss-g-dur: 1.3553 Loss-g-kl: 1.4126 lr: 0.0002 grad_norm_g: 427.9594 grad_norm_d: 11.6563
======> Epoch: 1216
Train Epoch: 1217 [34.62%] G-Loss: 33.3168 D-Loss: 2.1982 Loss-g-fm: 8.7457 Loss-g-mel: 18.8755 Loss-g-dur: 1.4003 Loss-g-kl: 1.6081 lr: 0.0002 grad_norm_g: 543.1725 grad_norm_d: 67.5275
======> Epoch: 1217
Train Epoch: 1218 [30.77%] G-Loss: 31.1249 D-Loss: 2.3436 Loss-g-fm: 8.0727 Loss-g-mel: 18.1045 Loss-g-dur: 1.2741 Loss-g-kl: 1.1722 lr: 0.0002 grad_norm_g: 853.4590 grad_norm_d: 72.0647
======> Epoch: 1218
Train Epoch: 1219 [26.92%] G-Loss: 34.7381 D-Loss: 2.2124 Loss-g-fm: 10.1425 Loss-g-mel: 19.2004 Loss-g-dur: 1.4265 Loss-g-kl: 1.4268 lr: 0.0002 grad_norm_g: 561.9146 grad_norm_d: 24.7481
======> Epoch: 1219
Train Epoch: 1220 [23.08%] G-Loss: 29.0363 D-Loss: 2.2847 Loss-g-fm: 8.7929 Loss-g-mel: 15.1720 Loss-g-dur: 1.1061 Loss-g-kl: 1.4169 lr: 0.0002 grad_norm_g: 65.6614 grad_norm_d: 9.7925
======> Epoch: 1220
Train Epoch: 1221 [19.23%] G-Loss: 31.5304 D-Loss: 2.3845 Loss-g-fm: 8.4879 Loss-g-mel: 17.9446 Loss-g-dur: 1.4011 Loss-g-kl: 1.3208 lr: 0.0002 grad_norm_g: 370.6454 grad_norm_d: 23.7758
======> Epoch: 1221
Train Epoch: 1222 [15.38%] G-Loss: 27.7423 D-Loss: 2.2266 Loss-g-fm: 8.1520 Loss-g-mel: 14.6507 Loss-g-dur: 1.1000 Loss-g-kl: 1.1961 lr: 0.0002 grad_norm_g: 797.4323 grad_norm_d: 85.4613
======> Epoch: 1222
Train Epoch: 1223 [11.54%] G-Loss: 34.6866 D-Loss: 2.1983 Loss-g-fm: 9.7725 Loss-g-mel: 19.3592 Loss-g-dur: 1.4896 Loss-g-kl: 1.5547 lr: 0.0002 grad_norm_g: 701.3576 grad_norm_d: 47.1044
======> Epoch: 1223
Train Epoch: 1224 [7.69%] G-Loss: 31.3908 D-Loss: 2.2889 Loss-g-fm: 8.3196 Loss-g-mel: 17.8789 Loss-g-dur: 1.3806 Loss-g-kl: 1.2913 lr: 0.0002 grad_norm_g: 261.7908 grad_norm_d: 34.3903
======> Epoch: 1224
Train Epoch: 1225 [3.85%] G-Loss: 31.9885 D-Loss: 2.2836 Loss-g-fm: 8.7414 Loss-g-mel: 17.9781 Loss-g-dur: 1.3912 Loss-g-kl: 1.4448 lr: 0.0002 grad_norm_g: 550.5004 grad_norm_d: 35.6232
======> Epoch: 1225
Train Epoch: 1226 [0.00%] G-Loss: 30.7844 D-Loss: 2.3426 Loss-g-fm: 8.3584 Loss-g-mel: 17.7927 Loss-g-dur: 1.3014 Loss-g-kl: 1.1582 lr: 0.0002 grad_norm_g: 651.3264 grad_norm_d: 54.5680
Train Epoch: 1226 [96.15%] G-Loss: 32.6124 D-Loss: 2.1385 Loss-g-fm: 9.0904 Loss-g-mel: 18.0874 Loss-g-dur: 1.2969 Loss-g-kl: 1.5719 lr: 0.0002 grad_norm_g: 768.2298 grad_norm_d: 82.2579
======> Epoch: 1226
Train Epoch: 1227 [92.31%] G-Loss: 22.5366 D-Loss: 2.2891 Loss-g-fm: 7.6483 Loss-g-mel: 9.8710 Loss-g-dur: 0.8364 Loss-g-kl: 1.4336 lr: 0.0002 grad_norm_g: 492.7781 grad_norm_d: 23.2862
======> Epoch: 1227
Train Epoch: 1228 [88.46%] G-Loss: 31.1610 D-Loss: 2.2413 Loss-g-fm: 8.6382 Loss-g-mel: 17.2711 Loss-g-dur: 1.3095 Loss-g-kl: 1.2281 lr: 0.0002 grad_norm_g: 860.0284 grad_norm_d: 47.6846
======> Epoch: 1228
Train Epoch: 1229 [84.62%] G-Loss: 31.5388 D-Loss: 2.3679 Loss-g-fm: 8.6867 Loss-g-mel: 17.7396 Loss-g-dur: 1.2896 Loss-g-kl: 1.3035 lr: 0.0002 grad_norm_g: 705.8071 grad_norm_d: 54.5351
======> Epoch: 1229
Train Epoch: 1230 [80.77%] G-Loss: 33.5038 D-Loss: 2.3808 Loss-g-fm: 9.1416 Loss-g-mel: 18.8619 Loss-g-dur: 1.3565 Loss-g-kl: 1.4777 lr: 0.0002 grad_norm_g: 179.8961 grad_norm_d: 69.6084
======> Epoch: 1230
Train Epoch: 1231 [76.92%] G-Loss: 33.2482 D-Loss: 2.2927 Loss-g-fm: 9.2068 Loss-g-mel: 18.7431 Loss-g-dur: 1.5274 Loss-g-kl: 1.4568 lr: 0.0002 grad_norm_g: 445.9267 grad_norm_d: 34.0956
Saving model and optimizer state at iteration 1231 to /ZFS4T/tts/data/VITS/model_saved/G_32000.pth
Saving model and optimizer state at iteration 1231 to /ZFS4T/tts/data/VITS/model_saved/D_32000.pth
======> Epoch: 1231
Train Epoch: 1232 [73.08%] G-Loss: 33.0457 D-Loss: 2.2159 Loss-g-fm: 9.1360 Loss-g-mel: 18.8367 Loss-g-dur: 1.3283 Loss-g-kl: 1.4399 lr: 0.0002 grad_norm_g: 47.9336 grad_norm_d: 25.5304
======> Epoch: 1232
Train Epoch: 1233 [69.23%] G-Loss: 34.5739 D-Loss: 2.0251 Loss-g-fm: 10.3956 Loss-g-mel: 18.6196 Loss-g-dur: 1.2566 Loss-g-kl: 1.4160 lr: 0.0002 grad_norm_g: 819.0424 grad_norm_d: 42.9755
======> Epoch: 1233
Train Epoch: 1234 [65.38%] G-Loss: 33.6428 D-Loss: 2.3194 Loss-g-fm: 9.5763 Loss-g-mel: 18.7759 Loss-g-dur: 1.4176 Loss-g-kl: 1.4545 lr: 0.0002 grad_norm_g: 82.1690 grad_norm_d: 27.7420
======> Epoch: 1234
Train Epoch: 1235 [61.54%] G-Loss: 30.5607 D-Loss: 2.3176 Loss-g-fm: 7.8304 Loss-g-mel: 18.0302 Loss-g-dur: 1.3616 Loss-g-kl: 1.0604 lr: 0.0002 grad_norm_g: 362.1089 grad_norm_d: 58.9812
======> Epoch: 1235
Train Epoch: 1236 [57.69%] G-Loss: 32.1503 D-Loss: 2.2344 Loss-g-fm: 9.3809 Loss-g-mel: 17.8489 Loss-g-dur: 1.3239 Loss-g-kl: 1.1974 lr: 0.0002 grad_norm_g: 858.8934 grad_norm_d: 78.6322
======> Epoch: 1236
Train Epoch: 1237 [53.85%] G-Loss: 32.4309 D-Loss: 2.3970 Loss-g-fm: 8.8803 Loss-g-mel: 18.3775 Loss-g-dur: 1.3032 Loss-g-kl: 1.2551 lr: 0.0002 grad_norm_g: 738.6459 grad_norm_d: 71.6990
======> Epoch: 1237
Train Epoch: 1238 [50.00%] G-Loss: 33.7359 D-Loss: 2.2787 Loss-g-fm: 9.3205 Loss-g-mel: 19.0314 Loss-g-dur: 1.3648 Loss-g-kl: 1.4953 lr: 0.0002 grad_norm_g: 524.8652 grad_norm_d: 31.1798
======> Epoch: 1238
Train Epoch: 1239 [46.15%] G-Loss: 21.6328 D-Loss: 2.2025 Loss-g-fm: 7.3800 Loss-g-mel: 9.6071 Loss-g-dur: 0.7858 Loss-g-kl: 1.2402 lr: 0.0002 grad_norm_g: 656.3988 grad_norm_d: 25.0918
======> Epoch: 1239
Train Epoch: 1240 [42.31%] G-Loss: 33.0041 D-Loss: 2.1804 Loss-g-fm: 9.0251 Loss-g-mel: 18.3836 Loss-g-dur: 1.3089 Loss-g-kl: 1.4900 lr: 0.0002 grad_norm_g: 936.5669 grad_norm_d: 72.1830
======> Epoch: 1240
Train Epoch: 1241 [38.46%] G-Loss: 32.8214 D-Loss: 2.2204 Loss-g-fm: 8.9653 Loss-g-mel: 18.0752 Loss-g-dur: 1.3774 Loss-g-kl: 1.5331 lr: 0.0002 grad_norm_g: 701.4178 grad_norm_d: 81.9678
======> Epoch: 1241
Train Epoch: 1242 [34.62%] G-Loss: 26.7775 D-Loss: 2.3974 Loss-g-fm: 7.3666 Loss-g-mel: 14.8328 Loss-g-dur: 1.1111 Loss-g-kl: 1.2409 lr: 0.0002 grad_norm_g: 409.4794 grad_norm_d: 29.9239
======> Epoch: 1242
Train Epoch: 1243 [30.77%] G-Loss: 25.9976 D-Loss: 2.5598 Loss-g-fm: 7.0690 Loss-g-mel: 14.4391 Loss-g-dur: 1.1411 Loss-g-kl: 1.1298 lr: 0.0002 grad_norm_g: 277.8422 grad_norm_d: 16.3149
======> Epoch: 1243
Train Epoch: 1244 [26.92%] G-Loss: 31.5891 D-Loss: 2.1448 Loss-g-fm: 8.7977 Loss-g-mel: 17.7982 Loss-g-dur: 1.2766 Loss-g-kl: 1.1311 lr: 0.0002 grad_norm_g: 535.4019 grad_norm_d: 41.0753
======> Epoch: 1244
Train Epoch: 1245 [23.08%] G-Loss: 21.5789 D-Loss: 2.2312 Loss-g-fm: 7.2169 Loss-g-mel: 9.6564 Loss-g-dur: 0.8110 Loss-g-kl: 1.2284 lr: 0.0002 grad_norm_g: 1018.3256 grad_norm_d: 45.3937
======> Epoch: 1245
Train Epoch: 1246 [19.23%] G-Loss: 32.5530 D-Loss: 2.2788 Loss-g-fm: 9.0237 Loss-g-mel: 18.2903 Loss-g-dur: 1.4838 Loss-g-kl: 1.4204 lr: 0.0002 grad_norm_g: 210.3103 grad_norm_d: 17.5595
======> Epoch: 1246
Train Epoch: 1247 [15.38%] G-Loss: 32.6995 D-Loss: 2.3949 Loss-g-fm: 9.0200 Loss-g-mel: 18.0123 Loss-g-dur: 1.4084 Loss-g-kl: 1.5689 lr: 0.0002 grad_norm_g: 787.5285 grad_norm_d: 95.1602
======> Epoch: 1247
Train Epoch: 1248 [11.54%] G-Loss: 26.7994 D-Loss: 2.4332 Loss-g-fm: 7.5669 Loss-g-mel: 14.3909 Loss-g-dur: 1.1177 Loss-g-kl: 1.2765 lr: 0.0002 grad_norm_g: 628.6858 grad_norm_d: 115.2512
======> Epoch: 1248
Train Epoch: 1249 [7.69%] G-Loss: 32.3950 D-Loss: 2.2559 Loss-g-fm: 9.1434 Loss-g-mel: 17.9525 Loss-g-dur: 1.3158 Loss-g-kl: 1.3962 lr: 0.0002 grad_norm_g: 515.5430 grad_norm_d: 38.0417
======> Epoch: 1249
Train Epoch: 1250 [3.85%] G-Loss: 32.3736 D-Loss: 2.2432 Loss-g-fm: 8.8746 Loss-g-mel: 18.1912 Loss-g-dur: 1.3217 Loss-g-kl: 1.4148 lr: 0.0002 grad_norm_g: 447.0298 grad_norm_d: 33.5620
======> Epoch: 1250
Train Epoch: 1251 [0.00%] G-Loss: 30.0116 D-Loss: 2.3843 Loss-g-fm: 7.6129 Loss-g-mel: 17.3859 Loss-g-dur: 1.2819 Loss-g-kl: 1.2696 lr: 0.0002 grad_norm_g: 493.2448 grad_norm_d: 57.1891
Train Epoch: 1251 [96.15%] G-Loss: 34.8092 D-Loss: 2.2868 Loss-g-fm: 9.4652 Loss-g-mel: 19.5311 Loss-g-dur: 1.4865 Loss-g-kl: 1.5484 lr: 0.0002 grad_norm_g: 680.6997 grad_norm_d: 35.6778
======> Epoch: 1251
Train Epoch: 1252 [92.31%] G-Loss: 33.0606 D-Loss: 2.2043 Loss-g-fm: 9.0576 Loss-g-mel: 18.6619 Loss-g-dur: 1.3798 Loss-g-kl: 1.5213 lr: 0.0002 grad_norm_g: 780.9306 grad_norm_d: 44.5140
======> Epoch: 1252
Train Epoch: 1253 [88.46%] G-Loss: 32.4653 D-Loss: 2.1414 Loss-g-fm: 9.0032 Loss-g-mel: 18.1163 Loss-g-dur: 1.2909 Loss-g-kl: 1.4205 lr: 0.0002 grad_norm_g: 682.0695 grad_norm_d: 53.1834
======> Epoch: 1253
Train Epoch: 1254 [84.62%] G-Loss: 33.7404 D-Loss: 2.1896 Loss-g-fm: 9.6076 Loss-g-mel: 18.1494 Loss-g-dur: 1.3033 Loss-g-kl: 1.4296 lr: 0.0002 grad_norm_g: 917.3796 grad_norm_d: 71.9542
======> Epoch: 1254
Train Epoch: 1255 [80.77%] G-Loss: 21.8953 D-Loss: 2.3257 Loss-g-fm: 7.2542 Loss-g-mel: 9.6631 Loss-g-dur: 0.8204 Loss-g-kl: 1.3682 lr: 0.0002 grad_norm_g: 774.2386 grad_norm_d: 72.5324
======> Epoch: 1255
Train Epoch: 1256 [76.92%] G-Loss: 33.2173 D-Loss: 2.2514 Loss-g-fm: 9.3320 Loss-g-mel: 18.4646 Loss-g-dur: 1.3654 Loss-g-kl: 1.4732 lr: 0.0002 grad_norm_g: 364.4382 grad_norm_d: 45.6947
======> Epoch: 1256
Train Epoch: 1257 [73.08%] G-Loss: 30.5698 D-Loss: 2.2546 Loss-g-fm: 7.8522 Loss-g-mel: 17.3772 Loss-g-dur: 1.3325 Loss-g-kl: 1.3499 lr: 0.0002 grad_norm_g: 144.3606 grad_norm_d: 9.5671
======> Epoch: 1257
Train Epoch: 1258 [69.23%] G-Loss: 31.6882 D-Loss: 2.2517 Loss-g-fm: 8.5204 Loss-g-mel: 17.9406 Loss-g-dur: 1.3265 Loss-g-kl: 1.4436 lr: 0.0002 grad_norm_g: 818.9351 grad_norm_d: 80.9223
======> Epoch: 1258
Train Epoch: 1259 [65.38%] G-Loss: 28.9662 D-Loss: 2.3382 Loss-g-fm: 7.4386 Loss-g-mel: 16.6869 Loss-g-dur: 1.2940 Loss-g-kl: 1.0746 lr: 0.0002 grad_norm_g: 839.0987 grad_norm_d: 58.2519
======> Epoch: 1259
Train Epoch: 1260 [61.54%] G-Loss: 31.6736 D-Loss: 2.3321 Loss-g-fm: 8.6056 Loss-g-mel: 18.1293 Loss-g-dur: 1.4223 Loss-g-kl: 1.1909 lr: 0.0002 grad_norm_g: 487.3661 grad_norm_d: 45.1145
======> Epoch: 1260
Train Epoch: 1261 [57.69%] G-Loss: 32.9512 D-Loss: 2.3531 Loss-g-fm: 9.6198 Loss-g-mel: 17.9102 Loss-g-dur: 1.3868 Loss-g-kl: 1.3503 lr: 0.0002 grad_norm_g: 816.7744 grad_norm_d: 48.7926
======> Epoch: 1261
Train Epoch: 1262 [53.85%] G-Loss: 22.2126 D-Loss: 2.4306 Loss-g-fm: 7.2657 Loss-g-mel: 9.5409 Loss-g-dur: 0.7987 Loss-g-kl: 1.4116 lr: 0.0002 grad_norm_g: 1122.1550 grad_norm_d: 53.7047
======> Epoch: 1262
Train Epoch: 1263 [50.00%] G-Loss: 31.7521 D-Loss: 2.2205 Loss-g-fm: 9.2133 Loss-g-mel: 17.8814 Loss-g-dur: 1.2548 Loss-g-kl: 1.0096 lr: 0.0002 grad_norm_g: 1003.3447 grad_norm_d: 96.5068
======> Epoch: 1263
Train Epoch: 1264 [46.15%] G-Loss: 33.0667 D-Loss: 2.1451 Loss-g-fm: 9.7568 Loss-g-mel: 18.4878 Loss-g-dur: 1.2808 Loss-g-kl: 0.9320 lr: 0.0002 grad_norm_g: 524.8131 grad_norm_d: 76.4660
======> Epoch: 1264
Train Epoch: 1265 [42.31%] G-Loss: 30.3458 D-Loss: 2.3139 Loss-g-fm: 7.6837 Loss-g-mel: 17.3592 Loss-g-dur: 1.2959 Loss-g-kl: 1.3121 lr: 0.0002 grad_norm_g: 291.0531 grad_norm_d: 61.2356
======> Epoch: 1265
Train Epoch: 1266 [38.46%] G-Loss: 32.3606 D-Loss: 2.3316 Loss-g-fm: 8.6325 Loss-g-mel: 18.5237 Loss-g-dur: 1.3566 Loss-g-kl: 1.4385 lr: 0.0002 grad_norm_g: 148.1797 grad_norm_d: 16.9976
======> Epoch: 1266
Train Epoch: 1267 [34.62%] G-Loss: 31.6303 D-Loss: 2.2995 Loss-g-fm: 8.6522 Loss-g-mel: 17.8477 Loss-g-dur: 1.3075 Loss-g-kl: 1.3003 lr: 0.0002 grad_norm_g: 78.0560 grad_norm_d: 8.3930
======> Epoch: 1267
Train Epoch: 1268 [30.77%] G-Loss: 31.9191 D-Loss: 2.2395 Loss-g-fm: 8.8314 Loss-g-mel: 17.8594 Loss-g-dur: 1.2464 Loss-g-kl: 1.4043 lr: 0.0002 grad_norm_g: 277.6521 grad_norm_d: 19.5150
======> Epoch: 1268
Train Epoch: 1269 [26.92%] G-Loss: 32.9723 D-Loss: 2.4388 Loss-g-fm: 8.5701 Loss-g-mel: 18.9192 Loss-g-dur: 1.3820 Loss-g-kl: 1.3786 lr: 0.0002 grad_norm_g: 804.4365 grad_norm_d: 26.8463
======> Epoch: 1269
Train Epoch: 1270 [23.08%] G-Loss: 33.4786 D-Loss: 2.2597 Loss-g-fm: 9.6800 Loss-g-mel: 18.1702 Loss-g-dur: 1.3896 Loss-g-kl: 1.5302 lr: 0.0002 grad_norm_g: 688.6554 grad_norm_d: 38.3763
======> Epoch: 1270
Train Epoch: 1271 [19.23%] G-Loss: 30.1981 D-Loss: 2.2263 Loss-g-fm: 8.1018 Loss-g-mel: 16.9247 Loss-g-dur: 1.3019 Loss-g-kl: 1.3802 lr: 0.0002 grad_norm_g: 798.0204 grad_norm_d: 86.6126
======> Epoch: 1271
Train Epoch: 1272 [15.38%] G-Loss: 34.8633 D-Loss: 2.1284 Loss-g-fm: 10.5900 Loss-g-mel: 18.4762 Loss-g-dur: 1.4622 Loss-g-kl: 1.3835 lr: 0.0002 grad_norm_g: 856.6028 grad_norm_d: 89.6913
======> Epoch: 1272
Train Epoch: 1273 [11.54%] G-Loss: 23.0480 D-Loss: 2.2759 Loss-g-fm: 7.5631 Loss-g-mel: 10.5168 Loss-g-dur: 0.7868 Loss-g-kl: 1.3705 lr: 0.0002 grad_norm_g: 971.6124 grad_norm_d: 44.7767
======> Epoch: 1273
Train Epoch: 1274 [7.69%] G-Loss: 33.3663 D-Loss: 2.2549 Loss-g-fm: 9.3135 Loss-g-mel: 18.5199 Loss-g-dur: 1.3724 Loss-g-kl: 1.5190 lr: 0.0002 grad_norm_g: 752.5584 grad_norm_d: 57.7810
======> Epoch: 1274
Train Epoch: 1275 [3.85%] G-Loss: 30.3697 D-Loss: 2.3762 Loss-g-fm: 7.9107 Loss-g-mel: 17.2656 Loss-g-dur: 1.2937 Loss-g-kl: 1.1214 lr: 0.0002 grad_norm_g: 633.9680 grad_norm_d: 159.1794
======> Epoch: 1275
Train Epoch: 1276 [0.00%] G-Loss: 32.5660 D-Loss: 2.2456 Loss-g-fm: 8.4383 Loss-g-mel: 18.4421 Loss-g-dur: 1.3933 Loss-g-kl: 1.4181 lr: 0.0002 grad_norm_g: 591.1847 grad_norm_d: 52.3401
Train Epoch: 1276 [96.15%] G-Loss: 31.6510 D-Loss: 2.3218 Loss-g-fm: 9.0071 Loss-g-mel: 17.6299 Loss-g-dur: 1.2780 Loss-g-kl: 1.3149 lr: 0.0002 grad_norm_g: 203.7451 grad_norm_d: 22.7040
======> Epoch: 1276
Train Epoch: 1277 [92.31%] G-Loss: 28.2006 D-Loss: 2.2319 Loss-g-fm: 8.4382 Loss-g-mel: 14.5988 Loss-g-dur: 1.1116 Loss-g-kl: 1.2538 lr: 0.0002 grad_norm_g: 591.2375 grad_norm_d: 44.7743
======> Epoch: 1277
Train Epoch: 1278 [88.46%] G-Loss: 33.7397 D-Loss: 2.3600 Loss-g-fm: 9.0725 Loss-g-mel: 18.5449 Loss-g-dur: 1.3960 Loss-g-kl: 1.5513 lr: 0.0002 grad_norm_g: 111.7973 grad_norm_d: 15.9822
======> Epoch: 1278
Train Epoch: 1279 [84.62%] G-Loss: 34.5919 D-Loss: 2.1299 Loss-g-fm: 9.9708 Loss-g-mel: 18.6804 Loss-g-dur: 1.5362 Loss-g-kl: 1.5983 lr: 0.0002 grad_norm_g: 933.3019 grad_norm_d: 59.4416
======> Epoch: 1279
Train Epoch: 1280 [80.77%] G-Loss: 30.9080 D-Loss: 2.3759 Loss-g-fm: 9.4067 Loss-g-mel: 15.4379 Loss-g-dur: 1.2137 Loss-g-kl: 1.3607 lr: 0.0002 grad_norm_g: 798.9730 grad_norm_d: 44.9096
======> Epoch: 1280
Train Epoch: 1281 [76.92%] G-Loss: 32.8404 D-Loss: 2.1909 Loss-g-fm: 9.5203 Loss-g-mel: 17.8475 Loss-g-dur: 1.3175 Loss-g-kl: 1.5199 lr: 0.0002 grad_norm_g: 579.2310 grad_norm_d: 19.6757
======> Epoch: 1281
Train Epoch: 1282 [73.08%] G-Loss: 33.0775 D-Loss: 2.1532 Loss-g-fm: 9.7095 Loss-g-mel: 18.0645 Loss-g-dur: 1.2718 Loss-g-kl: 1.4326 lr: 0.0002 grad_norm_g: 917.3972 grad_norm_d: 70.2273
======> Epoch: 1282
Train Epoch: 1283 [69.23%] G-Loss: 32.8271 D-Loss: 2.1875 Loss-g-fm: 9.6964 Loss-g-mel: 18.1014 Loss-g-dur: 1.2735 Loss-g-kl: 1.1328 lr: 0.0002 grad_norm_g: 746.2558 grad_norm_d: 57.5714
======> Epoch: 1283
Train Epoch: 1284 [65.38%] G-Loss: 32.6053 D-Loss: 2.1911 Loss-g-fm: 9.3559 Loss-g-mel: 18.0305 Loss-g-dur: 1.2648 Loss-g-kl: 1.1554 lr: 0.0002 grad_norm_g: 624.1830 grad_norm_d: 37.7179
======> Epoch: 1284
Train Epoch: 1285 [61.54%] G-Loss: 33.1166 D-Loss: 2.3147 Loss-g-fm: 9.0291 Loss-g-mel: 18.8806 Loss-g-dur: 1.3384 Loss-g-kl: 1.2747 lr: 0.0002 grad_norm_g: 663.0070 grad_norm_d: 54.7815
======> Epoch: 1285
Train Epoch: 1286 [57.69%] G-Loss: 32.2415 D-Loss: 2.2276 Loss-g-fm: 8.5606 Loss-g-mel: 18.3344 Loss-g-dur: 1.4174 Loss-g-kl: 1.4399 lr: 0.0002 grad_norm_g: 151.2949 grad_norm_d: 8.0120
======> Epoch: 1286
Train Epoch: 1287 [53.85%] G-Loss: 33.8387 D-Loss: 2.2848 Loss-g-fm: 9.7265 Loss-g-mel: 18.6391 Loss-g-dur: 1.3788 Loss-g-kl: 1.4883 lr: 0.0002 grad_norm_g: 143.7721 grad_norm_d: 52.6271
======> Epoch: 1287
Train Epoch: 1288 [50.00%] G-Loss: 33.8795 D-Loss: 2.3663 Loss-g-fm: 9.2312 Loss-g-mel: 18.6883 Loss-g-dur: 1.3885 Loss-g-kl: 1.4151 lr: 0.0002 grad_norm_g: 631.8629 grad_norm_d: 24.7109
======> Epoch: 1288
Train Epoch: 1289 [46.15%] G-Loss: 34.6599 D-Loss: 2.2075 Loss-g-fm: 10.1565 Loss-g-mel: 18.8809 Loss-g-dur: 1.4817 Loss-g-kl: 1.5409 lr: 0.0002 grad_norm_g: 835.9631 grad_norm_d: 43.3172
======> Epoch: 1289
Train Epoch: 1290 [42.31%] G-Loss: 33.1996 D-Loss: 2.3260 Loss-g-fm: 8.9378 Loss-g-mel: 18.4362 Loss-g-dur: 1.4262 Loss-g-kl: 1.4903 lr: 0.0002 grad_norm_g: 587.2398 grad_norm_d: 34.8106
======> Epoch: 1290
Train Epoch: 1291 [38.46%] G-Loss: 30.7634 D-Loss: 2.2437 Loss-g-fm: 8.2843 Loss-g-mel: 17.5209 Loss-g-dur: 1.3909 Loss-g-kl: 1.0980 lr: 0.0002 grad_norm_g: 853.3662 grad_norm_d: 83.3620
======> Epoch: 1291
Train Epoch: 1292 [34.62%] G-Loss: 33.3605 D-Loss: 2.3291 Loss-g-fm: 9.5013 Loss-g-mel: 18.7126 Loss-g-dur: 1.3767 Loss-g-kl: 1.4683 lr: 0.0002 grad_norm_g: 736.4209 grad_norm_d: 41.3360
======> Epoch: 1292
Train Epoch: 1293 [30.77%] G-Loss: 32.2540 D-Loss: 2.3469 Loss-g-fm: 9.2431 Loss-g-mel: 17.9285 Loss-g-dur: 1.2986 Loss-g-kl: 1.0981 lr: 0.0002 grad_norm_g: 900.0282 grad_norm_d: 99.1970
======> Epoch: 1293
Train Epoch: 1294 [26.92%] G-Loss: 31.7883 D-Loss: 2.2667 Loss-g-fm: 9.1641 Loss-g-mel: 17.6881 Loss-g-dur: 1.2414 Loss-g-kl: 1.1392 lr: 0.0002 grad_norm_g: 874.9988 grad_norm_d: 110.1588
======> Epoch: 1294
Train Epoch: 1295 [23.08%] G-Loss: 33.0127 D-Loss: 2.2967 Loss-g-fm: 9.4088 Loss-g-mel: 18.1797 Loss-g-dur: 1.4068 Loss-g-kl: 1.4180 lr: 0.0002 grad_norm_g: 809.6577 grad_norm_d: 60.7553
======> Epoch: 1295
Train Epoch: 1296 [19.23%] G-Loss: 31.1808 D-Loss: 2.1622 Loss-g-fm: 8.6353 Loss-g-mel: 17.3465 Loss-g-dur: 1.2550 Loss-g-kl: 1.3585 lr: 0.0002 grad_norm_g: 762.8574 grad_norm_d: 64.3976
======> Epoch: 1296
Train Epoch: 1297 [15.38%] G-Loss: 34.1732 D-Loss: 2.1445 Loss-g-fm: 10.0524 Loss-g-mel: 18.9159 Loss-g-dur: 1.3394 Loss-g-kl: 1.2505 lr: 0.0002 grad_norm_g: 610.6347 grad_norm_d: 54.5235
======> Epoch: 1297
Train Epoch: 1298 [11.54%] G-Loss: 32.6347 D-Loss: 2.2457 Loss-g-fm: 8.9592 Loss-g-mel: 18.0188 Loss-g-dur: 1.2798 Loss-g-kl: 1.3301 lr: 0.0002 grad_norm_g: 599.6028 grad_norm_d: 16.7584
======> Epoch: 1298
Train Epoch: 1299 [7.69%] G-Loss: 29.9825 D-Loss: 2.3704 Loss-g-fm: 8.2061 Loss-g-mel: 16.6520 Loss-g-dur: 1.2518 Loss-g-kl: 1.1084 lr: 0.0002 grad_norm_g: 894.8431 grad_norm_d: 100.8069
======> Epoch: 1299
Train Epoch: 1300 [3.85%] G-Loss: 32.1046 D-Loss: 2.1323 Loss-g-fm: 8.9902 Loss-g-mel: 18.1586 Loss-g-dur: 1.2888 Loss-g-kl: 1.2154 lr: 0.0002 grad_norm_g: 722.0649 grad_norm_d: 71.6012
======> Epoch: 1300
Train Epoch: 1301 [0.00%] G-Loss: 31.9473 D-Loss: 2.1863 Loss-g-fm: 9.1998 Loss-g-mel: 17.8620 Loss-g-dur: 1.3853 Loss-g-kl: 1.2905 lr: 0.0002 grad_norm_g: 75.1201 grad_norm_d: 9.6060
Train Epoch: 1301 [96.15%] G-Loss: 32.8097 D-Loss: 2.2336 Loss-g-fm: 9.0773 Loss-g-mel: 18.2332 Loss-g-dur: 1.4696 Loss-g-kl: 1.4187 lr: 0.0002 grad_norm_g: 395.8374 grad_norm_d: 48.4622
======> Epoch: 1301
Train Epoch: 1302 [92.31%] G-Loss: 32.1110 D-Loss: 2.2427 Loss-g-fm: 8.6247 Loss-g-mel: 17.9673 Loss-g-dur: 1.2884 Loss-g-kl: 1.2773 lr: 0.0002 grad_norm_g: 605.8637 grad_norm_d: 56.3122
======> Epoch: 1302
Train Epoch: 1303 [88.46%] G-Loss: 32.7594 D-Loss: 2.2514 Loss-g-fm: 9.4445 Loss-g-mel: 17.7768 Loss-g-dur: 1.3749 Loss-g-kl: 1.5019 lr: 0.0002 grad_norm_g: 936.8444 grad_norm_d: 67.2885
======> Epoch: 1303
Train Epoch: 1304 [84.62%] G-Loss: 32.4427 D-Loss: 2.2282 Loss-g-fm: 8.9496 Loss-g-mel: 18.5813 Loss-g-dur: 1.2761 Loss-g-kl: 1.2665 lr: 0.0002 grad_norm_g: 311.8531 grad_norm_d: 24.2375
======> Epoch: 1304
Train Epoch: 1305 [80.77%] G-Loss: 32.1563 D-Loss: 2.2015 Loss-g-fm: 8.5676 Loss-g-mel: 18.1401 Loss-g-dur: 1.2491 Loss-g-kl: 1.3239 lr: 0.0002 grad_norm_g: 920.6385 grad_norm_d: 46.3900
======> Epoch: 1305
Train Epoch: 1306 [76.92%] G-Loss: 21.2222 D-Loss: 2.2387 Loss-g-fm: 6.5227 Loss-g-mel: 9.5097 Loss-g-dur: 0.7696 Loss-g-kl: 1.4925 lr: 0.0002 grad_norm_g: 1112.0521 grad_norm_d: 36.0352
======> Epoch: 1306
Train Epoch: 1307 [73.08%] G-Loss: 31.1482 D-Loss: 2.2962 Loss-g-fm: 8.4907 Loss-g-mel: 17.5082 Loss-g-dur: 1.3884 Loss-g-kl: 1.4950 lr: 0.0002 grad_norm_g: 58.2811 grad_norm_d: 9.9994
======> Epoch: 1307
Train Epoch: 1308 [69.23%] G-Loss: 22.3340 D-Loss: 2.2867 Loss-g-fm: 6.9295 Loss-g-mel: 10.4052 Loss-g-dur: 0.8834 Loss-g-kl: 1.4416 lr: 0.0002 grad_norm_g: 1253.2164 grad_norm_d: 56.1460
Saving model and optimizer state at iteration 1308 to /ZFS4T/tts/data/VITS/model_saved/G_34000.pth
Saving model and optimizer state at iteration 1308 to /ZFS4T/tts/data/VITS/model_saved/D_34000.pth
======> Epoch: 1308
Train Epoch: 1309 [65.38%] G-Loss: 31.9787 D-Loss: 2.4187 Loss-g-fm: 8.3518 Loss-g-mel: 18.2507 Loss-g-dur: 1.3384 Loss-g-kl: 1.4742 lr: 0.0002 grad_norm_g: 95.7213 grad_norm_d: 113.8305
======> Epoch: 1309
Train Epoch: 1310 [61.54%] G-Loss: 35.1573 D-Loss: 2.4246 Loss-g-fm: 9.9460 Loss-g-mel: 19.5788 Loss-g-dur: 1.2851 Loss-g-kl: 1.5874 lr: 0.0002 grad_norm_g: 620.2218 grad_norm_d: 72.3507
======> Epoch: 1310
Train Epoch: 1311 [57.69%] G-Loss: 33.1884 D-Loss: 2.4398 Loss-g-fm: 9.0512 Loss-g-mel: 19.0243 Loss-g-dur: 1.3581 Loss-g-kl: 1.4428 lr: 0.0002 grad_norm_g: 558.5784 grad_norm_d: 28.4184
======> Epoch: 1311
Train Epoch: 1312 [53.85%] G-Loss: 31.0670 D-Loss: 2.2057 Loss-g-fm: 8.5168 Loss-g-mel: 17.3040 Loss-g-dur: 1.2366 Loss-g-kl: 1.2443 lr: 0.0002 grad_norm_g: 445.4781 grad_norm_d: 44.1634
======> Epoch: 1312
Train Epoch: 1313 [50.00%] G-Loss: 21.4755 D-Loss: 2.2709 Loss-g-fm: 7.2863 Loss-g-mel: 9.3997 Loss-g-dur: 0.8268 Loss-g-kl: 1.4530 lr: 0.0002 grad_norm_g: 755.6597 grad_norm_d: 19.3054
======> Epoch: 1313
Train Epoch: 1314 [46.15%] G-Loss: 32.2427 D-Loss: 2.2223 Loss-g-fm: 8.6979 Loss-g-mel: 18.0045 Loss-g-dur: 1.3677 Loss-g-kl: 1.4759 lr: 0.0002 grad_norm_g: 680.1758 grad_norm_d: 35.9256
======> Epoch: 1314
Train Epoch: 1315 [42.31%] G-Loss: 31.6748 D-Loss: 2.3040 Loss-g-fm: 8.3913 Loss-g-mel: 18.1113 Loss-g-dur: 1.2980 Loss-g-kl: 1.4202 lr: 0.0002 grad_norm_g: 465.7048 grad_norm_d: 45.0970
======> Epoch: 1315
Train Epoch: 1316 [38.46%] G-Loss: 33.7300 D-Loss: 2.1339 Loss-g-fm: 9.1542 Loss-g-mel: 19.1770 Loss-g-dur: 1.3616 Loss-g-kl: 1.4148 lr: 0.0002 grad_norm_g: 305.5850 grad_norm_d: 7.3412
======> Epoch: 1316
Train Epoch: 1317 [34.62%] G-Loss: 30.6142 D-Loss: 2.3693 Loss-g-fm: 8.4046 Loss-g-mel: 17.2378 Loss-g-dur: 1.2828 Loss-g-kl: 1.1852 lr: 0.0002 grad_norm_g: 646.2980 grad_norm_d: 32.8564
======> Epoch: 1317
Train Epoch: 1318 [30.77%] G-Loss: 22.7270 D-Loss: 2.2527 Loss-g-fm: 7.2719 Loss-g-mel: 10.4872 Loss-g-dur: 0.8561 Loss-g-kl: 1.4323 lr: 0.0002 grad_norm_g: 1088.4263 grad_norm_d: 41.2085
======> Epoch: 1318
Train Epoch: 1319 [26.92%] G-Loss: 32.1993 D-Loss: 2.3653 Loss-g-fm: 8.9673 Loss-g-mel: 17.8877 Loss-g-dur: 1.2989 Loss-g-kl: 1.2257 lr: 0.0002 grad_norm_g: 724.5010 grad_norm_d: 103.5334
======> Epoch: 1319
Train Epoch: 1320 [23.08%] G-Loss: 30.8188 D-Loss: 2.4302 Loss-g-fm: 8.3845 Loss-g-mel: 17.4975 Loss-g-dur: 1.2442 Loss-g-kl: 1.3450 lr: 0.0002 grad_norm_g: 371.5654 grad_norm_d: 25.6529
======> Epoch: 1320
Train Epoch: 1321 [19.23%] G-Loss: 30.2905 D-Loss: 2.3958 Loss-g-fm: 7.9385 Loss-g-mel: 17.4298 Loss-g-dur: 1.2925 Loss-g-kl: 1.2787 lr: 0.0002 grad_norm_g: 601.2981 grad_norm_d: 21.1332
======> Epoch: 1321
Train Epoch: 1322 [15.38%] G-Loss: 33.9228 D-Loss: 2.2243 Loss-g-fm: 9.6780 Loss-g-mel: 18.5967 Loss-g-dur: 1.4565 Loss-g-kl: 1.6374 lr: 0.0002 grad_norm_g: 110.0072 grad_norm_d: 4.2428
======> Epoch: 1322
Train Epoch: 1323 [11.54%] G-Loss: 21.1027 D-Loss: 2.4122 Loss-g-fm: 6.2981 Loss-g-mel: 10.0648 Loss-g-dur: 0.7938 Loss-g-kl: 1.5125 lr: 0.0002 grad_norm_g: 1139.9123 grad_norm_d: 52.4332
======> Epoch: 1323
Train Epoch: 1324 [7.69%] G-Loss: 33.9533 D-Loss: 2.2365 Loss-g-fm: 10.1814 Loss-g-mel: 18.6746 Loss-g-dur: 1.3183 Loss-g-kl: 1.2722 lr: 0.0002 grad_norm_g: 354.1542 grad_norm_d: 21.0030
======> Epoch: 1324
Train Epoch: 1325 [3.85%] G-Loss: 33.9323 D-Loss: 2.2197 Loss-g-fm: 10.1222 Loss-g-mel: 18.3565 Loss-g-dur: 1.3491 Loss-g-kl: 1.4168 lr: 0.0002 grad_norm_g: 580.2082 grad_norm_d: 45.7694
======> Epoch: 1325
Train Epoch: 1326 [0.00%] G-Loss: 22.7016 D-Loss: 2.2328 Loss-g-fm: 7.4051 Loss-g-mel: 10.2191 Loss-g-dur: 0.7867 Loss-g-kl: 1.4716 lr: 0.0002 grad_norm_g: 1405.4976 grad_norm_d: 61.0596
Train Epoch: 1326 [96.15%] G-Loss: 34.3781 D-Loss: 2.2635 Loss-g-fm: 9.9058 Loss-g-mel: 18.8929 Loss-g-dur: 1.4135 Loss-g-kl: 1.5269 lr: 0.0002 grad_norm_g: 663.0782 grad_norm_d: 68.0480
======> Epoch: 1326
Train Epoch: 1327 [92.31%] G-Loss: 22.5772 D-Loss: 2.1855 Loss-g-fm: 7.6728 Loss-g-mel: 9.9922 Loss-g-dur: 0.7622 Loss-g-kl: 1.4941 lr: 0.0002 grad_norm_g: 896.3523 grad_norm_d: 58.9178
======> Epoch: 1327
Train Epoch: 1328 [88.46%] G-Loss: 31.1846 D-Loss: 2.1946 Loss-g-fm: 8.4215 Loss-g-mel: 17.7668 Loss-g-dur: 1.2776 Loss-g-kl: 1.1694 lr: 0.0002 grad_norm_g: 728.7694 grad_norm_d: 71.2512
======> Epoch: 1328
Train Epoch: 1329 [84.62%] G-Loss: 30.1796 D-Loss: 2.2691 Loss-g-fm: 8.1055 Loss-g-mel: 16.9750 Loss-g-dur: 1.2780 Loss-g-kl: 1.2988 lr: 0.0002 grad_norm_g: 271.1646 grad_norm_d: 6.3118
======> Epoch: 1329
Train Epoch: 1330 [80.77%] G-Loss: 33.7597 D-Loss: 2.2729 Loss-g-fm: 9.4554 Loss-g-mel: 19.0303 Loss-g-dur: 1.3760 Loss-g-kl: 1.3892 lr: 0.0002 grad_norm_g: 918.0622 grad_norm_d: 47.6270
======> Epoch: 1330
Train Epoch: 1331 [76.92%] G-Loss: 31.7374 D-Loss: 2.1553 Loss-g-fm: 8.8534 Loss-g-mel: 17.7033 Loss-g-dur: 1.2349 Loss-g-kl: 1.3452 lr: 0.0002 grad_norm_g: 568.1341 grad_norm_d: 23.6763
======> Epoch: 1331
Train Epoch: 1332 [73.08%] G-Loss: 31.5995 D-Loss: 2.3138 Loss-g-fm: 8.9805 Loss-g-mel: 17.4284 Loss-g-dur: 1.2996 Loss-g-kl: 1.3785 lr: 0.0002 grad_norm_g: 452.4324 grad_norm_d: 11.2362
======> Epoch: 1332
Train Epoch: 1333 [69.23%] G-Loss: 33.4230 D-Loss: 2.5070 Loss-g-fm: 9.4823 Loss-g-mel: 18.2920 Loss-g-dur: 1.3819 Loss-g-kl: 1.5007 lr: 0.0002 grad_norm_g: 912.5341 grad_norm_d: 121.3421
======> Epoch: 1333
Train Epoch: 1334 [65.38%] G-Loss: 33.7667 D-Loss: 2.2724 Loss-g-fm: 10.0063 Loss-g-mel: 18.3215 Loss-g-dur: 1.3484 Loss-g-kl: 1.5619 lr: 0.0002 grad_norm_g: 749.9676 grad_norm_d: 71.5634
======> Epoch: 1334
Train Epoch: 1335 [61.54%] G-Loss: 31.9269 D-Loss: 2.2189 Loss-g-fm: 8.9873 Loss-g-mel: 18.0014 Loss-g-dur: 1.2354 Loss-g-kl: 1.2483 lr: 0.0002 grad_norm_g: 657.6952 grad_norm_d: 50.1162
======> Epoch: 1335
Train Epoch: 1336 [57.69%] G-Loss: 30.6168 D-Loss: 2.4054 Loss-g-fm: 8.0599 Loss-g-mel: 17.8942 Loss-g-dur: 1.2351 Loss-g-kl: 1.5593 lr: 0.0002 grad_norm_g: 91.4216 grad_norm_d: 43.7469
======> Epoch: 1336
Train Epoch: 1337 [53.85%] G-Loss: 20.8773 D-Loss: 2.3892 Loss-g-fm: 6.3789 Loss-g-mel: 9.9709 Loss-g-dur: 0.7778 Loss-g-kl: 1.3256 lr: 0.0002 grad_norm_g: 906.9243 grad_norm_d: 30.4073
======> Epoch: 1337
Train Epoch: 1338 [50.00%] G-Loss: 31.6408 D-Loss: 2.2291 Loss-g-fm: 8.6728 Loss-g-mel: 17.4631 Loss-g-dur: 1.3047 Loss-g-kl: 1.6143 lr: 0.0002 grad_norm_g: 516.5146 grad_norm_d: 41.3911
======> Epoch: 1338
Train Epoch: 1339 [46.15%] G-Loss: 31.8918 D-Loss: 2.3624 Loss-g-fm: 8.7738 Loss-g-mel: 17.9794 Loss-g-dur: 1.2778 Loss-g-kl: 1.3468 lr: 0.0002 grad_norm_g: 862.9555 grad_norm_d: 40.3677
======> Epoch: 1339
Train Epoch: 1340 [42.31%] G-Loss: 33.3601 D-Loss: 2.2034 Loss-g-fm: 9.6324 Loss-g-mel: 18.5791 Loss-g-dur: 1.2668 Loss-g-kl: 1.2468 lr: 0.0002 grad_norm_g: 758.2622 grad_norm_d: 34.9154
======> Epoch: 1340
Train Epoch: 1341 [38.46%] G-Loss: 31.1332 D-Loss: 2.5008 Loss-g-fm: 8.6879 Loss-g-mel: 17.3048 Loss-g-dur: 1.2764 Loss-g-kl: 1.1865 lr: 0.0002 grad_norm_g: 571.2191 grad_norm_d: 99.0772
======> Epoch: 1341
Train Epoch: 1342 [34.62%] G-Loss: 31.5251 D-Loss: 2.3455 Loss-g-fm: 8.3997 Loss-g-mel: 17.9230 Loss-g-dur: 1.3002 Loss-g-kl: 1.2775 lr: 0.0002 grad_norm_g: 276.5395 grad_norm_d: 12.9369
======> Epoch: 1342
Train Epoch: 1343 [30.77%] G-Loss: 32.6988 D-Loss: 2.1653 Loss-g-fm: 8.7157 Loss-g-mel: 18.5893 Loss-g-dur: 1.4036 Loss-g-kl: 1.4151 lr: 0.0002 grad_norm_g: 117.5232 grad_norm_d: 18.8952
======> Epoch: 1343
Train Epoch: 1344 [26.92%] G-Loss: 32.0303 D-Loss: 2.3473 Loss-g-fm: 8.5951 Loss-g-mel: 17.9766 Loss-g-dur: 1.2771 Loss-g-kl: 1.2905 lr: 0.0002 grad_norm_g: 298.6826 grad_norm_d: 15.5402
======> Epoch: 1344
Train Epoch: 1345 [23.08%] G-Loss: 20.6824 D-Loss: 2.3611 Loss-g-fm: 6.5195 Loss-g-mel: 9.4701 Loss-g-dur: 0.7973 Loss-g-kl: 1.3098 lr: 0.0002 grad_norm_g: 1129.3668 grad_norm_d: 45.1020
======> Epoch: 1345
Train Epoch: 1346 [19.23%] G-Loss: 22.1852 D-Loss: 2.2231 Loss-g-fm: 6.9720 Loss-g-mel: 10.4454 Loss-g-dur: 0.8623 Loss-g-kl: 1.3666 lr: 0.0002 grad_norm_g: 974.8098 grad_norm_d: 63.7062
======> Epoch: 1346
Train Epoch: 1347 [15.38%] G-Loss: 32.0366 D-Loss: 2.1957 Loss-g-fm: 9.2798 Loss-g-mel: 17.5393 Loss-g-dur: 1.2618 Loss-g-kl: 1.3249 lr: 0.0002 grad_norm_g: 851.2446 grad_norm_d: 66.9478
======> Epoch: 1347
Train Epoch: 1348 [11.54%] G-Loss: 30.2325 D-Loss: 2.3173 Loss-g-fm: 7.9005 Loss-g-mel: 17.2021 Loss-g-dur: 1.2908 Loss-g-kl: 1.3758 lr: 0.0002 grad_norm_g: 633.6652 grad_norm_d: 32.5947
======> Epoch: 1348
Train Epoch: 1349 [7.69%] G-Loss: 31.2848 D-Loss: 2.5225 Loss-g-fm: 8.7535 Loss-g-mel: 17.4350 Loss-g-dur: 1.2834 Loss-g-kl: 1.1603 lr: 0.0002 grad_norm_g: 269.7758 grad_norm_d: 25.5069
======> Epoch: 1349
Train Epoch: 1350 [3.85%] G-Loss: 32.0893 D-Loss: 2.2471 Loss-g-fm: 8.6375 Loss-g-mel: 18.3734 Loss-g-dur: 1.3165 Loss-g-kl: 1.2826 lr: 0.0002 grad_norm_g: 406.9030 grad_norm_d: 22.9147
======> Epoch: 1350
Train Epoch: 1351 [0.00%] G-Loss: 35.8071 D-Loss: 2.1244 Loss-g-fm: 11.2730 Loss-g-mel: 18.8248 Loss-g-dur: 1.3596 Loss-g-kl: 1.5684 lr: 0.0002 grad_norm_g: 650.2418 grad_norm_d: 34.7953
Train Epoch: 1351 [96.15%] G-Loss: 32.4193 D-Loss: 2.2204 Loss-g-fm: 9.0461 Loss-g-mel: 18.2124 Loss-g-dur: 1.3706 Loss-g-kl: 1.5657 lr: 0.0002 grad_norm_g: 540.6581 grad_norm_d: 31.5320
======> Epoch: 1351
Train Epoch: 1352 [92.31%] G-Loss: 30.6520 D-Loss: 2.3168 Loss-g-fm: 8.2575 Loss-g-mel: 17.4675 Loss-g-dur: 1.1996 Loss-g-kl: 1.1432 lr: 0.0002 grad_norm_g: 725.9672 grad_norm_d: 98.2835
======> Epoch: 1352
Train Epoch: 1353 [88.46%] G-Loss: 32.3948 D-Loss: 2.5802 Loss-g-fm: 8.9480 Loss-g-mel: 17.5229 Loss-g-dur: 1.3429 Loss-g-kl: 1.3924 lr: 0.0002 grad_norm_g: 355.5352 grad_norm_d: 75.4047
======> Epoch: 1353
Train Epoch: 1354 [84.62%] G-Loss: 26.8706 D-Loss: 2.3107 Loss-g-fm: 7.7474 Loss-g-mel: 14.5671 Loss-g-dur: 1.0762 Loss-g-kl: 1.1988 lr: 0.0002 grad_norm_g: 484.4097 grad_norm_d: 41.6154
======> Epoch: 1354
Train Epoch: 1355 [80.77%] G-Loss: 32.8454 D-Loss: 2.2361 Loss-g-fm: 9.3791 Loss-g-mel: 18.1254 Loss-g-dur: 1.3257 Loss-g-kl: 1.5924 lr: 0.0002 grad_norm_g: 333.1465 grad_norm_d: 8.2219
======> Epoch: 1355
Train Epoch: 1356 [76.92%] G-Loss: 31.6568 D-Loss: 2.3124 Loss-g-fm: 8.5480 Loss-g-mel: 17.8643 Loss-g-dur: 1.2834 Loss-g-kl: 1.4641 lr: 0.0002 grad_norm_g: 247.8401 grad_norm_d: 15.5902
======> Epoch: 1356
Train Epoch: 1357 [73.08%] G-Loss: 32.4357 D-Loss: 2.2719 Loss-g-fm: 9.6434 Loss-g-mel: 17.5828 Loss-g-dur: 1.3304 Loss-g-kl: 1.4808 lr: 0.0002 grad_norm_g: 753.6166 grad_norm_d: 58.4193
======> Epoch: 1357
Train Epoch: 1358 [69.23%] G-Loss: 32.5161 D-Loss: 2.2137 Loss-g-fm: 8.7785 Loss-g-mel: 18.4978 Loss-g-dur: 1.2595 Loss-g-kl: 1.3280 lr: 0.0002 grad_norm_g: 283.1145 grad_norm_d: 52.1271
======> Epoch: 1358
Train Epoch: 1359 [65.38%] G-Loss: 31.5914 D-Loss: 2.3227 Loss-g-fm: 9.0357 Loss-g-mel: 17.4855 Loss-g-dur: 1.2861 Loss-g-kl: 1.4435 lr: 0.0002 grad_norm_g: 434.2803 grad_norm_d: 17.3162
======> Epoch: 1359
Train Epoch: 1360 [61.54%] G-Loss: 32.6921 D-Loss: 2.2281 Loss-g-fm: 9.2528 Loss-g-mel: 18.1660 Loss-g-dur: 1.3426 Loss-g-kl: 1.2229 lr: 0.0002 grad_norm_g: 479.3898 grad_norm_d: 26.9180
======> Epoch: 1360
Train Epoch: 1361 [57.69%] G-Loss: 32.7738 D-Loss: 2.0463 Loss-g-fm: 9.6759 Loss-g-mel: 18.1052 Loss-g-dur: 1.2922 Loss-g-kl: 1.1512 lr: 0.0002 grad_norm_g: 795.6074 grad_norm_d: 45.0435
======> Epoch: 1361
Train Epoch: 1362 [53.85%] G-Loss: 34.5622 D-Loss: 2.0854 Loss-g-fm: 10.3146 Loss-g-mel: 18.6743 Loss-g-dur: 1.3882 Loss-g-kl: 1.4349 lr: 0.0002 grad_norm_g: 827.4271 grad_norm_d: 65.9050
======> Epoch: 1362
Train Epoch: 1363 [50.00%] G-Loss: 32.0417 D-Loss: 2.2227 Loss-g-fm: 9.0183 Loss-g-mel: 17.9791 Loss-g-dur: 1.2574 Loss-g-kl: 1.2908 lr: 0.0002 grad_norm_g: 551.7280 grad_norm_d: 61.7938
======> Epoch: 1363
Train Epoch: 1364 [46.15%] G-Loss: 32.9566 D-Loss: 2.3762 Loss-g-fm: 9.6037 Loss-g-mel: 17.8497 Loss-g-dur: 1.3536 Loss-g-kl: 1.4844 lr: 0.0002 grad_norm_g: 947.1480 grad_norm_d: 99.5284
======> Epoch: 1364
Train Epoch: 1365 [42.31%] G-Loss: 30.5296 D-Loss: 2.3256 Loss-g-fm: 8.0978 Loss-g-mel: 17.4131 Loss-g-dur: 1.2696 Loss-g-kl: 1.2868 lr: 0.0002 grad_norm_g: 698.4459 grad_norm_d: 37.5138
======> Epoch: 1365
Train Epoch: 1366 [38.46%] G-Loss: 30.7569 D-Loss: 2.1991 Loss-g-fm: 8.2276 Loss-g-mel: 17.5680 Loss-g-dur: 1.2653 Loss-g-kl: 1.2253 lr: 0.0002 grad_norm_g: 647.9856 grad_norm_d: 30.5553
======> Epoch: 1366
Train Epoch: 1367 [34.62%] G-Loss: 34.4369 D-Loss: 2.2782 Loss-g-fm: 9.3478 Loss-g-mel: 19.1514 Loss-g-dur: 1.4754 Loss-g-kl: 1.7489 lr: 0.0002 grad_norm_g: 852.3956 grad_norm_d: 63.4740
======> Epoch: 1367
Train Epoch: 1368 [30.77%] G-Loss: 32.2729 D-Loss: 2.2412 Loss-g-fm: 9.4046 Loss-g-mel: 17.8579 Loss-g-dur: 1.2713 Loss-g-kl: 1.2830 lr: 0.0002 grad_norm_g: 740.4019 grad_norm_d: 45.8499
======> Epoch: 1368
Train Epoch: 1369 [26.92%] G-Loss: 34.0379 D-Loss: 2.3051 Loss-g-fm: 9.9329 Loss-g-mel: 18.8848 Loss-g-dur: 1.3482 Loss-g-kl: 1.2676 lr: 0.0002 grad_norm_g: 354.2797 grad_norm_d: 31.4804
======> Epoch: 1369
Train Epoch: 1370 [23.08%] G-Loss: 33.5895 D-Loss: 2.1730 Loss-g-fm: 9.7350 Loss-g-mel: 18.5237 Loss-g-dur: 1.3538 Loss-g-kl: 1.3030 lr: 0.0002 grad_norm_g: 415.7593 grad_norm_d: 7.3385
======> Epoch: 1370
Train Epoch: 1371 [19.23%] G-Loss: 31.4560 D-Loss: 2.2231 Loss-g-fm: 8.8728 Loss-g-mel: 17.0182 Loss-g-dur: 1.2771 Loss-g-kl: 1.2387 lr: 0.0002 grad_norm_g: 887.6030 grad_norm_d: 79.4283
======> Epoch: 1371
Train Epoch: 1372 [15.38%] G-Loss: 33.9007 D-Loss: 2.2238 Loss-g-fm: 9.4171 Loss-g-mel: 18.7694 Loss-g-dur: 1.3258 Loss-g-kl: 1.4756 lr: 0.0002 grad_norm_g: 622.1290 grad_norm_d: 109.7392
======> Epoch: 1372
Train Epoch: 1373 [11.54%] G-Loss: 20.9609 D-Loss: 2.4047 Loss-g-fm: 6.9649 Loss-g-mel: 9.4366 Loss-g-dur: 0.8486 Loss-g-kl: 1.4720 lr: 0.0002 grad_norm_g: 275.5573 grad_norm_d: 22.0411
======> Epoch: 1373
Train Epoch: 1374 [7.69%] G-Loss: 34.2931 D-Loss: 2.1811 Loss-g-fm: 9.8112 Loss-g-mel: 18.7257 Loss-g-dur: 1.3674 Loss-g-kl: 1.7638 lr: 0.0002 grad_norm_g: 262.9228 grad_norm_d: 9.1122
======> Epoch: 1374
Train Epoch: 1375 [3.85%] G-Loss: 32.1121 D-Loss: 2.2746 Loss-g-fm: 9.2294 Loss-g-mel: 17.4750 Loss-g-dur: 1.2417 Loss-g-kl: 1.3994 lr: 0.0002 grad_norm_g: 708.5268 grad_norm_d: 33.1630
======> Epoch: 1375
Train Epoch: 1376 [0.00%] G-Loss: 29.8265 D-Loss: 2.2312 Loss-g-fm: 8.1793 Loss-g-mel: 16.4545 Loss-g-dur: 1.2555 Loss-g-kl: 1.3617 lr: 0.0002 grad_norm_g: 602.7332 grad_norm_d: 19.9823
Train Epoch: 1376 [96.15%] G-Loss: 30.9841 D-Loss: 2.2570 Loss-g-fm: 8.5342 Loss-g-mel: 17.1645 Loss-g-dur: 1.2911 Loss-g-kl: 1.2446 lr: 0.0002 grad_norm_g: 576.4839 grad_norm_d: 28.0149
======> Epoch: 1376
Train Epoch: 1377 [92.31%] G-Loss: 30.3517 D-Loss: 2.1072 Loss-g-fm: 9.7932 Loss-g-mel: 15.3534 Loss-g-dur: 1.0605 Loss-g-kl: 1.5314 lr: 0.0002 grad_norm_g: 53.2945 grad_norm_d: 15.3016
======> Epoch: 1377
Train Epoch: 1378 [88.46%] G-Loss: 34.4299 D-Loss: 2.1417 Loss-g-fm: 10.2384 Loss-g-mel: 18.7529 Loss-g-dur: 1.3920 Loss-g-kl: 1.4036 lr: 0.0002 grad_norm_g: 825.5705 grad_norm_d: 16.8375
======> Epoch: 1378
Train Epoch: 1379 [84.62%] G-Loss: 32.6345 D-Loss: 2.2050 Loss-g-fm: 9.1574 Loss-g-mel: 18.1882 Loss-g-dur: 1.2683 Loss-g-kl: 1.5495 lr: 0.0002 grad_norm_g: 618.9481 grad_norm_d: 46.9222
======> Epoch: 1379
Train Epoch: 1380 [80.77%] G-Loss: 31.2908 D-Loss: 2.3780 Loss-g-fm: 8.5816 Loss-g-mel: 17.6115 Loss-g-dur: 1.2343 Loss-g-kl: 1.3352 lr: 0.0002 grad_norm_g: 583.5831 grad_norm_d: 5.5771
======> Epoch: 1380
Train Epoch: 1381 [76.92%] G-Loss: 35.3797 D-Loss: 2.3277 Loss-g-fm: 11.1209 Loss-g-mel: 18.7934 Loss-g-dur: 1.4216 Loss-g-kl: 1.4170 lr: 0.0002 grad_norm_g: 770.3639 grad_norm_d: 67.0492
======> Epoch: 1381
Train Epoch: 1382 [73.08%] G-Loss: 31.5925 D-Loss: 2.2994 Loss-g-fm: 8.7633 Loss-g-mel: 17.6653 Loss-g-dur: 1.2963 Loss-g-kl: 1.3510 lr: 0.0002 grad_norm_g: 483.8830 grad_norm_d: 35.6133
======> Epoch: 1382
Train Epoch: 1383 [69.23%] G-Loss: 32.0771 D-Loss: 2.2573 Loss-g-fm: 9.3021 Loss-g-mel: 17.8997 Loss-g-dur: 1.3040 Loss-g-kl: 1.2504 lr: 0.0002 grad_norm_g: 573.2754 grad_norm_d: 49.4059
======> Epoch: 1383
Train Epoch: 1384 [65.38%] G-Loss: 32.4789 D-Loss: 2.1944 Loss-g-fm: 9.1779 Loss-g-mel: 18.1851 Loss-g-dur: 1.2914 Loss-g-kl: 1.3176 lr: 0.0002 grad_norm_g: 523.7190 grad_norm_d: 40.3436
======> Epoch: 1384
Train Epoch: 1385 [61.54%] G-Loss: 33.6888 D-Loss: 2.1250 Loss-g-fm: 10.2386 Loss-g-mel: 18.2093 Loss-g-dur: 1.2353 Loss-g-kl: 1.2138 lr: 0.0002 grad_norm_g: 715.2728 grad_norm_d: 31.7253
Saving model and optimizer state at iteration 1385 to /ZFS4T/tts/data/VITS/model_saved/G_36000.pth
Saving model and optimizer state at iteration 1385 to /ZFS4T/tts/data/VITS/model_saved/D_36000.pth
======> Epoch: 1385
Train Epoch: 1386 [57.69%] G-Loss: 33.3900 D-Loss: 2.1530 Loss-g-fm: 9.7852 Loss-g-mel: 18.3337 Loss-g-dur: 1.2713 Loss-g-kl: 1.2267 lr: 0.0002 grad_norm_g: 595.2071 grad_norm_d: 49.3098
======> Epoch: 1386
Train Epoch: 1387 [53.85%] G-Loss: 33.9180 D-Loss: 2.0674 Loss-g-fm: 10.2247 Loss-g-mel: 18.0923 Loss-g-dur: 1.3376 Loss-g-kl: 1.3934 lr: 0.0002 grad_norm_g: 1116.7823 grad_norm_d: 55.2180
======> Epoch: 1387
Train Epoch: 1388 [50.00%] G-Loss: 32.4399 D-Loss: 2.1925 Loss-g-fm: 9.5344 Loss-g-mel: 17.5573 Loss-g-dur: 1.2632 Loss-g-kl: 1.3702 lr: 0.0002 grad_norm_g: 780.2285 grad_norm_d: 74.9413
======> Epoch: 1388
Train Epoch: 1389 [46.15%] G-Loss: 33.1423 D-Loss: 2.2169 Loss-g-fm: 9.7671 Loss-g-mel: 18.1936 Loss-g-dur: 1.3331 Loss-g-kl: 1.3126 lr: 0.0002 grad_norm_g: 603.1164 grad_norm_d: 76.7295
======> Epoch: 1389
Train Epoch: 1390 [42.31%] G-Loss: 32.4021 D-Loss: 2.2420 Loss-g-fm: 9.3008 Loss-g-mel: 17.8264 Loss-g-dur: 1.2618 Loss-g-kl: 1.4040 lr: 0.0002 grad_norm_g: 377.6610 grad_norm_d: 35.8258
======> Epoch: 1390
Train Epoch: 1391 [38.46%] G-Loss: 30.9497 D-Loss: 2.2376 Loss-g-fm: 8.8024 Loss-g-mel: 17.1643 Loss-g-dur: 1.2501 Loss-g-kl: 1.4146 lr: 0.0002 grad_norm_g: 40.7051 grad_norm_d: 15.3176
======> Epoch: 1391
Train Epoch: 1392 [34.62%] G-Loss: 32.7359 D-Loss: 2.2185 Loss-g-fm: 9.1696 Loss-g-mel: 18.3486 Loss-g-dur: 1.3754 Loss-g-kl: 1.5612 lr: 0.0002 grad_norm_g: 161.4680 grad_norm_d: 30.6790
======> Epoch: 1392
Train Epoch: 1393 [30.77%] G-Loss: 34.6668 D-Loss: 2.2553 Loss-g-fm: 10.5659 Loss-g-mel: 18.5746 Loss-g-dur: 1.3599 Loss-g-kl: 1.5970 lr: 0.0002 grad_norm_g: 655.7338 grad_norm_d: 42.6531
======> Epoch: 1393
Train Epoch: 1394 [26.92%] G-Loss: 31.4115 D-Loss: 2.2031 Loss-g-fm: 9.2013 Loss-g-mel: 17.2323 Loss-g-dur: 1.2679 Loss-g-kl: 1.1610 lr: 0.0002 grad_norm_g: 765.8075 grad_norm_d: 67.8659
======> Epoch: 1394
Train Epoch: 1395 [23.08%] G-Loss: 34.0407 D-Loss: 2.2991 Loss-g-fm: 9.9596 Loss-g-mel: 18.9984 Loss-g-dur: 1.3460 Loss-g-kl: 1.2827 lr: 0.0002 grad_norm_g: 703.3469 grad_norm_d: 27.6940
======> Epoch: 1395
Train Epoch: 1396 [19.23%] G-Loss: 31.7732 D-Loss: 2.1577 Loss-g-fm: 8.8665 Loss-g-mel: 17.8294 Loss-g-dur: 1.2423 Loss-g-kl: 1.2250 lr: 0.0002 grad_norm_g: 685.5495 grad_norm_d: 56.1379
======> Epoch: 1396
Train Epoch: 1397 [15.38%] G-Loss: 31.9353 D-Loss: 2.2838 Loss-g-fm: 9.1594 Loss-g-mel: 17.6417 Loss-g-dur: 1.2162 Loss-g-kl: 1.3742 lr: 0.0002 grad_norm_g: 732.2129 grad_norm_d: 46.5710
======> Epoch: 1397
Train Epoch: 1398 [11.54%] G-Loss: 30.5366 D-Loss: 2.3685 Loss-g-fm: 8.3239 Loss-g-mel: 17.2669 Loss-g-dur: 1.2240 Loss-g-kl: 1.2865 lr: 0.0002 grad_norm_g: 870.5627 grad_norm_d: 82.0534
======> Epoch: 1398
Train Epoch: 1399 [7.69%] G-Loss: 33.3964 D-Loss: 2.4249 Loss-g-fm: 9.2111 Loss-g-mel: 19.1254 Loss-g-dur: 1.3365 Loss-g-kl: 1.2506 lr: 0.0002 grad_norm_g: 651.8693 grad_norm_d: 91.5964
======> Epoch: 1399
Train Epoch: 1400 [3.85%] G-Loss: 29.5702 D-Loss: 2.4455 Loss-g-fm: 7.7465 Loss-g-mel: 16.8733 Loss-g-dur: 1.2472 Loss-g-kl: 1.3630 lr: 0.0002 grad_norm_g: 276.6682 grad_norm_d: 42.6688
======> Epoch: 1400
Train Epoch: 1401 [0.00%] G-Loss: 30.6216 D-Loss: 2.2410 Loss-g-fm: 8.4430 Loss-g-mel: 17.1589 Loss-g-dur: 1.2895 Loss-g-kl: 1.3140 lr: 0.0002 grad_norm_g: 436.1044 grad_norm_d: 32.7093
Train Epoch: 1401 [96.15%] G-Loss: 33.3026 D-Loss: 2.2812 Loss-g-fm: 9.8927 Loss-g-mel: 17.9881 Loss-g-dur: 1.3433 Loss-g-kl: 1.4710 lr: 0.0002 grad_norm_g: 783.2916 grad_norm_d: 54.7241
======> Epoch: 1401
Train Epoch: 1402 [92.31%] G-Loss: 33.9358 D-Loss: 2.2522 Loss-g-fm: 9.9419 Loss-g-mel: 18.4679 Loss-g-dur: 1.4229 Loss-g-kl: 1.4918 lr: 0.0002 grad_norm_g: 1046.8717 grad_norm_d: 63.1275
======> Epoch: 1402
Train Epoch: 1403 [88.46%] G-Loss: 32.9973 D-Loss: 2.4181 Loss-g-fm: 9.7159 Loss-g-mel: 18.2158 Loss-g-dur: 1.3100 Loss-g-kl: 1.3712 lr: 0.0002 grad_norm_g: 282.1785 grad_norm_d: 23.1169
======> Epoch: 1403
Train Epoch: 1404 [84.62%] G-Loss: 34.4114 D-Loss: 2.1720 Loss-g-fm: 10.3360 Loss-g-mel: 18.7985 Loss-g-dur: 1.3781 Loss-g-kl: 1.4989 lr: 0.0002 grad_norm_g: 198.0103 grad_norm_d: 8.4944
======> Epoch: 1404
Train Epoch: 1405 [80.77%] G-Loss: 34.3732 D-Loss: 2.1604 Loss-g-fm: 10.4219 Loss-g-mel: 18.3170 Loss-g-dur: 1.3454 Loss-g-kl: 1.6286 lr: 0.0002 grad_norm_g: 736.5463 grad_norm_d: 60.8592
======> Epoch: 1405
Train Epoch: 1406 [76.92%] G-Loss: 31.0488 D-Loss: 2.3564 Loss-g-fm: 8.4962 Loss-g-mel: 17.5438 Loss-g-dur: 1.2458 Loss-g-kl: 1.3729 lr: 0.0002 grad_norm_g: 56.2681 grad_norm_d: 46.9475
======> Epoch: 1406
Train Epoch: 1407 [73.08%] G-Loss: 31.3895 D-Loss: 2.3228 Loss-g-fm: 9.0869 Loss-g-mel: 17.3223 Loss-g-dur: 1.2617 Loss-g-kl: 1.1171 lr: 0.0002 grad_norm_g: 101.9222 grad_norm_d: 21.8037
======> Epoch: 1407
Train Epoch: 1408 [69.23%] G-Loss: 34.2282 D-Loss: 2.2142 Loss-g-fm: 10.1931 Loss-g-mel: 18.4286 Loss-g-dur: 1.3460 Loss-g-kl: 1.4123 lr: 0.0002 grad_norm_g: 971.2250 grad_norm_d: 44.6979
======> Epoch: 1408
Train Epoch: 1409 [65.38%] G-Loss: 31.9592 D-Loss: 2.2763 Loss-g-fm: 8.8563 Loss-g-mel: 17.8830 Loss-g-dur: 1.2446 Loss-g-kl: 1.3003 lr: 0.0002 grad_norm_g: 146.0688 grad_norm_d: 15.4698
======> Epoch: 1409
Train Epoch: 1410 [61.54%] G-Loss: 33.5371 D-Loss: 2.2539 Loss-g-fm: 9.5824 Loss-g-mel: 18.3104 Loss-g-dur: 1.2840 Loss-g-kl: 1.6183 lr: 0.0002 grad_norm_g: 208.6150 grad_norm_d: 21.1596
======> Epoch: 1410
Train Epoch: 1411 [57.69%] G-Loss: 32.8997 D-Loss: 2.2465 Loss-g-fm: 9.4731 Loss-g-mel: 18.0396 Loss-g-dur: 1.2659 Loss-g-kl: 1.3059 lr: 0.0002 grad_norm_g: 792.6606 grad_norm_d: 64.8293
======> Epoch: 1411
Train Epoch: 1412 [53.85%] G-Loss: 34.5317 D-Loss: 2.1145 Loss-g-fm: 9.9684 Loss-g-mel: 19.0488 Loss-g-dur: 1.3611 Loss-g-kl: 1.4365 lr: 0.0002 grad_norm_g: 611.3325 grad_norm_d: 37.1381
======> Epoch: 1412
Train Epoch: 1413 [50.00%] G-Loss: 20.9691 D-Loss: 2.3557 Loss-g-fm: 6.5227 Loss-g-mel: 9.6948 Loss-g-dur: 0.8234 Loss-g-kl: 1.3922 lr: 0.0002 grad_norm_g: 1242.7053 grad_norm_d: 38.4243
======> Epoch: 1413
Train Epoch: 1414 [46.15%] G-Loss: 33.0482 D-Loss: 2.2785 Loss-g-fm: 9.4259 Loss-g-mel: 18.4434 Loss-g-dur: 1.3444 Loss-g-kl: 1.2495 lr: 0.0002 grad_norm_g: 983.5222 grad_norm_d: 73.0936
======> Epoch: 1414
Train Epoch: 1415 [42.31%] G-Loss: 32.4243 D-Loss: 2.1648 Loss-g-fm: 9.1081 Loss-g-mel: 17.7991 Loss-g-dur: 1.3341 Loss-g-kl: 1.4303 lr: 0.0002 grad_norm_g: 824.5959 grad_norm_d: 50.2288
======> Epoch: 1415
Train Epoch: 1416 [38.46%] G-Loss: 34.2891 D-Loss: 2.1439 Loss-g-fm: 10.4305 Loss-g-mel: 18.4037 Loss-g-dur: 1.3149 Loss-g-kl: 1.4706 lr: 0.0002 grad_norm_g: 907.1193 grad_norm_d: 56.6108
======> Epoch: 1416
Train Epoch: 1417 [34.62%] G-Loss: 32.1795 D-Loss: 2.2813 Loss-g-fm: 9.2495 Loss-g-mel: 17.4850 Loss-g-dur: 1.3343 Loss-g-kl: 1.4331 lr: 0.0002 grad_norm_g: 442.2958 grad_norm_d: 19.2285
======> Epoch: 1417
Train Epoch: 1418 [30.77%] G-Loss: 31.1611 D-Loss: 2.1496 Loss-g-fm: 8.9175 Loss-g-mel: 17.0661 Loss-g-dur: 1.2513 Loss-g-kl: 1.1564 lr: 0.0002 grad_norm_g: 796.3653 grad_norm_d: 57.0385
======> Epoch: 1418
Train Epoch: 1419 [26.92%] G-Loss: 34.9896 D-Loss: 2.1251 Loss-g-fm: 10.9072 Loss-g-mel: 18.5720 Loss-g-dur: 1.3394 Loss-g-kl: 1.6097 lr: 0.0002 grad_norm_g: 588.5889 grad_norm_d: 57.5644
======> Epoch: 1419
Train Epoch: 1420 [23.08%] G-Loss: 33.8461 D-Loss: 2.4194 Loss-g-fm: 9.7620 Loss-g-mel: 17.8979 Loss-g-dur: 1.5804 Loss-g-kl: 1.7387 lr: 0.0002 grad_norm_g: 456.1425 grad_norm_d: 10.4010
======> Epoch: 1420
Train Epoch: 1421 [19.23%] G-Loss: 33.3124 D-Loss: 2.1901 Loss-g-fm: 9.4372 Loss-g-mel: 18.2682 Loss-g-dur: 1.3196 Loss-g-kl: 1.5913 lr: 0.0002 grad_norm_g: 758.2049 grad_norm_d: 75.1790
======> Epoch: 1421
Train Epoch: 1422 [15.38%] G-Loss: 32.8408 D-Loss: 2.3428 Loss-g-fm: 9.2955 Loss-g-mel: 17.6725 Loss-g-dur: 1.2638 Loss-g-kl: 1.4169 lr: 0.0002 grad_norm_g: 145.6526 grad_norm_d: 30.6179
======> Epoch: 1422
Train Epoch: 1423 [11.54%] G-Loss: 34.4111 D-Loss: 2.2897 Loss-g-fm: 10.2102 Loss-g-mel: 18.8756 Loss-g-dur: 1.3882 Loss-g-kl: 1.5082 lr: 0.0002 grad_norm_g: 91.6456 grad_norm_d: 14.2335
======> Epoch: 1423
Train Epoch: 1424 [7.69%] G-Loss: 33.2228 D-Loss: 2.1484 Loss-g-fm: 9.6607 Loss-g-mel: 17.9353 Loss-g-dur: 1.2581 Loss-g-kl: 1.4587 lr: 0.0002 grad_norm_g: 705.6594 grad_norm_d: 38.0638
======> Epoch: 1424
Train Epoch: 1425 [3.85%] G-Loss: 34.8710 D-Loss: 2.2373 Loss-g-fm: 10.5199 Loss-g-mel: 18.7010 Loss-g-dur: 1.3733 Loss-g-kl: 1.4845 lr: 0.0002 grad_norm_g: 310.3702 grad_norm_d: 9.8933
======> Epoch: 1425
Train Epoch: 1426 [0.00%] G-Loss: 33.7525 D-Loss: 2.2307 Loss-g-fm: 10.3045 Loss-g-mel: 18.2259 Loss-g-dur: 1.2809 Loss-g-kl: 1.4839 lr: 0.0002 grad_norm_g: 748.0693 grad_norm_d: 58.2394
Train Epoch: 1426 [96.15%] G-Loss: 20.8934 D-Loss: 2.5699 Loss-g-fm: 6.1561 Loss-g-mel: 9.8513 Loss-g-dur: 0.7824 Loss-g-kl: 1.3884 lr: 0.0002 grad_norm_g: 1383.5462 grad_norm_d: 63.1437
======> Epoch: 1426
Train Epoch: 1427 [92.31%] G-Loss: 30.8058 D-Loss: 2.4259 Loss-g-fm: 8.4363 Loss-g-mel: 17.3863 Loss-g-dur: 1.2427 Loss-g-kl: 1.3643 lr: 0.0002 grad_norm_g: 172.9511 grad_norm_d: 24.0092
======> Epoch: 1427
Train Epoch: 1428 [88.46%] G-Loss: 32.1346 D-Loss: 2.2074 Loss-g-fm: 9.3321 Loss-g-mel: 17.8139 Loss-g-dur: 1.2647 Loss-g-kl: 1.2565 lr: 0.0002 grad_norm_g: 725.7843 grad_norm_d: 57.5807
======> Epoch: 1428
Train Epoch: 1429 [84.62%] G-Loss: 29.9347 D-Loss: 2.2188 Loss-g-fm: 9.2317 Loss-g-mel: 15.2597 Loss-g-dur: 1.0665 Loss-g-kl: 1.5558 lr: 0.0002 grad_norm_g: 336.2856 grad_norm_d: 19.5449
======> Epoch: 1429
Train Epoch: 1430 [80.77%] G-Loss: 31.4159 D-Loss: 2.3480 Loss-g-fm: 8.7209 Loss-g-mel: 17.5448 Loss-g-dur: 1.3395 Loss-g-kl: 1.3949 lr: 0.0002 grad_norm_g: 111.0407 grad_norm_d: 27.5497
======> Epoch: 1430
Train Epoch: 1431 [76.92%] G-Loss: 31.8395 D-Loss: 2.1265 Loss-g-fm: 9.3848 Loss-g-mel: 17.0934 Loss-g-dur: 1.2557 Loss-g-kl: 1.3670 lr: 0.0002 grad_norm_g: 843.3320 grad_norm_d: 70.0254
======> Epoch: 1431
Train Epoch: 1432 [73.08%] G-Loss: 32.3169 D-Loss: 2.3110 Loss-g-fm: 9.5502 Loss-g-mel: 17.4386 Loss-g-dur: 1.3328 Loss-g-kl: 1.4733 lr: 0.0002 grad_norm_g: 782.3183 grad_norm_d: 24.6887
======> Epoch: 1432
Train Epoch: 1433 [69.23%] G-Loss: 33.1780 D-Loss: 2.2673 Loss-g-fm: 9.7353 Loss-g-mel: 17.6558 Loss-g-dur: 1.2531 Loss-g-kl: 1.4677 lr: 0.0002 grad_norm_g: 927.0361 grad_norm_d: 69.9653
======> Epoch: 1433
Train Epoch: 1434 [65.38%] G-Loss: 32.8124 D-Loss: 2.2088 Loss-g-fm: 9.3921 Loss-g-mel: 18.1008 Loss-g-dur: 1.2510 Loss-g-kl: 1.3912 lr: 0.0002 grad_norm_g: 883.6588 grad_norm_d: 78.1803
======> Epoch: 1434
Train Epoch: 1435 [61.54%] G-Loss: 33.3169 D-Loss: 2.2088 Loss-g-fm: 9.7691 Loss-g-mel: 18.1724 Loss-g-dur: 1.4275 Loss-g-kl: 1.3025 lr: 0.0002 grad_norm_g: 793.5754 grad_norm_d: 44.8550
======> Epoch: 1435
Train Epoch: 1436 [57.69%] G-Loss: 21.2017 D-Loss: 2.3158 Loss-g-fm: 6.6338 Loss-g-mel: 9.6432 Loss-g-dur: 0.7800 Loss-g-kl: 1.4808 lr: 0.0002 grad_norm_g: 1315.8138 grad_norm_d: 50.7283
======> Epoch: 1436
Train Epoch: 1437 [53.85%] G-Loss: 33.9186 D-Loss: 2.1123 Loss-g-fm: 10.5957 Loss-g-mel: 18.1438 Loss-g-dur: 1.1909 Loss-g-kl: 1.3039 lr: 0.0002 grad_norm_g: 659.5339 grad_norm_d: 19.0020
======> Epoch: 1437
Train Epoch: 1438 [50.00%] G-Loss: 21.5146 D-Loss: 2.4904 Loss-g-fm: 6.6694 Loss-g-mel: 10.1132 Loss-g-dur: 0.7475 Loss-g-kl: 1.3311 lr: 0.0002 grad_norm_g: 474.4549 grad_norm_d: 20.3981
======> Epoch: 1438
Train Epoch: 1439 [46.15%] G-Loss: 31.1399 D-Loss: 2.2926 Loss-g-fm: 8.5470 Loss-g-mel: 17.4751 Loss-g-dur: 1.2584 Loss-g-kl: 1.3593 lr: 0.0002 grad_norm_g: 640.9440 grad_norm_d: 57.1326
======> Epoch: 1439
Train Epoch: 1440 [42.31%] G-Loss: 33.9111 D-Loss: 2.1946 Loss-g-fm: 10.1726 Loss-g-mel: 18.1018 Loss-g-dur: 1.4052 Loss-g-kl: 1.5813 lr: 0.0002 grad_norm_g: 1081.0621 grad_norm_d: 55.8849
======> Epoch: 1440
Train Epoch: 1441 [38.46%] G-Loss: 34.5943 D-Loss: 2.3605 Loss-g-fm: 10.1898 Loss-g-mel: 18.5831 Loss-g-dur: 1.3810 Loss-g-kl: 1.5390 lr: 0.0002 grad_norm_g: 839.1302 grad_norm_d: 106.8293
======> Epoch: 1441
Train Epoch: 1442 [34.62%] G-Loss: 33.3988 D-Loss: 2.3417 Loss-g-fm: 9.1393 Loss-g-mel: 18.5146 Loss-g-dur: 1.3254 Loss-g-kl: 1.4417 lr: 0.0002 grad_norm_g: 460.9826 grad_norm_d: 101.2467
======> Epoch: 1442
Train Epoch: 1443 [30.77%] G-Loss: 33.1993 D-Loss: 2.3597 Loss-g-fm: 9.4741 Loss-g-mel: 18.1580 Loss-g-dur: 1.3881 Loss-g-kl: 1.6374 lr: 0.0002 grad_norm_g: 759.4654 grad_norm_d: 30.8819
======> Epoch: 1443
Train Epoch: 1444 [26.92%] G-Loss: 31.5323 D-Loss: 2.2939 Loss-g-fm: 9.2184 Loss-g-mel: 17.2390 Loss-g-dur: 1.2725 Loss-g-kl: 1.4879 lr: 0.0002 grad_norm_g: 132.2686 grad_norm_d: 6.3729
======> Epoch: 1444
Train Epoch: 1445 [23.08%] G-Loss: 34.6826 D-Loss: 2.1757 Loss-g-fm: 9.9968 Loss-g-mel: 18.8845 Loss-g-dur: 1.4281 Loss-g-kl: 1.5628 lr: 0.0002 grad_norm_g: 964.9051 grad_norm_d: 42.5132
======> Epoch: 1445
Train Epoch: 1446 [19.23%] G-Loss: 33.1671 D-Loss: 2.1384 Loss-g-fm: 9.7947 Loss-g-mel: 17.8497 Loss-g-dur: 1.2423 Loss-g-kl: 1.5280 lr: 0.0002 grad_norm_g: 755.8789 grad_norm_d: 48.5949
======> Epoch: 1446
Train Epoch: 1447 [15.38%] G-Loss: 32.3843 D-Loss: 2.1712 Loss-g-fm: 9.6661 Loss-g-mel: 17.4734 Loss-g-dur: 1.2628 Loss-g-kl: 1.5368 lr: 0.0002 grad_norm_g: 713.5244 grad_norm_d: 52.1793
======> Epoch: 1447
Train Epoch: 1448 [11.54%] G-Loss: 31.9286 D-Loss: 2.2339 Loss-g-fm: 9.2657 Loss-g-mel: 17.3968 Loss-g-dur: 1.2681 Loss-g-kl: 1.2171 lr: 0.0002 grad_norm_g: 405.6394 grad_norm_d: 59.1433
======> Epoch: 1448
Train Epoch: 1449 [7.69%] G-Loss: 32.3688 D-Loss: 2.2317 Loss-g-fm: 9.5135 Loss-g-mel: 17.4968 Loss-g-dur: 1.2721 Loss-g-kl: 1.5241 lr: 0.0002 grad_norm_g: 96.9314 grad_norm_d: 18.6463
======> Epoch: 1449
Train Epoch: 1450 [3.85%] G-Loss: 33.3900 D-Loss: 2.1194 Loss-g-fm: 10.2355 Loss-g-mel: 17.5339 Loss-g-dur: 1.2362 Loss-g-kl: 1.3331 lr: 0.0002 grad_norm_g: 899.6191 grad_norm_d: 80.7179
======> Epoch: 1450
Train Epoch: 1451 [0.00%] G-Loss: 31.6333 D-Loss: 2.2669 Loss-g-fm: 8.9598 Loss-g-mel: 17.5608 Loss-g-dur: 1.2468 Loss-g-kl: 1.4012 lr: 0.0002 grad_norm_g: 485.3226 grad_norm_d: 33.4637
Train Epoch: 1451 [96.15%] G-Loss: 33.3320 D-Loss: 2.2469 Loss-g-fm: 10.2614 Loss-g-mel: 17.8336 Loss-g-dur: 1.2320 Loss-g-kl: 1.4361 lr: 0.0002 grad_norm_g: 1068.9114 grad_norm_d: 73.0347
======> Epoch: 1451
Train Epoch: 1452 [92.31%] G-Loss: 33.4837 D-Loss: 2.1577 Loss-g-fm: 9.3136 Loss-g-mel: 18.8533 Loss-g-dur: 1.4067 Loss-g-kl: 1.4365 lr: 0.0002 grad_norm_g: 708.4848 grad_norm_d: 24.2051
======> Epoch: 1452
Train Epoch: 1453 [88.46%] G-Loss: 33.4620 D-Loss: 2.2319 Loss-g-fm: 10.1557 Loss-g-mel: 18.0541 Loss-g-dur: 1.2696 Loss-g-kl: 1.3761 lr: 0.0002 grad_norm_g: 920.1818 grad_norm_d: 65.4962
======> Epoch: 1453
Train Epoch: 1454 [84.62%] G-Loss: 29.0429 D-Loss: 2.3517 Loss-g-fm: 9.3459 Loss-g-mel: 14.9721 Loss-g-dur: 1.0554 Loss-g-kl: 1.4547 lr: 0.0002 grad_norm_g: 810.3327 grad_norm_d: 69.2821
======> Epoch: 1454
Train Epoch: 1455 [80.77%] G-Loss: 34.3949 D-Loss: 2.2927 Loss-g-fm: 10.1282 Loss-g-mel: 18.6353 Loss-g-dur: 1.3272 Loss-g-kl: 1.4950 lr: 0.0002 grad_norm_g: 1044.1846 grad_norm_d: 65.3644
======> Epoch: 1455
Train Epoch: 1456 [76.92%] G-Loss: 31.3949 D-Loss: 2.2708 Loss-g-fm: 8.6383 Loss-g-mel: 17.1090 Loss-g-dur: 1.2231 Loss-g-kl: 1.5779 lr: 0.0002 grad_norm_g: 284.0957 grad_norm_d: 58.7626
======> Epoch: 1456
Train Epoch: 1457 [73.08%] G-Loss: 34.6360 D-Loss: 2.2092 Loss-g-fm: 10.4528 Loss-g-mel: 18.6178 Loss-g-dur: 1.4267 Loss-g-kl: 1.4016 lr: 0.0002 grad_norm_g: 632.4911 grad_norm_d: 55.3773
======> Epoch: 1457
Train Epoch: 1458 [69.23%] G-Loss: 20.8134 D-Loss: 2.4893 Loss-g-fm: 6.2370 Loss-g-mel: 9.7715 Loss-g-dur: 0.7379 Loss-g-kl: 1.5304 lr: 0.0002 grad_norm_g: 976.8527 grad_norm_d: 38.8716
======> Epoch: 1458
Train Epoch: 1459 [65.38%] G-Loss: 31.2248 D-Loss: 2.1807 Loss-g-fm: 8.6343 Loss-g-mel: 17.2777 Loss-g-dur: 1.2546 Loss-g-kl: 1.3187 lr: 0.0002 grad_norm_g: 832.3050 grad_norm_d: 37.4038
======> Epoch: 1459
Train Epoch: 1460 [61.54%] G-Loss: 33.1684 D-Loss: 2.3869 Loss-g-fm: 9.6043 Loss-g-mel: 18.1521 Loss-g-dur: 1.4508 Loss-g-kl: 1.4025 lr: 0.0002 grad_norm_g: 886.8455 grad_norm_d: 39.9393
======> Epoch: 1460
Train Epoch: 1461 [57.69%] G-Loss: 32.8703 D-Loss: 2.2579 Loss-g-fm: 9.6250 Loss-g-mel: 17.6591 Loss-g-dur: 1.2913 Loss-g-kl: 1.5849 lr: 0.0002 grad_norm_g: 958.8745 grad_norm_d: 83.2214
======> Epoch: 1461
Train Epoch: 1462 [53.85%] G-Loss: 30.9489 D-Loss: 2.2609 Loss-g-fm: 8.5682 Loss-g-mel: 17.3319 Loss-g-dur: 1.2473 Loss-g-kl: 1.3976 lr: 0.0002 grad_norm_g: 522.3255 grad_norm_d: 35.1836
Saving model and optimizer state at iteration 1462 to /ZFS4T/tts/data/VITS/model_saved/G_38000.pth
Saving model and optimizer state at iteration 1462 to /ZFS4T/tts/data/VITS/model_saved/D_38000.pth
======> Epoch: 1462
Train Epoch: 1463 [50.00%] G-Loss: 34.8794 D-Loss: 2.1530 Loss-g-fm: 10.5862 Loss-g-mel: 18.6944 Loss-g-dur: 1.3253 Loss-g-kl: 1.4650 lr: 0.0002 grad_norm_g: 698.4498 grad_norm_d: 45.3769
======> Epoch: 1463
Train Epoch: 1464 [46.15%] G-Loss: 30.0789 D-Loss: 2.1788 Loss-g-fm: 8.3071 Loss-g-mel: 16.5930 Loss-g-dur: 1.2299 Loss-g-kl: 1.2119 lr: 0.0002 grad_norm_g: 762.1530 grad_norm_d: 29.9776
======> Epoch: 1464
Train Epoch: 1465 [42.31%] G-Loss: 32.3980 D-Loss: 2.3239 Loss-g-fm: 9.4557 Loss-g-mel: 17.3182 Loss-g-dur: 1.4497 Loss-g-kl: 1.6934 lr: 0.0002 grad_norm_g: 463.5082 grad_norm_d: 46.7087
======> Epoch: 1465
Train Epoch: 1466 [38.46%] G-Loss: 29.9636 D-Loss: 2.3548 Loss-g-fm: 8.1345 Loss-g-mel: 16.6209 Loss-g-dur: 1.2641 Loss-g-kl: 1.3765 lr: 0.0002 grad_norm_g: 197.1824 grad_norm_d: 13.5497
======> Epoch: 1466
Train Epoch: 1467 [34.62%] G-Loss: 29.8693 D-Loss: 2.5167 Loss-g-fm: 7.5189 Loss-g-mel: 16.7881 Loss-g-dur: 1.2553 Loss-g-kl: 1.5746 lr: 0.0002 grad_norm_g: 663.1969 grad_norm_d: 137.3853
======> Epoch: 1467
Train Epoch: 1468 [30.77%] G-Loss: 35.3629 D-Loss: 2.3886 Loss-g-fm: 10.9165 Loss-g-mel: 19.0018 Loss-g-dur: 1.3400 Loss-g-kl: 1.4672 lr: 0.0002 grad_norm_g: 651.8458 grad_norm_d: 37.6839
======> Epoch: 1468
Train Epoch: 1469 [26.92%] G-Loss: 31.3730 D-Loss: 2.2868 Loss-g-fm: 9.2410 Loss-g-mel: 17.1044 Loss-g-dur: 1.2919 Loss-g-kl: 1.2611 lr: 0.0002 grad_norm_g: 31.3808 grad_norm_d: 17.8234
======> Epoch: 1469
Train Epoch: 1470 [23.08%] G-Loss: 20.5150 D-Loss: 2.3521 Loss-g-fm: 6.5079 Loss-g-mel: 9.4684 Loss-g-dur: 0.7280 Loss-g-kl: 1.3092 lr: 0.0002 grad_norm_g: 89.0999 grad_norm_d: 11.7780
======> Epoch: 1470
Train Epoch: 1471 [19.23%] G-Loss: 35.0681 D-Loss: 2.1460 Loss-g-fm: 11.0326 Loss-g-mel: 18.3520 Loss-g-dur: 1.3883 Loss-g-kl: 1.6005 lr: 0.0002 grad_norm_g: 565.8726 grad_norm_d: 42.3562
======> Epoch: 1471
Train Epoch: 1472 [15.38%] G-Loss: 34.6460 D-Loss: 2.2367 Loss-g-fm: 10.2002 Loss-g-mel: 18.7773 Loss-g-dur: 1.3569 Loss-g-kl: 1.6296 lr: 0.0002 grad_norm_g: 332.2600 grad_norm_d: 48.1876
======> Epoch: 1472
Train Epoch: 1473 [11.54%] G-Loss: 20.5036 D-Loss: 2.1421 Loss-g-fm: 6.5269 Loss-g-mel: 8.8756 Loss-g-dur: 0.7708 Loss-g-kl: 1.4308 lr: 0.0002 grad_norm_g: 1212.2476 grad_norm_d: 45.0261
======> Epoch: 1473
Train Epoch: 1474 [7.69%] G-Loss: 34.9997 D-Loss: 2.0473 Loss-g-fm: 10.8174 Loss-g-mel: 18.6065 Loss-g-dur: 1.3122 Loss-g-kl: 1.4124 lr: 0.0002 grad_norm_g: 972.5876 grad_norm_d: 47.7193
======> Epoch: 1474
Train Epoch: 1475 [3.85%] G-Loss: 31.3837 D-Loss: 2.3040 Loss-g-fm: 8.9725 Loss-g-mel: 17.3637 Loss-g-dur: 1.2484 Loss-g-kl: 1.3379 lr: 0.0002 grad_norm_g: 691.4543 grad_norm_d: 88.3734
======> Epoch: 1475
Train Epoch: 1476 [0.00%] G-Loss: 32.6261 D-Loss: 2.2014 Loss-g-fm: 9.4439 Loss-g-mel: 17.8286 Loss-g-dur: 1.2353 Loss-g-kl: 1.4652 lr: 0.0002 grad_norm_g: 192.3001 grad_norm_d: 11.6607
Train Epoch: 1476 [96.15%] G-Loss: 33.7072 D-Loss: 2.1720 Loss-g-fm: 9.6391 Loss-g-mel: 18.3730 Loss-g-dur: 1.3589 Loss-g-kl: 1.6413 lr: 0.0002 grad_norm_g: 162.5254 grad_norm_d: 4.9361
======> Epoch: 1476
Train Epoch: 1477 [92.31%] G-Loss: 31.7180 D-Loss: 2.3474 Loss-g-fm: 8.7713 Loss-g-mel: 17.7838 Loss-g-dur: 1.3004 Loss-g-kl: 1.4928 lr: 0.0002 grad_norm_g: 200.2735 grad_norm_d: 5.7785
======> Epoch: 1477
Train Epoch: 1478 [88.46%] G-Loss: 33.2630 D-Loss: 2.3122 Loss-g-fm: 9.3967 Loss-g-mel: 17.9040 Loss-g-dur: 1.3781 Loss-g-kl: 1.7276 lr: 0.0002 grad_norm_g: 675.5835 grad_norm_d: 39.1687
======> Epoch: 1478
Train Epoch: 1479 [84.62%] G-Loss: 32.5980 D-Loss: 2.2363 Loss-g-fm: 9.2792 Loss-g-mel: 17.6775 Loss-g-dur: 1.2982 Loss-g-kl: 1.4950 lr: 0.0002 grad_norm_g: 188.5625 grad_norm_d: 21.5887
======> Epoch: 1479
Train Epoch: 1480 [80.77%] G-Loss: 33.8992 D-Loss: 2.2228 Loss-g-fm: 10.2651 Loss-g-mel: 18.6034 Loss-g-dur: 1.3013 Loss-g-kl: 1.3226 lr: 0.0002 grad_norm_g: 232.1155 grad_norm_d: 16.3409
======> Epoch: 1480
Train Epoch: 1481 [76.92%] G-Loss: 32.0568 D-Loss: 2.2860 Loss-g-fm: 9.3087 Loss-g-mel: 17.2380 Loss-g-dur: 1.2625 Loss-g-kl: 1.3909 lr: 0.0002 grad_norm_g: 1003.1280 grad_norm_d: 81.7511
======> Epoch: 1481
Train Epoch: 1482 [73.08%] G-Loss: 34.8406 D-Loss: 2.2422 Loss-g-fm: 10.9789 Loss-g-mel: 18.1778 Loss-g-dur: 1.3204 Loss-g-kl: 1.5030 lr: 0.0002 grad_norm_g: 762.6611 grad_norm_d: 48.4221
======> Epoch: 1482
Train Epoch: 1483 [69.23%] G-Loss: 31.1769 D-Loss: 2.3261 Loss-g-fm: 8.2143 Loss-g-mel: 17.9101 Loss-g-dur: 1.2648 Loss-g-kl: 1.5648 lr: 0.0002 grad_norm_g: 142.1885 grad_norm_d: 24.4564
======> Epoch: 1483
Train Epoch: 1484 [65.38%] G-Loss: 31.7968 D-Loss: 2.1850 Loss-g-fm: 9.5299 Loss-g-mel: 17.5308 Loss-g-dur: 1.2192 Loss-g-kl: 1.2540 lr: 0.0002 grad_norm_g: 254.0106 grad_norm_d: 15.6231
======> Epoch: 1484
Train Epoch: 1485 [61.54%] G-Loss: 32.4232 D-Loss: 2.1827 Loss-g-fm: 9.4401 Loss-g-mel: 17.7835 Loss-g-dur: 1.2470 Loss-g-kl: 1.3365 lr: 0.0002 grad_norm_g: 640.7396 grad_norm_d: 35.8146
======> Epoch: 1485
Train Epoch: 1486 [57.69%] G-Loss: 31.9534 D-Loss: 2.2485 Loss-g-fm: 8.8340 Loss-g-mel: 17.9373 Loss-g-dur: 1.2742 Loss-g-kl: 1.3531 lr: 0.0002 grad_norm_g: 249.2499 grad_norm_d: 55.8580
======> Epoch: 1486
Train Epoch: 1487 [53.85%] G-Loss: 20.8046 D-Loss: 2.4896 Loss-g-fm: 6.5498 Loss-g-mel: 9.1076 Loss-g-dur: 0.9288 Loss-g-kl: 1.4051 lr: 0.0002 grad_norm_g: 375.7097 grad_norm_d: 16.6056
======> Epoch: 1487
Train Epoch: 1488 [50.00%] G-Loss: 31.5789 D-Loss: 2.2635 Loss-g-fm: 9.0460 Loss-g-mel: 17.5273 Loss-g-dur: 1.2050 Loss-g-kl: 1.3570 lr: 0.0002 grad_norm_g: 222.8518 grad_norm_d: 31.5282
======> Epoch: 1488
Train Epoch: 1489 [46.15%] G-Loss: 32.9013 D-Loss: 2.0934 Loss-g-fm: 9.8766 Loss-g-mel: 17.7021 Loss-g-dur: 1.2126 Loss-g-kl: 1.4550 lr: 0.0002 grad_norm_g: 549.7929 grad_norm_d: 42.0996
======> Epoch: 1489
Train Epoch: 1490 [42.31%] G-Loss: 28.8409 D-Loss: 2.2709 Loss-g-fm: 8.9550 Loss-g-mel: 15.0590 Loss-g-dur: 1.0731 Loss-g-kl: 1.2024 lr: 0.0002 grad_norm_g: 363.0728 grad_norm_d: 22.8670
======> Epoch: 1490
Train Epoch: 1491 [38.46%] G-Loss: 20.7388 D-Loss: 2.5523 Loss-g-fm: 6.3125 Loss-g-mel: 9.4677 Loss-g-dur: 0.8081 Loss-g-kl: 1.3847 lr: 0.0002 grad_norm_g: 565.9461 grad_norm_d: 20.3234
======> Epoch: 1491
Train Epoch: 1492 [34.62%] G-Loss: 27.8049 D-Loss: 2.2967 Loss-g-fm: 8.3526 Loss-g-mel: 14.5341 Loss-g-dur: 1.0814 Loss-g-kl: 1.2848 lr: 0.0002 grad_norm_g: 270.2665 grad_norm_d: 42.0391
======> Epoch: 1492
Train Epoch: 1493 [30.77%] G-Loss: 33.3090 D-Loss: 2.1938 Loss-g-fm: 10.1158 Loss-g-mel: 17.7703 Loss-g-dur: 1.2518 Loss-g-kl: 1.3741 lr: 0.0002 grad_norm_g: 1015.6451 grad_norm_d: 48.5662
======> Epoch: 1493
Train Epoch: 1494 [26.92%] G-Loss: 32.3159 D-Loss: 2.0697 Loss-g-fm: 9.5228 Loss-g-mel: 17.6525 Loss-g-dur: 1.2421 Loss-g-kl: 1.3661 lr: 0.0002 grad_norm_g: 543.2100 grad_norm_d: 28.5571
======> Epoch: 1494
Train Epoch: 1495 [23.08%] G-Loss: 34.5336 D-Loss: 2.1646 Loss-g-fm: 10.4103 Loss-g-mel: 18.2865 Loss-g-dur: 1.6157 Loss-g-kl: 1.5385 lr: 0.0002 grad_norm_g: 523.2796 grad_norm_d: 31.7567
======> Epoch: 1495
Train Epoch: 1496 [19.23%] G-Loss: 20.7439 D-Loss: 2.3172 Loss-g-fm: 6.5371 Loss-g-mel: 9.2287 Loss-g-dur: 0.8039 Loss-g-kl: 1.4238 lr: 0.0002 grad_norm_g: inf grad_norm_d: 62.5567
======> Epoch: 1496
Train Epoch: 1497 [15.38%] G-Loss: 31.2017 D-Loss: 2.3497 Loss-g-fm: 8.5298 Loss-g-mel: 17.4236 Loss-g-dur: 1.2674 Loss-g-kl: 1.5565 lr: 0.0002 grad_norm_g: 345.8756 grad_norm_d: 30.1594
======> Epoch: 1497
Train Epoch: 1498 [11.54%] G-Loss: 32.0538 D-Loss: 2.2386 Loss-g-fm: 9.4396 Loss-g-mel: 17.8538 Loss-g-dur: 1.2415 Loss-g-kl: 1.3106 lr: 0.0002 grad_norm_g: 775.1763 grad_norm_d: 57.3274
======> Epoch: 1498
Train Epoch: 1499 [7.69%] G-Loss: 32.0243 D-Loss: 2.2529 Loss-g-fm: 9.0004 Loss-g-mel: 17.6105 Loss-g-dur: 1.2959 Loss-g-kl: 1.4863 lr: 0.0002 grad_norm_g: 938.2807 grad_norm_d: 88.5841
======> Epoch: 1499
Train Epoch: 1500 [3.85%] G-Loss: 31.4155 D-Loss: 2.3261 Loss-g-fm: 8.7090 Loss-g-mel: 17.5428 Loss-g-dur: 1.2291 Loss-g-kl: 1.2771 lr: 0.0002 grad_norm_g: 363.4110 grad_norm_d: 60.0237
======> Epoch: 1500
Train Epoch: 1501 [0.00%] G-Loss: 31.0290 D-Loss: 2.2997 Loss-g-fm: 8.7787 Loss-g-mel: 17.0998 Loss-g-dur: 1.2075 Loss-g-kl: 1.2401 lr: 0.0002 grad_norm_g: 601.2931 grad_norm_d: 68.2424
Train Epoch: 1501 [96.15%] G-Loss: 31.0070 D-Loss: 2.2761 Loss-g-fm: 8.6814 Loss-g-mel: 17.5751 Loss-g-dur: 1.2347 Loss-g-kl: 1.3064 lr: 0.0002 grad_norm_g: 133.2633 grad_norm_d: 25.1496
======> Epoch: 1501
Train Epoch: 1502 [92.31%] G-Loss: 33.1798 D-Loss: 2.2391 Loss-g-fm: 9.6710 Loss-g-mel: 18.0482 Loss-g-dur: 1.3449 Loss-g-kl: 1.4355 lr: 0.0002 grad_norm_g: 655.5061 grad_norm_d: 65.2981
======> Epoch: 1502
Train Epoch: 1503 [88.46%] G-Loss: 31.4692 D-Loss: 2.2729 Loss-g-fm: 8.9214 Loss-g-mel: 17.4110 Loss-g-dur: 1.2376 Loss-g-kl: 1.3032 lr: 0.0002 grad_norm_g: 859.2179 grad_norm_d: 56.2866
======> Epoch: 1503
Train Epoch: 1504 [84.62%] G-Loss: 32.3762 D-Loss: 2.2353 Loss-g-fm: 9.3867 Loss-g-mel: 17.9318 Loss-g-dur: 1.2272 Loss-g-kl: 1.3164 lr: 0.0002 grad_norm_g: 71.4527 grad_norm_d: 54.5752
======> Epoch: 1504
Train Epoch: 1505 [80.77%] G-Loss: 32.6447 D-Loss: 2.0705 Loss-g-fm: 9.9060 Loss-g-mel: 17.1384 Loss-g-dur: 1.2639 Loss-g-kl: 1.6476 lr: 0.0002 grad_norm_g: 370.8684 grad_norm_d: 51.3129
======> Epoch: 1505
Train Epoch: 1506 [76.92%] G-Loss: 28.3759 D-Loss: 2.2618 Loss-g-fm: 8.6394 Loss-g-mel: 14.7633 Loss-g-dur: 1.0398 Loss-g-kl: 1.4043 lr: 0.0002 grad_norm_g: 311.4243 grad_norm_d: 43.6900
======> Epoch: 1506
Train Epoch: 1507 [73.08%] G-Loss: 33.4755 D-Loss: 2.1926 Loss-g-fm: 9.8640 Loss-g-mel: 18.1277 Loss-g-dur: 1.3115 Loss-g-kl: 1.5127 lr: 0.0002 grad_norm_g: 634.7125 grad_norm_d: 47.6311
======> Epoch: 1507
Train Epoch: 1508 [69.23%] G-Loss: 34.8989 D-Loss: 2.1360 Loss-g-fm: 10.7110 Loss-g-mel: 18.6348 Loss-g-dur: 1.3263 Loss-g-kl: 1.5282 lr: 0.0002 grad_norm_g: 1029.0243 grad_norm_d: 53.0679
======> Epoch: 1508
Train Epoch: 1509 [65.38%] G-Loss: 28.6220 D-Loss: 2.3003 Loss-g-fm: 8.9049 Loss-g-mel: 14.7935 Loss-g-dur: 1.0734 Loss-g-kl: 1.3522 lr: 0.0002 grad_norm_g: 202.8167 grad_norm_d: 46.6243
======> Epoch: 1509
Train Epoch: 1510 [61.54%] G-Loss: 34.9045 D-Loss: 2.2642 Loss-g-fm: 10.3330 Loss-g-mel: 19.1045 Loss-g-dur: 1.3529 Loss-g-kl: 1.5343 lr: 0.0002 grad_norm_g: 328.6191 grad_norm_d: 38.3161
======> Epoch: 1510
Train Epoch: 1511 [57.69%] G-Loss: 20.0282 D-Loss: 2.3063 Loss-g-fm: 6.3900 Loss-g-mel: 8.7612 Loss-g-dur: 0.7362 Loss-g-kl: 1.4360 lr: 0.0002 grad_norm_g: 835.8299 grad_norm_d: 46.6462
======> Epoch: 1511
Train Epoch: 1512 [53.85%] G-Loss: 34.3776 D-Loss: 2.1780 Loss-g-fm: 10.3334 Loss-g-mel: 18.8496 Loss-g-dur: 1.3904 Loss-g-kl: 1.4158 lr: 0.0002 grad_norm_g: 38.8078 grad_norm_d: 4.3466
======> Epoch: 1512
Train Epoch: 1513 [50.00%] G-Loss: 31.2468 D-Loss: 2.3005 Loss-g-fm: 8.6139 Loss-g-mel: 17.4754 Loss-g-dur: 1.2504 Loss-g-kl: 1.4209 lr: 0.0002 grad_norm_g: 96.2262 grad_norm_d: 10.9966
======> Epoch: 1513
Train Epoch: 1514 [46.15%] G-Loss: 35.4779 D-Loss: 2.1726 Loss-g-fm: 10.8261 Loss-g-mel: 19.0860 Loss-g-dur: 1.3681 Loss-g-kl: 1.5229 lr: 0.0002 grad_norm_g: 560.5962 grad_norm_d: 27.0979
======> Epoch: 1514
Train Epoch: 1515 [42.31%] G-Loss: 34.0550 D-Loss: 2.0671 Loss-g-fm: 10.6880 Loss-g-mel: 18.1450 Loss-g-dur: 1.2319 Loss-g-kl: 1.2713 lr: 0.0002 grad_norm_g: 947.6129 grad_norm_d: 54.0446
======> Epoch: 1515
Train Epoch: 1516 [38.46%] G-Loss: 30.0762 D-Loss: 2.4925 Loss-g-fm: 7.9872 Loss-g-mel: 16.5444 Loss-g-dur: 1.2454 Loss-g-kl: 1.4531 lr: 0.0002 grad_norm_g: 628.1514 grad_norm_d: 90.4296
======> Epoch: 1516
Train Epoch: 1517 [34.62%] G-Loss: 28.0425 D-Loss: 2.5870 Loss-g-fm: 7.0560 Loss-g-mel: 16.1343 Loss-g-dur: 1.2178 Loss-g-kl: 1.2390 lr: 0.0002 grad_norm_g: 361.2681 grad_norm_d: 74.8413
======> Epoch: 1517
Train Epoch: 1518 [30.77%] G-Loss: 32.6663 D-Loss: 2.1935 Loss-g-fm: 9.2564 Loss-g-mel: 17.9202 Loss-g-dur: 1.3141 Loss-g-kl: 1.5293 lr: 0.0002 grad_norm_g: 650.8505 grad_norm_d: 24.7721
======> Epoch: 1518
Train Epoch: 1519 [26.92%] G-Loss: 32.7777 D-Loss: 2.2468 Loss-g-fm: 9.8480 Loss-g-mel: 17.7299 Loss-g-dur: 1.2346 Loss-g-kl: 1.2150 lr: 0.0002 grad_norm_g: 694.4359 grad_norm_d: 51.0051
======> Epoch: 1519
Train Epoch: 1520 [23.08%] G-Loss: 30.6014 D-Loss: 2.3364 Loss-g-fm: 8.6349 Loss-g-mel: 16.9882 Loss-g-dur: 1.3789 Loss-g-kl: 1.3100 lr: 0.0002 grad_norm_g: 134.3982 grad_norm_d: 8.1597
======> Epoch: 1520
Train Epoch: 1521 [19.23%] G-Loss: 31.9187 D-Loss: 2.2483 Loss-g-fm: 9.3196 Loss-g-mel: 17.7635 Loss-g-dur: 1.2202 Loss-g-kl: 1.2920 lr: 0.0002 grad_norm_g: 708.0645 grad_norm_d: 60.4224
======> Epoch: 1521
Train Epoch: 1522 [15.38%] G-Loss: 27.7049 D-Loss: 2.1872 Loss-g-fm: 8.3749 Loss-g-mel: 13.9962 Loss-g-dur: 1.0726 Loss-g-kl: 1.5612 lr: 0.0002 grad_norm_g: 1013.6104 grad_norm_d: 64.1859
======> Epoch: 1522
Train Epoch: 1523 [11.54%] G-Loss: 30.2922 D-Loss: 2.3536 Loss-g-fm: 8.1139 Loss-g-mel: 16.6622 Loss-g-dur: 1.2030 Loss-g-kl: 1.4931 lr: 0.0002 grad_norm_g: 415.5228 grad_norm_d: 87.2065
======> Epoch: 1523
Train Epoch: 1524 [7.69%] G-Loss: 32.1385 D-Loss: 2.3002 Loss-g-fm: 9.1830 Loss-g-mel: 17.8433 Loss-g-dur: 1.3353 Loss-g-kl: 1.3985 lr: 0.0002 grad_norm_g: 589.6043 grad_norm_d: 44.3038
======> Epoch: 1524
Train Epoch: 1525 [3.85%] G-Loss: 34.7521 D-Loss: 2.1870 Loss-g-fm: 10.4345 Loss-g-mel: 18.6683 Loss-g-dur: 1.3287 Loss-g-kl: 1.5669 lr: 0.0002 grad_norm_g: 989.5687 grad_norm_d: 63.2351
======> Epoch: 1525
Train Epoch: 1526 [0.00%] G-Loss: 30.3145 D-Loss: 2.2479 Loss-g-fm: 8.0012 Loss-g-mel: 17.0175 Loss-g-dur: 1.2180 Loss-g-kl: 1.3449 lr: 0.0002 grad_norm_g: 749.9910 grad_norm_d: 73.0392
Train Epoch: 1526 [96.15%] G-Loss: 33.5502 D-Loss: 2.2069 Loss-g-fm: 9.9648 Loss-g-mel: 18.0932 Loss-g-dur: 1.3060 Loss-g-kl: 1.6518 lr: 0.0002 grad_norm_g: 44.2859 grad_norm_d: 22.8414
======> Epoch: 1526
Train Epoch: 1527 [92.31%] G-Loss: 30.5046 D-Loss: 2.3367 Loss-g-fm: 8.3429 Loss-g-mel: 16.8589 Loss-g-dur: 1.2453 Loss-g-kl: 1.5812 lr: 0.0002 grad_norm_g: 836.9403 grad_norm_d: 67.5580
======> Epoch: 1527
Train Epoch: 1528 [88.46%] G-Loss: 35.2630 D-Loss: 2.0960 Loss-g-fm: 10.6700 Loss-g-mel: 18.7942 Loss-g-dur: 1.3307 Loss-g-kl: 1.7576 lr: 0.0002 grad_norm_g: 832.3459 grad_norm_d: 45.4148
======> Epoch: 1528
Train Epoch: 1529 [84.62%] G-Loss: 31.0118 D-Loss: 2.2685 Loss-g-fm: 8.7356 Loss-g-mel: 17.2600 Loss-g-dur: 1.2756 Loss-g-kl: 1.4550 lr: 0.0002 grad_norm_g: 657.4943 grad_norm_d: 33.9449
======> Epoch: 1529
Train Epoch: 1530 [80.77%] G-Loss: 32.2925 D-Loss: 2.1429 Loss-g-fm: 9.2863 Loss-g-mel: 17.7207 Loss-g-dur: 1.2395 Loss-g-kl: 1.3850 lr: 0.0002 grad_norm_g: 753.8288 grad_norm_d: 51.6133
======> Epoch: 1530
Train Epoch: 1531 [76.92%] G-Loss: 28.4699 D-Loss: 2.1523 Loss-g-fm: 8.7385 Loss-g-mel: 14.7834 Loss-g-dur: 1.0455 Loss-g-kl: 1.3596 lr: 0.0002 grad_norm_g: 796.9732 grad_norm_d: 42.1803
======> Epoch: 1531
Train Epoch: 1532 [73.08%] G-Loss: 31.7311 D-Loss: 2.1986 Loss-g-fm: 9.3385 Loss-g-mel: 16.9064 Loss-g-dur: 1.2418 Loss-g-kl: 1.5583 lr: 0.0002 grad_norm_g: 996.8016 grad_norm_d: 71.7841
======> Epoch: 1532
Train Epoch: 1533 [69.23%] G-Loss: 31.6567 D-Loss: 2.3045 Loss-g-fm: 9.1683 Loss-g-mel: 17.5365 Loss-g-dur: 1.2582 Loss-g-kl: 1.1766 lr: 0.0002 grad_norm_g: 830.8199 grad_norm_d: 90.5511
======> Epoch: 1533
Train Epoch: 1534 [65.38%] G-Loss: 32.9345 D-Loss: 2.2555 Loss-g-fm: 9.8842 Loss-g-mel: 17.9308 Loss-g-dur: 1.2518 Loss-g-kl: 1.3122 lr: 0.0002 grad_norm_g: 865.0982 grad_norm_d: 97.8084
======> Epoch: 1534
Train Epoch: 1535 [61.54%] G-Loss: 33.6372 D-Loss: 2.2476 Loss-g-fm: 9.7500 Loss-g-mel: 18.1285 Loss-g-dur: 1.2903 Loss-g-kl: 1.4867 lr: 0.0002 grad_norm_g: 556.5161 grad_norm_d: 49.8730
======> Epoch: 1535
Train Epoch: 1536 [57.69%] G-Loss: 31.5347 D-Loss: 2.2749 Loss-g-fm: 9.0634 Loss-g-mel: 17.2448 Loss-g-dur: 1.2364 Loss-g-kl: 1.4672 lr: 0.0002 grad_norm_g: 101.7390 grad_norm_d: 18.8119
======> Epoch: 1536
Train Epoch: 1537 [53.85%] G-Loss: 33.7517 D-Loss: 2.1836 Loss-g-fm: 9.9993 Loss-g-mel: 18.3762 Loss-g-dur: 1.3160 Loss-g-kl: 1.5920 lr: 0.0002 grad_norm_g: 503.0041 grad_norm_d: 18.1645
======> Epoch: 1537
Train Epoch: 1538 [50.00%] G-Loss: 32.4435 D-Loss: 2.1588 Loss-g-fm: 9.3539 Loss-g-mel: 17.4491 Loss-g-dur: 1.2017 Loss-g-kl: 1.4302 lr: 0.0002 grad_norm_g: 703.9008 grad_norm_d: 45.4349
======> Epoch: 1538
Train Epoch: 1539 [46.15%] G-Loss: 32.5120 D-Loss: 2.1938 Loss-g-fm: 9.8434 Loss-g-mel: 17.3907 Loss-g-dur: 1.2330 Loss-g-kl: 1.3789 lr: 0.0002 grad_norm_g: 843.6345 grad_norm_d: 50.0888
Saving model and optimizer state at iteration 1539 to /ZFS4T/tts/data/VITS/model_saved/G_40000.pth
Saving model and optimizer state at iteration 1539 to /ZFS4T/tts/data/VITS/model_saved/D_40000.pth
======> Epoch: 1539
Train Epoch: 1540 [42.31%] G-Loss: 33.7828 D-Loss: 2.1367 Loss-g-fm: 10.1414 Loss-g-mel: 18.1583 Loss-g-dur: 1.2722 Loss-g-kl: 1.5048 lr: 0.0002 grad_norm_g: 830.0621 grad_norm_d: 46.7676
======> Epoch: 1540
Train Epoch: 1541 [38.46%] G-Loss: 21.9063 D-Loss: 2.2465 Loss-g-fm: 7.6727 Loss-g-mel: 9.3654 Loss-g-dur: 0.7790 Loss-g-kl: 1.4305 lr: 0.0002 grad_norm_g: 1266.4169 grad_norm_d: 47.6191
======> Epoch: 1541
Train Epoch: 1542 [34.62%] G-Loss: 31.5035 D-Loss: 2.2291 Loss-g-fm: 8.9831 Loss-g-mel: 17.4262 Loss-g-dur: 1.2167 Loss-g-kl: 1.3153 lr: 0.0002 grad_norm_g: 733.6895 grad_norm_d: 95.7189
======> Epoch: 1542
Train Epoch: 1543 [30.77%] G-Loss: 33.7155 D-Loss: 2.3118 Loss-g-fm: 9.7308 Loss-g-mel: 18.6105 Loss-g-dur: 1.3392 Loss-g-kl: 1.3887 lr: 0.0002 grad_norm_g: 549.2539 grad_norm_d: 78.5737
======> Epoch: 1543
Train Epoch: 1544 [26.92%] G-Loss: 30.2844 D-Loss: 2.2432 Loss-g-fm: 8.5351 Loss-g-mel: 16.6321 Loss-g-dur: 1.2339 Loss-g-kl: 1.2018 lr: 0.0002 grad_norm_g: 791.2099 grad_norm_d: 68.8308
======> Epoch: 1544
Train Epoch: 1545 [23.08%] G-Loss: 33.5170 D-Loss: 2.1898 Loss-g-fm: 10.2851 Loss-g-mel: 17.7776 Loss-g-dur: 1.2122 Loss-g-kl: 1.5970 lr: 0.0002 grad_norm_g: 686.0780 grad_norm_d: 46.2763
======> Epoch: 1545
Train Epoch: 1546 [19.23%] G-Loss: 34.5091 D-Loss: 2.1014 Loss-g-fm: 10.4763 Loss-g-mel: 18.4538 Loss-g-dur: 1.3206 Loss-g-kl: 1.5963 lr: 0.0002 grad_norm_g: 834.8116 grad_norm_d: 55.5623
======> Epoch: 1546
Train Epoch: 1547 [15.38%] G-Loss: 33.6990 D-Loss: 2.1332 Loss-g-fm: 10.0261 Loss-g-mel: 18.0381 Loss-g-dur: 1.3155 Loss-g-kl: 1.5666 lr: 0.0002 grad_norm_g: 951.7611 grad_norm_d: 61.4932
======> Epoch: 1547
Train Epoch: 1548 [11.54%] G-Loss: 32.0753 D-Loss: 2.2776 Loss-g-fm: 9.6600 Loss-g-mel: 17.1160 Loss-g-dur: 1.2789 Loss-g-kl: 1.3333 lr: 0.0002 grad_norm_g: 978.0858 grad_norm_d: 75.6673
======> Epoch: 1548
Train Epoch: 1549 [7.69%] G-Loss: 32.2314 D-Loss: 2.3125 Loss-g-fm: 9.3616 Loss-g-mel: 17.8757 Loss-g-dur: 1.2445 Loss-g-kl: 1.2048 lr: 0.0002 grad_norm_g: 240.8887 grad_norm_d: 84.1865
======> Epoch: 1549
Train Epoch: 1550 [3.85%] G-Loss: 32.1884 D-Loss: 2.2841 Loss-g-fm: 9.8571 Loss-g-mel: 17.2311 Loss-g-dur: 1.2898 Loss-g-kl: 1.4460 lr: 0.0002 grad_norm_g: 98.9151 grad_norm_d: 7.7967
======> Epoch: 1550
Train Epoch: 1551 [0.00%] G-Loss: 34.8287 D-Loss: 2.2561 Loss-g-fm: 10.8063 Loss-g-mel: 18.7095 Loss-g-dur: 1.3333 Loss-g-kl: 1.3634 lr: 0.0002 grad_norm_g: 596.3951 grad_norm_d: 39.4402
Train Epoch: 1551 [96.15%] G-Loss: 33.6970 D-Loss: 2.0691 Loss-g-fm: 10.1910 Loss-g-mel: 18.2541 Loss-g-dur: 1.2654 Loss-g-kl: 1.3782 lr: 0.0002 grad_norm_g: 576.7765 grad_norm_d: 30.4423
======> Epoch: 1551
Train Epoch: 1552 [92.31%] G-Loss: 31.4797 D-Loss: 2.2666 Loss-g-fm: 8.7761 Loss-g-mel: 17.4347 Loss-g-dur: 1.1948 Loss-g-kl: 1.4635 lr: 0.0002 grad_norm_g: 545.5072 grad_norm_d: 11.3764
======> Epoch: 1552
Train Epoch: 1553 [88.46%] G-Loss: 31.1199 D-Loss: 2.2602 Loss-g-fm: 8.6981 Loss-g-mel: 17.3098 Loss-g-dur: 1.2280 Loss-g-kl: 1.2685 lr: 0.0002 grad_norm_g: 290.5496 grad_norm_d: 24.3049
======> Epoch: 1553
Train Epoch: 1554 [84.62%] G-Loss: 21.5354 D-Loss: 2.4096 Loss-g-fm: 7.2170 Loss-g-mel: 9.2967 Loss-g-dur: 0.7564 Loss-g-kl: 1.3898 lr: 0.0002 grad_norm_g: 1456.5575 grad_norm_d: 47.6094
======> Epoch: 1554
Train Epoch: 1555 [80.77%] G-Loss: 33.2692 D-Loss: 2.2412 Loss-g-fm: 10.0666 Loss-g-mel: 17.7763 Loss-g-dur: 1.2189 Loss-g-kl: 1.4149 lr: 0.0002 grad_norm_g: 513.4443 grad_norm_d: 51.5691
======> Epoch: 1555
Train Epoch: 1556 [76.92%] G-Loss: 34.8860 D-Loss: 2.2068 Loss-g-fm: 10.9041 Loss-g-mel: 18.5261 Loss-g-dur: 1.2716 Loss-g-kl: 1.4780 lr: 0.0002 grad_norm_g: 1039.7253 grad_norm_d: 58.6993
======> Epoch: 1556
Train Epoch: 1557 [73.08%] G-Loss: 32.6507 D-Loss: 2.2161 Loss-g-fm: 9.5405 Loss-g-mel: 17.6056 Loss-g-dur: 1.3090 Loss-g-kl: 1.3862 lr: 0.0002 grad_norm_g: 642.3501 grad_norm_d: 66.3163
======> Epoch: 1557
Train Epoch: 1558 [69.23%] G-Loss: 33.7769 D-Loss: 2.2358 Loss-g-fm: 9.7896 Loss-g-mel: 18.5413 Loss-g-dur: 1.3205 Loss-g-kl: 1.6210 lr: 0.0002 grad_norm_g: 535.7371 grad_norm_d: 56.5097
======> Epoch: 1558
Train Epoch: 1559 [65.38%] G-Loss: 33.9159 D-Loss: 2.1471 Loss-g-fm: 10.5127 Loss-g-mel: 17.8386 Loss-g-dur: 1.3067 Loss-g-kl: 1.4575 lr: 0.0002 grad_norm_g: 681.5259 grad_norm_d: 40.2876
======> Epoch: 1559
Train Epoch: 1560 [61.54%] G-Loss: 31.1790 D-Loss: 2.1737 Loss-g-fm: 8.8146 Loss-g-mel: 17.2566 Loss-g-dur: 1.2305 Loss-g-kl: 1.2307 lr: 0.0002 grad_norm_g: 365.5442 grad_norm_d: 50.6054
======> Epoch: 1560
Train Epoch: 1561 [57.69%] G-Loss: 21.1985 D-Loss: 2.0399 Loss-g-fm: 7.6589 Loss-g-mel: 8.9527 Loss-g-dur: 0.7918 Loss-g-kl: 1.3793 lr: 0.0002 grad_norm_g: 1180.7468 grad_norm_d: 37.4359
======> Epoch: 1561
Train Epoch: 1562 [53.85%] G-Loss: 31.7729 D-Loss: 2.2896 Loss-g-fm: 9.0198 Loss-g-mel: 17.6100 Loss-g-dur: 1.2549 Loss-g-kl: 1.2669 lr: 0.0002 grad_norm_g: 449.6910 grad_norm_d: 14.0886
======> Epoch: 1562
Train Epoch: 1563 [50.00%] G-Loss: 32.7594 D-Loss: 2.3036 Loss-g-fm: 10.0504 Loss-g-mel: 17.1907 Loss-g-dur: 1.2308 Loss-g-kl: 1.3036 lr: 0.0002 grad_norm_g: 871.0316 grad_norm_d: 75.9002
======> Epoch: 1563
Train Epoch: 1564 [46.15%] G-Loss: 32.9610 D-Loss: 2.4312 Loss-g-fm: 9.4700 Loss-g-mel: 18.1876 Loss-g-dur: 1.2985 Loss-g-kl: 1.5412 lr: 0.0002 grad_norm_g: 535.8820 grad_norm_d: 59.4970
======> Epoch: 1564
Train Epoch: 1565 [42.31%] G-Loss: 34.4303 D-Loss: 2.1354 Loss-g-fm: 10.2930 Loss-g-mel: 18.3082 Loss-g-dur: 1.4165 Loss-g-kl: 1.5255 lr: 0.0002 grad_norm_g: 422.1036 grad_norm_d: 6.7287
======> Epoch: 1565
Train Epoch: 1566 [38.46%] G-Loss: 31.5690 D-Loss: 2.1933 Loss-g-fm: 9.3345 Loss-g-mel: 17.1099 Loss-g-dur: 1.3268 Loss-g-kl: 1.4695 lr: 0.0002 grad_norm_g: 711.4295 grad_norm_d: 64.9325
======> Epoch: 1566
Train Epoch: 1567 [34.62%] G-Loss: 33.2704 D-Loss: 2.0677 Loss-g-fm: 9.8906 Loss-g-mel: 18.0310 Loss-g-dur: 1.2273 Loss-g-kl: 1.2443 lr: 0.0002 grad_norm_g: 318.1586 grad_norm_d: 64.4509
======> Epoch: 1567
Train Epoch: 1568 [30.77%] G-Loss: 31.0697 D-Loss: 2.2633 Loss-g-fm: 8.9912 Loss-g-mel: 16.9018 Loss-g-dur: 1.2972 Loss-g-kl: 1.3356 lr: 0.0002 grad_norm_g: 458.9656 grad_norm_d: 20.0748
======> Epoch: 1568
Train Epoch: 1569 [26.92%] G-Loss: 31.6408 D-Loss: 2.4383 Loss-g-fm: 8.9280 Loss-g-mel: 17.4770 Loss-g-dur: 1.3081 Loss-g-kl: 1.5915 lr: 0.0002 grad_norm_g: 364.6378 grad_norm_d: 11.6759
======> Epoch: 1569
Train Epoch: 1570 [23.08%] G-Loss: 33.2729 D-Loss: 2.1643 Loss-g-fm: 10.0250 Loss-g-mel: 18.0701 Loss-g-dur: 1.2475 Loss-g-kl: 1.4138 lr: 0.0002 grad_norm_g: 688.4953 grad_norm_d: 40.8411
======> Epoch: 1570
Train Epoch: 1571 [19.23%] G-Loss: 19.7913 D-Loss: 2.5522 Loss-g-fm: 5.9442 Loss-g-mel: 9.1771 Loss-g-dur: 0.7934 Loss-g-kl: 1.4233 lr: 0.0002 grad_norm_g: 1091.0314 grad_norm_d: 57.0452
======> Epoch: 1571
Train Epoch: 1572 [15.38%] G-Loss: 34.0023 D-Loss: 2.2644 Loss-g-fm: 10.1107 Loss-g-mel: 18.0253 Loss-g-dur: 1.4304 Loss-g-kl: 1.5988 lr: 0.0002 grad_norm_g: 554.1393 grad_norm_d: 22.4782
======> Epoch: 1572
Train Epoch: 1573 [11.54%] G-Loss: 32.7808 D-Loss: 2.2012 Loss-g-fm: 10.4069 Loss-g-mel: 17.1521 Loss-g-dur: 1.2649 Loss-g-kl: 1.3937 lr: 0.0002 grad_norm_g: 862.8156 grad_norm_d: 53.5503
======> Epoch: 1573
Train Epoch: 1574 [7.69%] G-Loss: 34.2141 D-Loss: 2.3459 Loss-g-fm: 10.4307 Loss-g-mel: 18.0778 Loss-g-dur: 1.2952 Loss-g-kl: 1.6232 lr: 0.0002 grad_norm_g: 783.2744 grad_norm_d: 52.5544
======> Epoch: 1574
Train Epoch: 1575 [3.85%] G-Loss: 34.1872 D-Loss: 2.1134 Loss-g-fm: 10.8072 Loss-g-mel: 18.0853 Loss-g-dur: 1.3088 Loss-g-kl: 1.4189 lr: 0.0002 grad_norm_g: 491.5193 grad_norm_d: 31.1026
======> Epoch: 1575
Train Epoch: 1576 [0.00%] G-Loss: 33.0401 D-Loss: 2.2730 Loss-g-fm: 10.1348 Loss-g-mel: 17.5785 Loss-g-dur: 1.2303 Loss-g-kl: 1.3033 lr: 0.0002 grad_norm_g: 674.2077 grad_norm_d: 52.5475
Train Epoch: 1576 [96.15%] G-Loss: 32.5243 D-Loss: 2.0564 Loss-g-fm: 9.7387 Loss-g-mel: 17.3757 Loss-g-dur: 1.2253 Loss-g-kl: 1.4840 lr: 0.0002 grad_norm_g: 158.6050 grad_norm_d: 15.2168
======> Epoch: 1576
Train Epoch: 1577 [92.31%] G-Loss: 30.2858 D-Loss: 2.2255 Loss-g-fm: 8.7353 Loss-g-mel: 16.7663 Loss-g-dur: 1.2188 Loss-g-kl: 1.2208 lr: 0.0002 grad_norm_g: 715.1388 grad_norm_d: 67.9828
======> Epoch: 1577
Train Epoch: 1578 [88.46%] G-Loss: 35.8329 D-Loss: 2.0291 Loss-g-fm: 11.7643 Loss-g-mel: 18.3508 Loss-g-dur: 1.2954 Loss-g-kl: 1.4906 lr: 0.0002 grad_norm_g: 767.0581 grad_norm_d: 19.6979
======> Epoch: 1578
Train Epoch: 1579 [84.62%] G-Loss: 34.5928 D-Loss: 2.0914 Loss-g-fm: 10.6208 Loss-g-mel: 18.4553 Loss-g-dur: 1.2963 Loss-g-kl: 1.4837 lr: 0.0002 grad_norm_g: 666.3601 grad_norm_d: 71.0692
======> Epoch: 1579
Train Epoch: 1580 [80.77%] G-Loss: 33.5920 D-Loss: 2.2092 Loss-g-fm: 10.3849 Loss-g-mel: 17.7272 Loss-g-dur: 1.3331 Loss-g-kl: 1.4739 lr: 0.0002 grad_norm_g: 700.6948 grad_norm_d: 49.7240
======> Epoch: 1580
Train Epoch: 1581 [76.92%] G-Loss: 21.0670 D-Loss: 2.1872 Loss-g-fm: 7.2129 Loss-g-mel: 8.9931 Loss-g-dur: 0.7203 Loss-g-kl: 1.4093 lr: 0.0002 grad_norm_g: 1351.5364 grad_norm_d: 40.1140
======> Epoch: 1581
Train Epoch: 1582 [73.08%] G-Loss: 34.8354 D-Loss: 2.1231 Loss-g-fm: 11.1135 Loss-g-mel: 18.2015 Loss-g-dur: 1.3202 Loss-g-kl: 1.5186 lr: 0.0002 grad_norm_g: 882.0628 grad_norm_d: 51.9801
======> Epoch: 1582
Train Epoch: 1583 [69.23%] G-Loss: 32.4827 D-Loss: 2.4054 Loss-g-fm: 9.6337 Loss-g-mel: 17.4960 Loss-g-dur: 1.1966 Loss-g-kl: 1.6060 lr: 0.0002 grad_norm_g: 871.6230 grad_norm_d: 57.5462
======> Epoch: 1583
Train Epoch: 1584 [65.38%] G-Loss: 31.0678 D-Loss: 2.2618 Loss-g-fm: 8.9938 Loss-g-mel: 16.9694 Loss-g-dur: 1.2394 Loss-g-kl: 1.2836 lr: 0.0002 grad_norm_g: 615.5373 grad_norm_d: 27.9849
======> Epoch: 1584
Train Epoch: 1585 [61.54%] G-Loss: 33.0978 D-Loss: 2.2032 Loss-g-fm: 10.3351 Loss-g-mel: 17.4090 Loss-g-dur: 1.2447 Loss-g-kl: 1.4208 lr: 0.0002 grad_norm_g: 1067.7023 grad_norm_d: 82.7761
======> Epoch: 1585
Train Epoch: 1586 [57.69%] G-Loss: 33.0622 D-Loss: 2.1703 Loss-g-fm: 9.7478 Loss-g-mel: 18.1671 Loss-g-dur: 1.1902 Loss-g-kl: 1.1099 lr: 0.0002 grad_norm_g: 830.1534 grad_norm_d: 81.2031
======> Epoch: 1586
Train Epoch: 1587 [53.85%] G-Loss: 32.7503 D-Loss: 2.1496 Loss-g-fm: 9.4645 Loss-g-mel: 17.9944 Loss-g-dur: 1.2203 Loss-g-kl: 1.3578 lr: 0.0002 grad_norm_g: 765.9947 grad_norm_d: 48.1938
======> Epoch: 1587
Train Epoch: 1588 [50.00%] G-Loss: 30.9642 D-Loss: 2.1990 Loss-g-fm: 8.5970 Loss-g-mel: 17.0402 Loss-g-dur: 1.2376 Loss-g-kl: 1.3510 lr: 0.0002 grad_norm_g: 781.4883 grad_norm_d: 49.4162
======> Epoch: 1588
Train Epoch: 1589 [46.15%] G-Loss: 19.8416 D-Loss: 2.4014 Loss-g-fm: 5.9040 Loss-g-mel: 9.1520 Loss-g-dur: 0.7463 Loss-g-kl: 1.4540 lr: 0.0002 grad_norm_g: 1100.5708 grad_norm_d: 34.8872
======> Epoch: 1589
Train Epoch: 1590 [42.31%] G-Loss: 30.3060 D-Loss: 2.0929 Loss-g-fm: 9.7741 Loss-g-mel: 15.1698 Loss-g-dur: 1.0671 Loss-g-kl: 1.5369 lr: 0.0002 grad_norm_g: 912.3063 grad_norm_d: 115.5430
======> Epoch: 1590
Train Epoch: 1591 [38.46%] G-Loss: 33.6003 D-Loss: 2.3717 Loss-g-fm: 9.9776 Loss-g-mel: 17.9718 Loss-g-dur: 1.3049 Loss-g-kl: 1.6196 lr: 0.0002 grad_norm_g: 653.9257 grad_norm_d: 70.3216
======> Epoch: 1591
Train Epoch: 1592 [34.62%] G-Loss: 32.4511 D-Loss: 2.2689 Loss-g-fm: 9.2371 Loss-g-mel: 17.7241 Loss-g-dur: 1.2277 Loss-g-kl: 1.2802 lr: 0.0002 grad_norm_g: 473.2531 grad_norm_d: 33.2115
======> Epoch: 1592
Train Epoch: 1593 [30.77%] G-Loss: 32.8337 D-Loss: 2.0428 Loss-g-fm: 9.8318 Loss-g-mel: 17.7033 Loss-g-dur: 1.2259 Loss-g-kl: 1.3161 lr: 0.0002 grad_norm_g: 736.9093 grad_norm_d: 49.4344
======> Epoch: 1593
Train Epoch: 1594 [26.92%] G-Loss: 30.3430 D-Loss: 2.3218 Loss-g-fm: 8.6380 Loss-g-mel: 16.5722 Loss-g-dur: 1.2747 Loss-g-kl: 1.3962 lr: 0.0002 grad_norm_g: 772.9118 grad_norm_d: 42.8078
======> Epoch: 1594
Train Epoch: 1595 [23.08%] G-Loss: 33.8266 D-Loss: 2.1860 Loss-g-fm: 10.1275 Loss-g-mel: 17.8407 Loss-g-dur: 1.3888 Loss-g-kl: 1.4864 lr: 0.0002 grad_norm_g: 314.5413 grad_norm_d: 39.2084
======> Epoch: 1595
Train Epoch: 1596 [19.23%] G-Loss: 31.7796 D-Loss: 2.2482 Loss-g-fm: 9.0179 Loss-g-mel: 17.2439 Loss-g-dur: 1.2765 Loss-g-kl: 1.5544 lr: 0.0002 grad_norm_g: 698.0307 grad_norm_d: 67.9984
======> Epoch: 1596
Train Epoch: 1597 [15.38%] G-Loss: 32.1153 D-Loss: 2.2634 Loss-g-fm: 8.7627 Loss-g-mel: 17.8192 Loss-g-dur: 1.2514 Loss-g-kl: 1.3306 lr: 0.0002 grad_norm_g: 803.7170 grad_norm_d: 64.5142
======> Epoch: 1597
Train Epoch: 1598 [11.54%] G-Loss: 31.7396 D-Loss: 2.2154 Loss-g-fm: 9.4696 Loss-g-mel: 17.0523 Loss-g-dur: 1.2338 Loss-g-kl: 1.2630 lr: 0.0002 grad_norm_g: 185.3858 grad_norm_d: 23.6841
======> Epoch: 1598
Train Epoch: 1599 [7.69%] G-Loss: 35.4825 D-Loss: 2.1207 Loss-g-fm: 11.1617 Loss-g-mel: 18.5453 Loss-g-dur: 1.3128 Loss-g-kl: 1.6294 lr: 0.0002 grad_norm_g: 942.6989 grad_norm_d: 61.2411
======> Epoch: 1599
Train Epoch: 1600 [3.85%] G-Loss: 34.9110 D-Loss: 2.1991 Loss-g-fm: 10.8832 Loss-g-mel: 18.4657 Loss-g-dur: 1.3298 Loss-g-kl: 1.6094 lr: 0.0002 grad_norm_g: 180.6540 grad_norm_d: 25.2848
======> Epoch: 1600
Train Epoch: 1601 [0.00%] G-Loss: 33.8959 D-Loss: 2.1304 Loss-g-fm: 10.4195 Loss-g-mel: 17.7429 Loss-g-dur: 1.2146 Loss-g-kl: 1.4594 lr: 0.0002 grad_norm_g: 618.3922 grad_norm_d: 6.2621
Train Epoch: 1601 [96.15%] G-Loss: 19.8942 D-Loss: 2.4672 Loss-g-fm: 5.8083 Loss-g-mel: 9.3447 Loss-g-dur: 0.7463 Loss-g-kl: 1.4671 lr: 0.0002 grad_norm_g: 1460.5229 grad_norm_d: 82.6440
======> Epoch: 1601
Train Epoch: 1602 [92.31%] G-Loss: 36.3586 D-Loss: 2.1045 Loss-g-fm: 11.9192 Loss-g-mel: 19.1142 Loss-g-dur: 1.2928 Loss-g-kl: 1.4529 lr: 0.0002 grad_norm_g: 836.3284 grad_norm_d: 45.6274
======> Epoch: 1602
Train Epoch: 1603 [88.46%] G-Loss: 33.9671 D-Loss: 2.2678 Loss-g-fm: 10.2146 Loss-g-mel: 18.1155 Loss-g-dur: 1.3478 Loss-g-kl: 1.4854 lr: 0.0002 grad_norm_g: 614.1812 grad_norm_d: 44.8705
======> Epoch: 1603
Train Epoch: 1604 [84.62%] G-Loss: 32.7846 D-Loss: 2.1336 Loss-g-fm: 9.8948 Loss-g-mel: 17.6879 Loss-g-dur: 1.1955 Loss-g-kl: 1.2018 lr: 0.0002 grad_norm_g: 952.0848 grad_norm_d: 60.5553
======> Epoch: 1604
Train Epoch: 1605 [80.77%] G-Loss: 34.5863 D-Loss: 2.2278 Loss-g-fm: 10.9415 Loss-g-mel: 18.2032 Loss-g-dur: 1.3885 Loss-g-kl: 1.5322 lr: 0.0002 grad_norm_g: 950.5306 grad_norm_d: 39.5739
======> Epoch: 1605
Train Epoch: 1606 [76.92%] G-Loss: 30.1811 D-Loss: 2.3815 Loss-g-fm: 8.6133 Loss-g-mel: 16.4658 Loss-g-dur: 1.2563 Loss-g-kl: 1.4965 lr: 0.0002 grad_norm_g: 574.7514 grad_norm_d: 71.7072
======> Epoch: 1606
Train Epoch: 1607 [73.08%] G-Loss: 34.1418 D-Loss: 2.2003 Loss-g-fm: 10.5351 Loss-g-mel: 17.6873 Loss-g-dur: 1.3694 Loss-g-kl: 1.6343 lr: 0.0002 grad_norm_g: 815.9507 grad_norm_d: 52.2985
======> Epoch: 1607
Train Epoch: 1608 [69.23%] G-Loss: 31.6332 D-Loss: 2.2424 Loss-g-fm: 9.7357 Loss-g-mel: 16.5340 Loss-g-dur: 1.2106 Loss-g-kl: 1.3668 lr: 0.0002 grad_norm_g: 892.5830 grad_norm_d: 74.9866
======> Epoch: 1608
Train Epoch: 1609 [65.38%] G-Loss: 35.0066 D-Loss: 2.2174 Loss-g-fm: 11.1373 Loss-g-mel: 18.4460 Loss-g-dur: 1.2946 Loss-g-kl: 1.5778 lr: 0.0002 grad_norm_g: 604.3525 grad_norm_d: 66.3991
======> Epoch: 1609
Train Epoch: 1610 [61.54%] G-Loss: 30.3303 D-Loss: 2.0952 Loss-g-fm: 9.9186 Loss-g-mel: 15.0326 Loss-g-dur: 1.1244 Loss-g-kl: 1.6081 lr: 0.0002 grad_norm_g: 738.8591 grad_norm_d: 23.4951
======> Epoch: 1610
Train Epoch: 1611 [57.69%] G-Loss: 29.0417 D-Loss: 2.3002 Loss-g-fm: 9.2067 Loss-g-mel: 14.5651 Loss-g-dur: 1.1229 Loss-g-kl: 1.4505 lr: 0.0002 grad_norm_g: 1092.5318 grad_norm_d: 86.4312
======> Epoch: 1611
Train Epoch: 1612 [53.85%] G-Loss: 31.9315 D-Loss: 2.4148 Loss-g-fm: 9.3919 Loss-g-mel: 17.5424 Loss-g-dur: 1.1900 Loss-g-kl: 1.2204 lr: 0.0002 grad_norm_g: 348.0007 grad_norm_d: 14.6142
======> Epoch: 1612
Train Epoch: 1613 [50.00%] G-Loss: 19.9903 D-Loss: 2.3607 Loss-g-fm: 6.0437 Loss-g-mel: 9.0175 Loss-g-dur: 0.7306 Loss-g-kl: 1.3620 lr: 0.0002 grad_norm_g: 998.2906 grad_norm_d: 47.0490
======> Epoch: 1613
Train Epoch: 1614 [46.15%] G-Loss: 32.6687 D-Loss: 2.1897 Loss-g-fm: 9.8710 Loss-g-mel: 17.5878 Loss-g-dur: 1.2215 Loss-g-kl: 1.2596 lr: 0.0002 grad_norm_g: 506.0777 grad_norm_d: 14.7969
======> Epoch: 1614
Train Epoch: 1615 [42.31%] G-Loss: 35.6268 D-Loss: 1.9788 Loss-g-fm: 11.3039 Loss-g-mel: 18.5399 Loss-g-dur: 1.4601 Loss-g-kl: 1.4776 lr: 0.0002 grad_norm_g: 766.3838 grad_norm_d: 26.5522
======> Epoch: 1615
Train Epoch: 1616 [38.46%] G-Loss: 34.5094 D-Loss: 2.2578 Loss-g-fm: 10.3711 Loss-g-mel: 18.4042 Loss-g-dur: 1.3917 Loss-g-kl: 1.5862 lr: 0.0002 grad_norm_g: 699.0295 grad_norm_d: 42.3716
Saving model and optimizer state at iteration 1616 to /ZFS4T/tts/data/VITS/model_saved/G_42000.pth
Saving model and optimizer state at iteration 1616 to /ZFS4T/tts/data/VITS/model_saved/D_42000.pth
======> Epoch: 1616
Train Epoch: 1617 [34.62%] G-Loss: 35.3009 D-Loss: 2.1091 Loss-g-fm: 11.4080 Loss-g-mel: 18.1765 Loss-g-dur: 1.3056 Loss-g-kl: 1.5016 lr: 0.0002 grad_norm_g: 195.3982 grad_norm_d: 20.0907
======> Epoch: 1617
Train Epoch: 1618 [30.77%] G-Loss: 32.1865 D-Loss: 2.2782 Loss-g-fm: 9.4757 Loss-g-mel: 17.6177 Loss-g-dur: 1.2412 Loss-g-kl: 1.4646 lr: 0.0002 grad_norm_g: 730.4122 grad_norm_d: 33.3990
======> Epoch: 1618
Train Epoch: 1619 [26.92%] G-Loss: 32.5148 D-Loss: 2.1590 Loss-g-fm: 9.4085 Loss-g-mel: 17.9139 Loss-g-dur: 1.3245 Loss-g-kl: 1.4353 lr: 0.0002 grad_norm_g: 237.0616 grad_norm_d: 17.5186
======> Epoch: 1619
Train Epoch: 1620 [23.08%] G-Loss: 20.7080 D-Loss: 2.3595 Loss-g-fm: 7.0050 Loss-g-mel: 9.0203 Loss-g-dur: 0.7844 Loss-g-kl: 1.4999 lr: 0.0002 grad_norm_g: 1385.6647 grad_norm_d: 53.8905
======> Epoch: 1620
Train Epoch: 1621 [19.23%] G-Loss: 35.0601 D-Loss: 2.1553 Loss-g-fm: 10.8823 Loss-g-mel: 18.6626 Loss-g-dur: 1.3551 Loss-g-kl: 1.5662 lr: 0.0002 grad_norm_g: 161.4990 grad_norm_d: 27.7805
======> Epoch: 1621
Train Epoch: 1622 [15.38%] G-Loss: 32.9689 D-Loss: 2.1578 Loss-g-fm: 10.4106 Loss-g-mel: 17.3539 Loss-g-dur: 1.2220 Loss-g-kl: 1.1702 lr: 0.0002 grad_norm_g: 918.2446 grad_norm_d: 53.6148
======> Epoch: 1622
Train Epoch: 1623 [11.54%] G-Loss: 34.0579 D-Loss: 2.0933 Loss-g-fm: 10.7405 Loss-g-mel: 17.7157 Loss-g-dur: 1.3109 Loss-g-kl: 1.4280 lr: 0.0002 grad_norm_g: 841.1320 grad_norm_d: 44.0683
======> Epoch: 1623
Train Epoch: 1624 [7.69%] G-Loss: 33.1248 D-Loss: 2.3618 Loss-g-fm: 10.0987 Loss-g-mel: 17.6925 Loss-g-dur: 1.3133 Loss-g-kl: 1.4653 lr: 0.0002 grad_norm_g: 635.8259 grad_norm_d: 46.5726
======> Epoch: 1624
Train Epoch: 1625 [3.85%] G-Loss: 33.0092 D-Loss: 2.2782 Loss-g-fm: 10.2779 Loss-g-mel: 17.3824 Loss-g-dur: 1.2021 Loss-g-kl: 1.4241 lr: 0.0002 grad_norm_g: 454.3333 grad_norm_d: 11.5928
======> Epoch: 1625
Train Epoch: 1626 [0.00%] G-Loss: 38.3487 D-Loss: 2.2503 Loss-g-fm: 13.2199 Loss-g-mel: 19.2008 Loss-g-dur: 1.5070 Loss-g-kl: 1.8003 lr: 0.0002 grad_norm_g: 985.8620 grad_norm_d: 51.9457
Train Epoch: 1626 [96.15%] G-Loss: 30.0655 D-Loss: 2.3306 Loss-g-fm: 8.1931 Loss-g-mel: 16.7695 Loss-g-dur: 1.1905 Loss-g-kl: 1.2909 lr: 0.0002 grad_norm_g: 962.6434 grad_norm_d: 84.3518
======> Epoch: 1626
Train Epoch: 1627 [92.31%] G-Loss: 32.5255 D-Loss: 2.0622 Loss-g-fm: 9.8477 Loss-g-mel: 17.5066 Loss-g-dur: 1.2623 Loss-g-kl: 1.4204 lr: 0.0002 grad_norm_g: 844.1493 grad_norm_d: 72.5207
======> Epoch: 1627
Train Epoch: 1628 [88.46%] G-Loss: 32.8589 D-Loss: 2.1867 Loss-g-fm: 9.5390 Loss-g-mel: 17.8615 Loss-g-dur: 1.3102 Loss-g-kl: 1.1697 lr: 0.0002 grad_norm_g: 442.7034 grad_norm_d: 12.2675
======> Epoch: 1628
Train Epoch: 1629 [84.62%] G-Loss: 31.8910 D-Loss: 2.1739 Loss-g-fm: 9.4615 Loss-g-mel: 17.3544 Loss-g-dur: 1.2208 Loss-g-kl: 1.3589 lr: 0.0002 grad_norm_g: 844.7060 grad_norm_d: 43.2621
======> Epoch: 1629
Train Epoch: 1630 [80.77%] G-Loss: 33.4356 D-Loss: 2.2687 Loss-g-fm: 10.0002 Loss-g-mel: 17.8502 Loss-g-dur: 1.4050 Loss-g-kl: 1.7073 lr: 0.0002 grad_norm_g: 46.8208 grad_norm_d: 8.7047
======> Epoch: 1630
Train Epoch: 1631 [76.92%] G-Loss: 34.5551 D-Loss: 2.1756 Loss-g-fm: 10.5588 Loss-g-mel: 18.4471 Loss-g-dur: 1.2821 Loss-g-kl: 1.5968 lr: 0.0002 grad_norm_g: 830.1144 grad_norm_d: 36.4485
======> Epoch: 1631
Train Epoch: 1632 [73.08%] G-Loss: 34.2110 D-Loss: 2.2124 Loss-g-fm: 10.5548 Loss-g-mel: 18.1883 Loss-g-dur: 1.3635 Loss-g-kl: 1.5312 lr: 0.0002 grad_norm_g: 226.8500 grad_norm_d: 6.0496
======> Epoch: 1632
Train Epoch: 1633 [69.23%] G-Loss: 20.1054 D-Loss: 2.3210 Loss-g-fm: 6.3311 Loss-g-mel: 9.1227 Loss-g-dur: 0.7455 Loss-g-kl: 1.3849 lr: 0.0002 grad_norm_g: 1398.6300 grad_norm_d: 75.0398
======> Epoch: 1633
Train Epoch: 1634 [65.38%] G-Loss: 33.5731 D-Loss: 2.5798 Loss-g-fm: 10.1349 Loss-g-mel: 17.4400 Loss-g-dur: 1.2414 Loss-g-kl: 1.2699 lr: 0.0002 grad_norm_g: 814.7160 grad_norm_d: 80.3144
======> Epoch: 1634
Train Epoch: 1635 [61.54%] G-Loss: 31.6215 D-Loss: 2.2697 Loss-g-fm: 9.3170 Loss-g-mel: 17.2873 Loss-g-dur: 1.1833 Loss-g-kl: 1.3424 lr: 0.0002 grad_norm_g: 512.1865 grad_norm_d: 57.8150
======> Epoch: 1635
Train Epoch: 1636 [57.69%] G-Loss: 32.0642 D-Loss: 2.1864 Loss-g-fm: 9.2805 Loss-g-mel: 17.5149 Loss-g-dur: 1.2020 Loss-g-kl: 1.2766 lr: 0.0002 grad_norm_g: 764.9990 grad_norm_d: 63.7934
======> Epoch: 1636
Train Epoch: 1637 [53.85%] G-Loss: 21.0510 D-Loss: 2.2184 Loss-g-fm: 7.0194 Loss-g-mel: 9.1449 Loss-g-dur: 0.7047 Loss-g-kl: 1.3446 lr: 0.0002 grad_norm_g: 1127.1869 grad_norm_d: 29.4825
======> Epoch: 1637
Train Epoch: 1638 [50.00%] G-Loss: 34.1947 D-Loss: 2.1654 Loss-g-fm: 10.8505 Loss-g-mel: 17.7482 Loss-g-dur: 1.2489 Loss-g-kl: 1.4458 lr: 0.0002 grad_norm_g: 941.2681 grad_norm_d: 43.0379
======> Epoch: 1638
Train Epoch: 1639 [46.15%] G-Loss: 29.1903 D-Loss: 2.4561 Loss-g-fm: 7.7079 Loss-g-mel: 16.4054 Loss-g-dur: 1.2113 Loss-g-kl: 1.4325 lr: 0.0002 grad_norm_g: 361.0430 grad_norm_d: 7.4533
======> Epoch: 1639
Train Epoch: 1640 [42.31%] G-Loss: 31.9660 D-Loss: 2.2525 Loss-g-fm: 9.3874 Loss-g-mel: 17.2915 Loss-g-dur: 1.2299 Loss-g-kl: 1.3100 lr: 0.0002 grad_norm_g: 933.5701 grad_norm_d: 84.0474
======> Epoch: 1640
Train Epoch: 1641 [38.46%] G-Loss: 30.4316 D-Loss: 2.1553 Loss-g-fm: 10.1467 Loss-g-mel: 15.1091 Loss-g-dur: 1.0762 Loss-g-kl: 1.4601 lr: 0.0002 grad_norm_g: 294.8066 grad_norm_d: 30.8911
======> Epoch: 1641
Train Epoch: 1642 [34.62%] G-Loss: 33.0545 D-Loss: 2.3126 Loss-g-fm: 9.3552 Loss-g-mel: 17.7962 Loss-g-dur: 1.2867 Loss-g-kl: 1.6036 lr: 0.0002 grad_norm_g: 151.6678 grad_norm_d: 17.9308
======> Epoch: 1642
Train Epoch: 1643 [30.77%] G-Loss: 32.0804 D-Loss: 2.2496 Loss-g-fm: 9.3616 Loss-g-mel: 17.5219 Loss-g-dur: 1.2180 Loss-g-kl: 1.2708 lr: 0.0002 grad_norm_g: 765.6966 grad_norm_d: 64.6490
======> Epoch: 1643
Train Epoch: 1644 [26.92%] G-Loss: 33.3270 D-Loss: 2.2406 Loss-g-fm: 9.8192 Loss-g-mel: 18.1554 Loss-g-dur: 1.3203 Loss-g-kl: 1.5992 lr: 0.0002 grad_norm_g: 441.5926 grad_norm_d: 13.4933
======> Epoch: 1644
Train Epoch: 1645 [23.08%] G-Loss: 32.5069 D-Loss: 2.1291 Loss-g-fm: 9.4179 Loss-g-mel: 17.8292 Loss-g-dur: 1.2036 Loss-g-kl: 1.4431 lr: 0.0002 grad_norm_g: 403.3361 grad_norm_d: 4.2070
======> Epoch: 1645
Train Epoch: 1646 [19.23%] G-Loss: 33.5167 D-Loss: 2.1099 Loss-g-fm: 10.2044 Loss-g-mel: 17.6504 Loss-g-dur: 1.2203 Loss-g-kl: 1.5657 lr: 0.0002 grad_norm_g: 996.1542 grad_norm_d: 77.8126
======> Epoch: 1646
Train Epoch: 1647 [15.38%] G-Loss: 35.5516 D-Loss: 2.2210 Loss-g-fm: 10.9203 Loss-g-mel: 18.7349 Loss-g-dur: 1.3577 Loss-g-kl: 1.4994 lr: 0.0002 grad_norm_g: 94.4052 grad_norm_d: 21.1434
======> Epoch: 1647
Train Epoch: 1648 [11.54%] G-Loss: 28.5377 D-Loss: 2.2981 Loss-g-fm: 9.1861 Loss-g-mel: 14.2163 Loss-g-dur: 1.0339 Loss-g-kl: 1.5285 lr: 0.0002 grad_norm_g: 1195.6562 grad_norm_d: 124.7105
======> Epoch: 1648
Train Epoch: 1649 [7.69%] G-Loss: 33.5123 D-Loss: 2.3308 Loss-g-fm: 9.9829 Loss-g-mel: 18.1241 Loss-g-dur: 1.2541 Loss-g-kl: 1.5121 lr: 0.0002 grad_norm_g: 662.2307 grad_norm_d: 80.3309
======> Epoch: 1649
Train Epoch: 1650 [3.85%] G-Loss: 32.1693 D-Loss: 2.1854 Loss-g-fm: 9.6023 Loss-g-mel: 17.7239 Loss-g-dur: 1.2482 Loss-g-kl: 1.1574 lr: 0.0002 grad_norm_g: 629.5779 grad_norm_d: 29.4286
======> Epoch: 1650
Train Epoch: 1651 [0.00%] G-Loss: 33.8735 D-Loss: 2.2348 Loss-g-fm: 10.4165 Loss-g-mel: 18.0190 Loss-g-dur: 1.2755 Loss-g-kl: 1.6322 lr: 0.0002 grad_norm_g: 689.7755 grad_norm_d: 45.1362
Train Epoch: 1651 [96.15%] G-Loss: 33.3242 D-Loss: 2.2659 Loss-g-fm: 9.7251 Loss-g-mel: 17.7327 Loss-g-dur: 1.3369 Loss-g-kl: 1.5537 lr: 0.0002 grad_norm_g: 636.0043 grad_norm_d: 16.6816
======> Epoch: 1651
Train Epoch: 1652 [92.31%] G-Loss: 34.5068 D-Loss: 2.2277 Loss-g-fm: 10.7197 Loss-g-mel: 18.2622 Loss-g-dur: 1.2980 Loss-g-kl: 1.5891 lr: 0.0002 grad_norm_g: 660.4781 grad_norm_d: 52.4231
======> Epoch: 1652
Train Epoch: 1653 [88.46%] G-Loss: 31.3280 D-Loss: 2.1994 Loss-g-fm: 9.2172 Loss-g-mel: 16.9818 Loss-g-dur: 1.1746 Loss-g-kl: 1.4063 lr: 0.0002 grad_norm_g: 302.7696 grad_norm_d: 23.8375
======> Epoch: 1653
Train Epoch: 1654 [84.62%] G-Loss: 33.8386 D-Loss: 2.2428 Loss-g-fm: 10.0790 Loss-g-mel: 18.3912 Loss-g-dur: 1.2528 Loss-g-kl: 1.4821 lr: 0.0002 grad_norm_g: 55.8312 grad_norm_d: 13.9904
======> Epoch: 1654
Train Epoch: 1655 [80.77%] G-Loss: 32.9029 D-Loss: 2.2247 Loss-g-fm: 10.2322 Loss-g-mel: 17.4562 Loss-g-dur: 1.2301 Loss-g-kl: 1.4582 lr: 0.0002 grad_norm_g: 251.6942 grad_norm_d: 17.3561
======> Epoch: 1655
Train Epoch: 1656 [76.92%] G-Loss: 31.5839 D-Loss: 2.2242 Loss-g-fm: 9.2323 Loss-g-mel: 17.2734 Loss-g-dur: 1.2089 Loss-g-kl: 1.4000 lr: 0.0002 grad_norm_g: 309.8281 grad_norm_d: 15.4615
======> Epoch: 1656
Train Epoch: 1657 [73.08%] G-Loss: 32.5968 D-Loss: 2.4042 Loss-g-fm: 9.3298 Loss-g-mel: 17.9509 Loss-g-dur: 1.3214 Loss-g-kl: 1.6719 lr: 0.0002 grad_norm_g: 763.7775 grad_norm_d: 56.6466
======> Epoch: 1657
Train Epoch: 1658 [69.23%] G-Loss: 19.4319 D-Loss: 2.5356 Loss-g-fm: 5.4839 Loss-g-mel: 9.3358 Loss-g-dur: 0.7260 Loss-g-kl: 1.4338 lr: 0.0002 grad_norm_g: 318.0540 grad_norm_d: 12.1116
======> Epoch: 1658
Train Epoch: 1659 [65.38%] G-Loss: 35.5568 D-Loss: 2.1314 Loss-g-fm: 11.6363 Loss-g-mel: 18.2551 Loss-g-dur: 1.2565 Loss-g-kl: 1.5538 lr: 0.0002 grad_norm_g: 830.5471 grad_norm_d: 20.2621
======> Epoch: 1659
Train Epoch: 1660 [61.54%] G-Loss: 36.0690 D-Loss: 2.0755 Loss-g-fm: 11.4170 Loss-g-mel: 19.0555 Loss-g-dur: 1.1800 Loss-g-kl: 1.6344 lr: 0.0002 grad_norm_g: 914.1097 grad_norm_d: 53.7579
======> Epoch: 1660
Train Epoch: 1661 [57.69%] G-Loss: 20.7038 D-Loss: 2.1978 Loss-g-fm: 6.8031 Loss-g-mel: 8.6912 Loss-g-dur: 0.7927 Loss-g-kl: 1.4050 lr: 0.0002 grad_norm_g: 1189.1851 grad_norm_d: 21.9856
======> Epoch: 1661
Train Epoch: 1662 [53.85%] G-Loss: 35.6646 D-Loss: 2.1952 Loss-g-fm: 11.5886 Loss-g-mel: 18.1493 Loss-g-dur: 1.2736 Loss-g-kl: 1.6084 lr: 0.0002 grad_norm_g: 649.0391 grad_norm_d: 74.6776
======> Epoch: 1662
Train Epoch: 1663 [50.00%] G-Loss: 32.4019 D-Loss: 2.4028 Loss-g-fm: 9.3376 Loss-g-mel: 17.4389 Loss-g-dur: 1.2523 Loss-g-kl: 1.3695 lr: 0.0002 grad_norm_g: 75.5327 grad_norm_d: 19.7274
======> Epoch: 1663
Train Epoch: 1664 [46.15%] G-Loss: 32.2834 D-Loss: 2.2320 Loss-g-fm: 9.0229 Loss-g-mel: 17.9925 Loss-g-dur: 1.2086 Loss-g-kl: 1.6353 lr: 0.0002 grad_norm_g: 136.0576 grad_norm_d: 7.9442
======> Epoch: 1664
Train Epoch: 1665 [42.31%] G-Loss: 31.5462 D-Loss: 2.1586 Loss-g-fm: 9.7218 Loss-g-mel: 16.6898 Loss-g-dur: 1.1921 Loss-g-kl: 1.2846 lr: 0.0002 grad_norm_g: 725.7077 grad_norm_d: 49.2304
======> Epoch: 1665
Train Epoch: 1666 [38.46%] G-Loss: 33.8581 D-Loss: 2.2475 Loss-g-fm: 10.2273 Loss-g-mel: 18.2081 Loss-g-dur: 1.2701 Loss-g-kl: 1.4309 lr: 0.0002 grad_norm_g: 209.2761 grad_norm_d: 12.7289
======> Epoch: 1666
Train Epoch: 1667 [34.62%] G-Loss: 34.8236 D-Loss: 2.2164 Loss-g-fm: 10.6551 Loss-g-mel: 18.3810 Loss-g-dur: 1.3135 Loss-g-kl: 1.5297 lr: 0.0002 grad_norm_g: 602.3041 grad_norm_d: 47.7365
======> Epoch: 1667
Train Epoch: 1668 [30.77%] G-Loss: 31.6194 D-Loss: 2.3341 Loss-g-fm: 9.2598 Loss-g-mel: 17.0824 Loss-g-dur: 1.2185 Loss-g-kl: 1.4216 lr: 0.0002 grad_norm_g: 196.8797 grad_norm_d: 25.4728
======> Epoch: 1668
Train Epoch: 1669 [26.92%] G-Loss: 32.6761 D-Loss: 2.1869 Loss-g-fm: 10.1320 Loss-g-mel: 17.4835 Loss-g-dur: 1.2304 Loss-g-kl: 1.3634 lr: 0.0002 grad_norm_g: 66.6262 grad_norm_d: 18.8755
======> Epoch: 1669
Train Epoch: 1670 [23.08%] G-Loss: 33.5351 D-Loss: 2.1797 Loss-g-fm: 10.0700 Loss-g-mel: 18.0335 Loss-g-dur: 1.2198 Loss-g-kl: 1.5314 lr: 0.0002 grad_norm_g: 851.7654 grad_norm_d: 56.3266
======> Epoch: 1670
Train Epoch: 1671 [19.23%] G-Loss: 30.5183 D-Loss: 2.3038 Loss-g-fm: 8.9143 Loss-g-mel: 17.0986 Loss-g-dur: 1.2024 Loss-g-kl: 1.3445 lr: 0.0002 grad_norm_g: 227.0906 grad_norm_d: 23.3137
======> Epoch: 1671
Train Epoch: 1672 [15.38%] G-Loss: 32.2259 D-Loss: 2.0687 Loss-g-fm: 9.7329 Loss-g-mel: 17.1487 Loss-g-dur: 1.2283 Loss-g-kl: 1.3868 lr: 0.0002 grad_norm_g: 915.6679 grad_norm_d: 58.6492
======> Epoch: 1672
Train Epoch: 1673 [11.54%] G-Loss: 35.4944 D-Loss: 2.1183 Loss-g-fm: 11.3847 Loss-g-mel: 18.3629 Loss-g-dur: 1.3036 Loss-g-kl: 1.5452 lr: 0.0002 grad_norm_g: 765.3397 grad_norm_d: 50.5759
======> Epoch: 1673
Train Epoch: 1674 [7.69%] G-Loss: 30.9684 D-Loss: 2.2201 Loss-g-fm: 8.8136 Loss-g-mel: 16.9701 Loss-g-dur: 1.1870 Loss-g-kl: 1.3042 lr: 0.0002 grad_norm_g: 698.9966 grad_norm_d: 43.6152
======> Epoch: 1674
Train Epoch: 1675 [3.85%] G-Loss: 33.0557 D-Loss: 2.2381 Loss-g-fm: 9.9170 Loss-g-mel: 18.0848 Loss-g-dur: 1.1640 Loss-g-kl: 1.1588 lr: 0.0002 grad_norm_g: 940.1159 grad_norm_d: 54.3314
======> Epoch: 1675
Train Epoch: 1676 [0.00%] G-Loss: 31.2793 D-Loss: 2.5053 Loss-g-fm: 8.8467 Loss-g-mel: 17.0988 Loss-g-dur: 1.2666 Loss-g-kl: 1.4083 lr: 0.0002 grad_norm_g: 128.4790 grad_norm_d: 92.9627
Train Epoch: 1676 [96.15%] G-Loss: 36.7717 D-Loss: 2.4360 Loss-g-fm: 12.1300 Loss-g-mel: 18.7449 Loss-g-dur: 1.2916 Loss-g-kl: 1.4794 lr: 0.0002 grad_norm_g: 833.4360 grad_norm_d: 52.4385
======> Epoch: 1676
Train Epoch: 1677 [92.31%] G-Loss: 33.4187 D-Loss: 2.2926 Loss-g-fm: 10.3649 Loss-g-mel: 17.6738 Loss-g-dur: 1.2902 Loss-g-kl: 1.5100 lr: 0.0002 grad_norm_g: 878.8230 grad_norm_d: 27.0427
======> Epoch: 1677
Train Epoch: 1678 [88.46%] G-Loss: 32.9961 D-Loss: 2.1669 Loss-g-fm: 10.0006 Loss-g-mel: 17.8071 Loss-g-dur: 1.1996 Loss-g-kl: 1.4448 lr: 0.0002 grad_norm_g: 653.9357 grad_norm_d: 27.6824
======> Epoch: 1678
Train Epoch: 1679 [84.62%] G-Loss: 34.7567 D-Loss: 2.2814 Loss-g-fm: 10.4784 Loss-g-mel: 18.8367 Loss-g-dur: 1.2746 Loss-g-kl: 1.4602 lr: 0.0002 grad_norm_g: 489.0946 grad_norm_d: 30.8556
======> Epoch: 1679
Train Epoch: 1680 [80.77%] G-Loss: 32.2427 D-Loss: 2.2056 Loss-g-fm: 9.5176 Loss-g-mel: 17.1620 Loss-g-dur: 1.2224 Loss-g-kl: 1.3755 lr: 0.0002 grad_norm_g: 140.5144 grad_norm_d: 15.7724
======> Epoch: 1680
Train Epoch: 1681 [76.92%] G-Loss: 34.4199 D-Loss: 2.1415 Loss-g-fm: 10.4556 Loss-g-mel: 18.3693 Loss-g-dur: 1.3687 Loss-g-kl: 1.5219 lr: 0.0002 grad_norm_g: 147.1687 grad_norm_d: 23.1275
======> Epoch: 1681
Train Epoch: 1682 [73.08%] G-Loss: 19.6768 D-Loss: 2.4354 Loss-g-fm: 5.7920 Loss-g-mel: 9.0517 Loss-g-dur: 0.7753 Loss-g-kl: 1.4179 lr: 0.0002 grad_norm_g: 845.0378 grad_norm_d: 33.8576
======> Epoch: 1682
Train Epoch: 1683 [69.23%] G-Loss: 34.0207 D-Loss: 2.3092 Loss-g-fm: 10.4382 Loss-g-mel: 18.2172 Loss-g-dur: 1.3048 Loss-g-kl: 1.5752 lr: 0.0002 grad_norm_g: 653.7397 grad_norm_d: 19.8299
======> Epoch: 1683
Train Epoch: 1684 [65.38%] G-Loss: 33.7711 D-Loss: 2.1856 Loss-g-fm: 10.6314 Loss-g-mel: 17.6842 Loss-g-dur: 1.2065 Loss-g-kl: 1.5514 lr: 0.0002 grad_norm_g: 737.3120 grad_norm_d: 54.8037
======> Epoch: 1684
Train Epoch: 1685 [61.54%] G-Loss: 26.4311 D-Loss: 2.2552 Loss-g-fm: 7.7026 Loss-g-mel: 13.7000 Loss-g-dur: 1.0020 Loss-g-kl: 1.3849 lr: 0.0002 grad_norm_g: 808.9429 grad_norm_d: 53.9555
======> Epoch: 1685
Train Epoch: 1686 [57.69%] G-Loss: 35.8578 D-Loss: 2.1058 Loss-g-fm: 11.9208 Loss-g-mel: 17.9591 Loss-g-dur: 1.2648 Loss-g-kl: 1.4251 lr: 0.0002 grad_norm_g: 60.7128 grad_norm_d: 14.8005
======> Epoch: 1686
Train Epoch: 1687 [53.85%] G-Loss: 19.9768 D-Loss: 2.2794 Loss-g-fm: 6.2489 Loss-g-mel: 9.1081 Loss-g-dur: 0.7295 Loss-g-kl: 1.4162 lr: 0.0002 grad_norm_g: 1293.3040 grad_norm_d: 58.6687
======> Epoch: 1687
Train Epoch: 1688 [50.00%] G-Loss: 33.6581 D-Loss: 2.3403 Loss-g-fm: 10.0805 Loss-g-mel: 18.2884 Loss-g-dur: 1.2714 Loss-g-kl: 1.4151 lr: 0.0002 grad_norm_g: 601.6772 grad_norm_d: 87.1203
======> Epoch: 1688
Train Epoch: 1689 [46.15%] G-Loss: 19.6049 D-Loss: 2.2750 Loss-g-fm: 6.0362 Loss-g-mel: 9.0791 Loss-g-dur: 0.6947 Loss-g-kl: 1.4428 lr: 0.0002 grad_norm_g: 724.3353 grad_norm_d: 37.9601
======> Epoch: 1689
Train Epoch: 1690 [42.31%] G-Loss: 34.9373 D-Loss: 2.2988 Loss-g-fm: 11.0897 Loss-g-mel: 18.3722 Loss-g-dur: 1.3283 Loss-g-kl: 1.5815 lr: 0.0002 grad_norm_g: 366.9462 grad_norm_d: 18.8092
======> Epoch: 1690
Train Epoch: 1691 [38.46%] G-Loss: 30.4152 D-Loss: 2.2435 Loss-g-fm: 8.7598 Loss-g-mel: 16.6340 Loss-g-dur: 1.1421 Loss-g-kl: 1.2078 lr: 0.0002 grad_norm_g: 597.8150 grad_norm_d: 47.7886
======> Epoch: 1691
Train Epoch: 1692 [34.62%] G-Loss: 28.3919 D-Loss: 2.1638 Loss-g-fm: 9.0730 Loss-g-mel: 14.3479 Loss-g-dur: 1.1460 Loss-g-kl: 1.4283 lr: 0.0002 grad_norm_g: 841.5294 grad_norm_d: 56.3314
======> Epoch: 1692
Train Epoch: 1693 [30.77%] G-Loss: 33.1222 D-Loss: 2.0852 Loss-g-fm: 10.4072 Loss-g-mel: 17.5363 Loss-g-dur: 1.1860 Loss-g-kl: 1.2080 lr: 0.0002 grad_norm_g: 760.6836 grad_norm_d: 45.2417
terminate called without an active exception
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f44c239b820>
Traceback (most recent call last):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1731136) is killed by signal: Aborted. 
Saving model and optimizer state at iteration 1693 to /ZFS4T/tts/data/VITS/model_saved/G_44000.pth
Saving model and optimizer state at iteration 1693 to /ZFS4T/tts/data/VITS/model_saved/D_44000.pth
======> Epoch: 1693
Train Epoch: 1694 [26.92%] G-Loss: 34.9650 D-Loss: 2.1615 Loss-g-fm: 11.0666 Loss-g-mel: 18.2487 Loss-g-dur: 1.2701 Loss-g-kl: 1.5037 lr: 0.0002 grad_norm_g: 957.3023 grad_norm_d: 69.3206
======> Epoch: 1694
Train Epoch: 1695 [23.08%] G-Loss: 19.8953 D-Loss: 2.3653 Loss-g-fm: 5.8014 Loss-g-mel: 9.2068 Loss-g-dur: 0.7976 Loss-g-kl: 1.4757 lr: 0.0002 grad_norm_g: 744.3705 grad_norm_d: 67.8966
======> Epoch: 1695
Train Epoch: 1696 [19.23%] G-Loss: 34.1341 D-Loss: 2.1674 Loss-g-fm: 11.1553 Loss-g-mel: 17.6753 Loss-g-dur: 1.2673 Loss-g-kl: 1.4397 lr: 0.0002 grad_norm_g: 698.9165 grad_norm_d: 51.1036
======> Epoch: 1696
Train Epoch: 1697 [15.38%] G-Loss: 32.7104 D-Loss: 2.3580 Loss-g-fm: 9.4981 Loss-g-mel: 17.5438 Loss-g-dur: 1.1882 Loss-g-kl: 1.3049 lr: 0.0002 grad_norm_g: 587.0352 grad_norm_d: 80.7848
======> Epoch: 1697
Train Epoch: 1698 [11.54%] G-Loss: 19.6778 D-Loss: 2.2131 Loss-g-fm: 6.4087 Loss-g-mel: 8.7123 Loss-g-dur: 0.6784 Loss-g-kl: 1.2765 lr: 0.0002 grad_norm_g: 1076.7813 grad_norm_d: 34.3459
======> Epoch: 1698
Train Epoch: 1699 [7.69%] G-Loss: 31.2269 D-Loss: 2.2347 Loss-g-fm: 9.4161 Loss-g-mel: 16.9871 Loss-g-dur: 1.1810 Loss-g-kl: 1.0775 lr: 0.0002 grad_norm_g: 613.5477 grad_norm_d: 56.8941
======> Epoch: 1699
Train Epoch: 1700 [3.85%] G-Loss: 32.4807 D-Loss: 2.3965 Loss-g-fm: 10.3405 Loss-g-mel: 16.9990 Loss-g-dur: 1.2159 Loss-g-kl: 1.4883 lr: 0.0002 grad_norm_g: 513.8066 grad_norm_d: 31.3985
======> Epoch: 1700
Train Epoch: 1701 [0.00%] G-Loss: 33.2668 D-Loss: 2.1812 Loss-g-fm: 10.5722 Loss-g-mel: 17.5354 Loss-g-dur: 1.1757 Loss-g-kl: 1.5024 lr: 0.0002 grad_norm_g: 878.5288 grad_norm_d: 58.6597
Train Epoch: 1701 [96.15%] G-Loss: 34.2864 D-Loss: 2.2582 Loss-g-fm: 10.5426 Loss-g-mel: 18.3692 Loss-g-dur: 1.2777 Loss-g-kl: 1.3539 lr: 0.0002 grad_norm_g: 744.4221 grad_norm_d: 16.6663
======> Epoch: 1701
Train Epoch: 1702 [92.31%] G-Loss: 35.7108 D-Loss: 2.1800 Loss-g-fm: 11.7054 Loss-g-mel: 18.5015 Loss-g-dur: 1.2613 Loss-g-kl: 1.5539 lr: 0.0002 grad_norm_g: 716.7379 grad_norm_d: 44.2781
======> Epoch: 1702
Train Epoch: 1703 [88.46%] G-Loss: 30.9800 D-Loss: 2.3357 Loss-g-fm: 8.8668 Loss-g-mel: 17.0651 Loss-g-dur: 1.2209 Loss-g-kl: 1.3349 lr: 0.0002 grad_norm_g: 149.0578 grad_norm_d: 28.7615
======> Epoch: 1703
Train Epoch: 1704 [84.62%] G-Loss: 32.5310 D-Loss: 2.2380 Loss-g-fm: 9.9946 Loss-g-mel: 17.3553 Loss-g-dur: 1.2367 Loss-g-kl: 1.4956 lr: 0.0002 grad_norm_g: 44.7490 grad_norm_d: 5.9250
======> Epoch: 1704
Train Epoch: 1705 [80.77%] G-Loss: 34.8488 D-Loss: 2.1788 Loss-g-fm: 10.7727 Loss-g-mel: 18.5416 Loss-g-dur: 1.2854 Loss-g-kl: 1.4991 lr: 0.0002 grad_norm_g: 465.5818 grad_norm_d: 12.3332
======> Epoch: 1705
Train Epoch: 1706 [76.92%] G-Loss: 35.3736 D-Loss: 2.1403 Loss-g-fm: 11.4922 Loss-g-mel: 18.1044 Loss-g-dur: 1.3472 Loss-g-kl: 1.7136 lr: 0.0002 grad_norm_g: 348.9598 grad_norm_d: 6.9211
======> Epoch: 1706
Train Epoch: 1707 [73.08%] G-Loss: 32.5439 D-Loss: 2.2821 Loss-g-fm: 10.0912 Loss-g-mel: 17.0193 Loss-g-dur: 1.2858 Loss-g-kl: 1.4263 lr: 0.0002 grad_norm_g: 976.4038 grad_norm_d: 49.5797
======> Epoch: 1707
Train Epoch: 1708 [69.23%] G-Loss: 32.5510 D-Loss: 2.1180 Loss-g-fm: 9.7437 Loss-g-mel: 17.3477 Loss-g-dur: 1.2392 Loss-g-kl: 1.4368 lr: 0.0002 grad_norm_g: 738.4655 grad_norm_d: 22.4209
======> Epoch: 1708
Train Epoch: 1709 [65.38%] G-Loss: 30.8882 D-Loss: 2.2949 Loss-g-fm: 8.8376 Loss-g-mel: 16.7946 Loss-g-dur: 1.2006 Loss-g-kl: 1.4494 lr: 0.0002 grad_norm_g: 980.8007 grad_norm_d: 61.0471
======> Epoch: 1709
Train Epoch: 1710 [61.54%] G-Loss: 32.4664 D-Loss: 2.3329 Loss-g-fm: 9.4451 Loss-g-mel: 17.2819 Loss-g-dur: 1.2003 Loss-g-kl: 1.5145 lr: 0.0002 grad_norm_g: 754.5888 grad_norm_d: 74.6236
======> Epoch: 1710
Train Epoch: 1711 [57.69%] G-Loss: 33.3998 D-Loss: 2.0552 Loss-g-fm: 10.2735 Loss-g-mel: 17.8152 Loss-g-dur: 1.2120 Loss-g-kl: 1.3785 lr: 0.0002 grad_norm_g: 370.7421 grad_norm_d: 15.4806
======> Epoch: 1711
Train Epoch: 1712 [53.85%] G-Loss: 33.5676 D-Loss: 2.3667 Loss-g-fm: 9.9592 Loss-g-mel: 18.0728 Loss-g-dur: 1.2904 Loss-g-kl: 1.5578 lr: 0.0002 grad_norm_g: 412.1548 grad_norm_d: 9.3320
======> Epoch: 1712
Train Epoch: 1713 [50.00%] G-Loss: 31.3050 D-Loss: 2.2108 Loss-g-fm: 9.1016 Loss-g-mel: 17.0933 Loss-g-dur: 1.1610 Loss-g-kl: 1.5113 lr: 0.0002 grad_norm_g: 189.0990 grad_norm_d: 10.1524
======> Epoch: 1713
Train Epoch: 1714 [46.15%] G-Loss: 27.4109 D-Loss: 2.3468 Loss-g-fm: 8.3465 Loss-g-mel: 14.0297 Loss-g-dur: 1.0442 Loss-g-kl: 1.3460 lr: 0.0002 grad_norm_g: 488.7213 grad_norm_d: 12.9736
======> Epoch: 1714
Train Epoch: 1715 [42.31%] G-Loss: 34.9973 D-Loss: 2.0673 Loss-g-fm: 11.3410 Loss-g-mel: 18.4731 Loss-g-dur: 1.2488 Loss-g-kl: 1.4087 lr: 0.0002 grad_norm_g: 831.3587 grad_norm_d: 59.0118
======> Epoch: 1715
Train Epoch: 1716 [38.46%] G-Loss: 33.4747 D-Loss: 2.3157 Loss-g-fm: 9.9147 Loss-g-mel: 17.8717 Loss-g-dur: 1.2031 Loss-g-kl: 1.5341 lr: 0.0002 grad_norm_g: 94.1497 grad_norm_d: 38.7688
======> Epoch: 1716
Train Epoch: 1717 [34.62%] G-Loss: 34.8955 D-Loss: 2.1194 Loss-g-fm: 11.1295 Loss-g-mel: 18.0371 Loss-g-dur: 1.2691 Loss-g-kl: 1.6881 lr: 0.0002 grad_norm_g: 238.0189 grad_norm_d: 33.3053
======> Epoch: 1717
Train Epoch: 1718 [30.77%] G-Loss: 31.6828 D-Loss: 2.2265 Loss-g-fm: 9.1651 Loss-g-mel: 17.2416 Loss-g-dur: 1.1909 Loss-g-kl: 1.5234 lr: 0.0002 grad_norm_g: 282.4172 grad_norm_d: 13.1324
======> Epoch: 1718
Train Epoch: 1719 [26.92%] G-Loss: 32.3497 D-Loss: 2.0412 Loss-g-fm: 9.6910 Loss-g-mel: 17.1681 Loss-g-dur: 1.1942 Loss-g-kl: 1.4290 lr: 0.0002 grad_norm_g: 785.5301 grad_norm_d: 38.1732
======> Epoch: 1719
Train Epoch: 1720 [23.08%] G-Loss: 34.7653 D-Loss: 2.1033 Loss-g-fm: 10.7761 Loss-g-mel: 18.1209 Loss-g-dur: 1.2774 Loss-g-kl: 1.6046 lr: 0.0002 grad_norm_g: 734.0255 grad_norm_d: 79.1899
======> Epoch: 1720
Train Epoch: 1721 [19.23%] G-Loss: 30.7644 D-Loss: 2.4039 Loss-g-fm: 9.2857 Loss-g-mel: 16.5898 Loss-g-dur: 1.2013 Loss-g-kl: 1.3042 lr: 0.0002 grad_norm_g: 521.7535 grad_norm_d: 117.1625
======> Epoch: 1721
Train Epoch: 1722 [15.38%] G-Loss: 30.7447 D-Loss: 2.3864 Loss-g-fm: 9.1939 Loss-g-mel: 16.6405 Loss-g-dur: 1.1939 Loss-g-kl: 1.3086 lr: 0.0002 grad_norm_g: 327.6016 grad_norm_d: 11.7737
======> Epoch: 1722
Train Epoch: 1723 [11.54%] G-Loss: 19.9848 D-Loss: 2.3857 Loss-g-fm: 6.1990 Loss-g-mel: 9.0257 Loss-g-dur: 0.9153 Loss-g-kl: 1.5120 lr: 0.0002 grad_norm_g: 365.6055 grad_norm_d: 18.9395
======> Epoch: 1723
Train Epoch: 1724 [7.69%] G-Loss: 33.8758 D-Loss: 2.2178 Loss-g-fm: 10.8133 Loss-g-mel: 17.8996 Loss-g-dur: 1.1604 Loss-g-kl: 1.4212 lr: 0.0002 grad_norm_g: 383.2172 grad_norm_d: 14.2417
======> Epoch: 1724
Train Epoch: 1725 [3.85%] G-Loss: 35.6164 D-Loss: 2.1378 Loss-g-fm: 11.2336 Loss-g-mel: 18.6950 Loss-g-dur: 1.3473 Loss-g-kl: 1.5184 lr: 0.0002 grad_norm_g: 801.1083 grad_norm_d: 22.4137
======> Epoch: 1725
Train Epoch: 1726 [0.00%] G-Loss: 34.3832 D-Loss: 2.2401 Loss-g-fm: 10.8853 Loss-g-mel: 17.7604 Loss-g-dur: 1.2840 Loss-g-kl: 1.6684 lr: 0.0002 grad_norm_g: 728.9644 grad_norm_d: 34.3016
Train Epoch: 1726 [96.15%] G-Loss: 19.9488 D-Loss: 2.2287 Loss-g-fm: 6.7538 Loss-g-mel: 8.5985 Loss-g-dur: 0.6647 Loss-g-kl: 1.4106 lr: 0.0002 grad_norm_g: 1259.7712 grad_norm_d: 61.6807
======> Epoch: 1726
Train Epoch: 1727 [92.31%] G-Loss: 19.9229 D-Loss: 2.3409 Loss-g-fm: 6.4419 Loss-g-mel: 8.4649 Loss-g-dur: 0.6711 Loss-g-kl: 1.3640 lr: 0.0002 grad_norm_g: 1366.3180 grad_norm_d: 77.4847
======> Epoch: 1727
Train Epoch: 1728 [88.46%] G-Loss: 35.0490 D-Loss: 2.1363 Loss-g-fm: 11.3028 Loss-g-mel: 18.1855 Loss-g-dur: 1.2480 Loss-g-kl: 1.6124 lr: 0.0002 grad_norm_g: 349.1906 grad_norm_d: 35.8958
======> Epoch: 1728
Train Epoch: 1729 [84.62%] G-Loss: 30.2811 D-Loss: 2.3194 Loss-g-fm: 8.3855 Loss-g-mel: 16.8913 Loss-g-dur: 1.2251 Loss-g-kl: 1.3784 lr: 0.0002 grad_norm_g: 813.8486 grad_norm_d: 40.3298
======> Epoch: 1729
Train Epoch: 1730 [80.77%] G-Loss: 30.3407 D-Loss: 2.3019 Loss-g-fm: 8.6327 Loss-g-mel: 16.9575 Loss-g-dur: 1.1938 Loss-g-kl: 1.2047 lr: 0.0002 grad_norm_g: 430.0214 grad_norm_d: 45.1439
======> Epoch: 1730
Train Epoch: 1731 [76.92%] G-Loss: 33.8942 D-Loss: 2.3048 Loss-g-fm: 9.9103 Loss-g-mel: 18.4207 Loss-g-dur: 1.2875 Loss-g-kl: 1.4696 lr: 0.0002 grad_norm_g: 839.2495 grad_norm_d: 59.8073
======> Epoch: 1731
Train Epoch: 1732 [73.08%] G-Loss: 32.2504 D-Loss: 2.2134 Loss-g-fm: 9.6152 Loss-g-mel: 17.3417 Loss-g-dur: 1.2021 Loss-g-kl: 1.3842 lr: 0.0002 grad_norm_g: 924.9158 grad_norm_d: 53.3669
======> Epoch: 1732
Train Epoch: 1733 [69.23%] G-Loss: 33.8390 D-Loss: 2.2256 Loss-g-fm: 10.0510 Loss-g-mel: 18.0479 Loss-g-dur: 1.3724 Loss-g-kl: 1.5875 lr: 0.0002 grad_norm_g: 750.9372 grad_norm_d: 46.9712
======> Epoch: 1733
Train Epoch: 1734 [65.38%] G-Loss: 32.2473 D-Loss: 2.3371 Loss-g-fm: 9.1887 Loss-g-mel: 17.1745 Loss-g-dur: 1.2480 Loss-g-kl: 1.6159 lr: 0.0002 grad_norm_g: 549.4071 grad_norm_d: 113.9020
======> Epoch: 1734
Train Epoch: 1735 [61.54%] G-Loss: 34.7645 D-Loss: 2.1617 Loss-g-fm: 10.8309 Loss-g-mel: 18.3089 Loss-g-dur: 1.3389 Loss-g-kl: 1.6536 lr: 0.0002 grad_norm_g: 688.4378 grad_norm_d: 59.2542
======> Epoch: 1735
Train Epoch: 1736 [57.69%] G-Loss: 33.8345 D-Loss: 2.1294 Loss-g-fm: 10.6555 Loss-g-mel: 18.0795 Loss-g-dur: 1.2351 Loss-g-kl: 1.2013 lr: 0.0002 grad_norm_g: 633.6234 grad_norm_d: 28.7369
======> Epoch: 1736
Train Epoch: 1737 [53.85%] G-Loss: 33.6263 D-Loss: 2.2699 Loss-g-fm: 10.3957 Loss-g-mel: 18.0268 Loss-g-dur: 1.2742 Loss-g-kl: 1.5607 lr: 0.0002 grad_norm_g: 506.9055 grad_norm_d: 28.7464
======> Epoch: 1737
Train Epoch: 1738 [50.00%] G-Loss: 20.6780 D-Loss: 2.3017 Loss-g-fm: 6.9232 Loss-g-mel: 8.6121 Loss-g-dur: 0.6507 Loss-g-kl: 1.4838 lr: 0.0002 grad_norm_g: 1488.1694 grad_norm_d: 54.1646
======> Epoch: 1738
Train Epoch: 1739 [46.15%] G-Loss: 32.1429 D-Loss: 2.2977 Loss-g-fm: 9.6816 Loss-g-mel: 17.1405 Loss-g-dur: 1.2223 Loss-g-kl: 1.2875 lr: 0.0002 grad_norm_g: 753.1839 grad_norm_d: 65.1971
======> Epoch: 1739
Train Epoch: 1740 [42.31%] G-Loss: 32.2419 D-Loss: 2.2669 Loss-g-fm: 10.1185 Loss-g-mel: 16.8554 Loss-g-dur: 1.1913 Loss-g-kl: 1.3051 lr: 0.0002 grad_norm_g: 717.5938 grad_norm_d: 40.8140
======> Epoch: 1740
Train Epoch: 1741 [38.46%] G-Loss: 33.7200 D-Loss: 2.1074 Loss-g-fm: 10.2102 Loss-g-mel: 17.9584 Loss-g-dur: 1.1896 Loss-g-kl: 1.5795 lr: 0.0002 grad_norm_g: 750.8135 grad_norm_d: 36.2630
======> Epoch: 1741
Train Epoch: 1742 [34.62%] G-Loss: 28.9368 D-Loss: 2.1105 Loss-g-fm: 9.5518 Loss-g-mel: 14.2807 Loss-g-dur: 1.0257 Loss-g-kl: 1.3091 lr: 0.0002 grad_norm_g: 1090.5041 grad_norm_d: 67.8838
======> Epoch: 1742
Train Epoch: 1743 [30.77%] G-Loss: 35.4470 D-Loss: 2.0662 Loss-g-fm: 11.6996 Loss-g-mel: 18.0417 Loss-g-dur: 1.3722 Loss-g-kl: 1.5967 lr: 0.0002 grad_norm_g: 817.5919 grad_norm_d: 69.7620
======> Epoch: 1743
Train Epoch: 1744 [26.92%] G-Loss: 33.9752 D-Loss: 2.1250 Loss-g-fm: 11.0146 Loss-g-mel: 17.6624 Loss-g-dur: 1.2025 Loss-g-kl: 1.3301 lr: 0.0002 grad_norm_g: 1056.3019 grad_norm_d: 68.1155
======> Epoch: 1744
Train Epoch: 1745 [23.08%] G-Loss: 30.7887 D-Loss: 2.2310 Loss-g-fm: 8.9907 Loss-g-mel: 16.9725 Loss-g-dur: 1.2136 Loss-g-kl: 1.1619 lr: 0.0002 grad_norm_g: 154.0743 grad_norm_d: 23.2517
======> Epoch: 1745
Train Epoch: 1746 [19.23%] G-Loss: 18.7232 D-Loss: 2.2654 Loss-g-fm: 5.9474 Loss-g-mel: 7.9901 Loss-g-dur: 0.7020 Loss-g-kl: 1.4303 lr: 0.0002 grad_norm_g: 1422.4844 grad_norm_d: 59.7276
======> Epoch: 1746
Train Epoch: 1747 [15.38%] G-Loss: 33.3270 D-Loss: 2.1438 Loss-g-fm: 9.9391 Loss-g-mel: 17.9146 Loss-g-dur: 1.3564 Loss-g-kl: 1.4973 lr: 0.0002 grad_norm_g: 643.8444 grad_norm_d: 60.4894
======> Epoch: 1747
Train Epoch: 1748 [11.54%] G-Loss: 33.2381 D-Loss: 2.2238 Loss-g-fm: 10.1651 Loss-g-mel: 17.6666 Loss-g-dur: 1.2774 Loss-g-kl: 1.5523 lr: 0.0002 grad_norm_g: 845.3578 grad_norm_d: 56.3431
======> Epoch: 1748
Train Epoch: 1749 [7.69%] G-Loss: 33.2735 D-Loss: 2.2228 Loss-g-fm: 9.8862 Loss-g-mel: 17.9298 Loss-g-dur: 1.3210 Loss-g-kl: 1.4441 lr: 0.0002 grad_norm_g: 696.5219 grad_norm_d: 68.1779
======> Epoch: 1749
Train Epoch: 1750 [3.85%] G-Loss: 31.3155 D-Loss: 2.3248 Loss-g-fm: 8.7671 Loss-g-mel: 16.8223 Loss-g-dur: 1.1807 Loss-g-kl: 1.4181 lr: 0.0002 grad_norm_g: 911.1586 grad_norm_d: 42.7442
======> Epoch: 1750
Train Epoch: 1751 [0.00%] G-Loss: 34.4192 D-Loss: 2.1737 Loss-g-fm: 10.7004 Loss-g-mel: 17.8776 Loss-g-dur: 1.3650 Loss-g-kl: 1.5052 lr: 0.0002 grad_norm_g: 996.6334 grad_norm_d: 46.1315
Train Epoch: 1751 [96.15%] G-Loss: 34.7981 D-Loss: 2.2226 Loss-g-fm: 10.6706 Loss-g-mel: 18.6244 Loss-g-dur: 1.3038 Loss-g-kl: 1.7543 lr: 0.0002 grad_norm_g: 406.5525 grad_norm_d: 37.9274
======> Epoch: 1751
Train Epoch: 1752 [92.31%] G-Loss: 33.3173 D-Loss: 2.2857 Loss-g-fm: 10.4986 Loss-g-mel: 17.4409 Loss-g-dur: 1.1753 Loss-g-kl: 1.5628 lr: 0.0002 grad_norm_g: 125.3704 grad_norm_d: 23.6993
======> Epoch: 1752
Train Epoch: 1753 [88.46%] G-Loss: 34.7822 D-Loss: 2.1683 Loss-g-fm: 11.4524 Loss-g-mel: 18.0169 Loss-g-dur: 1.1743 Loss-g-kl: 1.3594 lr: 0.0002 grad_norm_g: 1001.5566 grad_norm_d: 56.7172
======> Epoch: 1753
Train Epoch: 1754 [84.62%] G-Loss: 33.8412 D-Loss: 2.2306 Loss-g-fm: 10.7333 Loss-g-mel: 17.5708 Loss-g-dur: 1.1863 Loss-g-kl: 1.5007 lr: 0.0002 grad_norm_g: 941.5828 grad_norm_d: 66.6529
======> Epoch: 1754
Train Epoch: 1755 [80.77%] G-Loss: 33.6108 D-Loss: 2.2375 Loss-g-fm: 10.3844 Loss-g-mel: 17.7102 Loss-g-dur: 1.2036 Loss-g-kl: 1.5099 lr: 0.0002 grad_norm_g: 812.7709 grad_norm_d: 72.6991
======> Epoch: 1755
Train Epoch: 1756 [76.92%] G-Loss: 35.9603 D-Loss: 2.0471 Loss-g-fm: 11.2650 Loss-g-mel: 18.8527 Loss-g-dur: 1.4149 Loss-g-kl: 1.4768 lr: 0.0002 grad_norm_g: 970.9729 grad_norm_d: 38.1285
======> Epoch: 1756
Train Epoch: 1757 [73.08%] G-Loss: 36.1567 D-Loss: 2.2711 Loss-g-fm: 11.0826 Loss-g-mel: 18.8837 Loss-g-dur: 1.3741 Loss-g-kl: 1.6289 lr: 0.0002 grad_norm_g: 754.5429 grad_norm_d: 100.5794
======> Epoch: 1757
Train Epoch: 1758 [69.23%] G-Loss: 33.6859 D-Loss: 2.2758 Loss-g-fm: 10.3275 Loss-g-mel: 18.4414 Loss-g-dur: 1.2552 Loss-g-kl: 1.4883 lr: 0.0002 grad_norm_g: 807.9136 grad_norm_d: 59.6658
======> Epoch: 1758
Train Epoch: 1759 [65.38%] G-Loss: 33.8439 D-Loss: 2.2397 Loss-g-fm: 10.7976 Loss-g-mel: 17.5189 Loss-g-dur: 1.2184 Loss-g-kl: 1.4297 lr: 0.0002 grad_norm_g: 949.1387 grad_norm_d: 70.8552
======> Epoch: 1759
Train Epoch: 1760 [61.54%] G-Loss: 33.4412 D-Loss: 2.1234 Loss-g-fm: 10.6228 Loss-g-mel: 17.7121 Loss-g-dur: 1.2058 Loss-g-kl: 1.3490 lr: 0.0002 grad_norm_g: 783.7638 grad_norm_d: 41.0684
======> Epoch: 1760
Train Epoch: 1761 [57.69%] G-Loss: 32.2127 D-Loss: 2.2882 Loss-g-fm: 9.9630 Loss-g-mel: 17.0355 Loss-g-dur: 1.1780 Loss-g-kl: 1.4548 lr: 0.0002 grad_norm_g: 205.6060 grad_norm_d: 6.2154
======> Epoch: 1761
Train Epoch: 1762 [53.85%] G-Loss: 33.1048 D-Loss: 2.2310 Loss-g-fm: 10.5835 Loss-g-mel: 17.3288 Loss-g-dur: 1.1896 Loss-g-kl: 1.4692 lr: 0.0002 grad_norm_g: 506.1010 grad_norm_d: 23.8757
======> Epoch: 1762
Train Epoch: 1763 [50.00%] G-Loss: 32.9549 D-Loss: 2.2116 Loss-g-fm: 10.5642 Loss-g-mel: 17.1509 Loss-g-dur: 1.1762 Loss-g-kl: 1.4893 lr: 0.0002 grad_norm_g: 399.1459 grad_norm_d: 27.3929
======> Epoch: 1763
Train Epoch: 1764 [46.15%] G-Loss: 34.1811 D-Loss: 2.1273 Loss-g-fm: 11.2343 Loss-g-mel: 18.2448 Loss-g-dur: 1.2229 Loss-g-kl: 1.0517 lr: 0.0002 grad_norm_g: 580.2619 grad_norm_d: 25.3307
======> Epoch: 1764
Train Epoch: 1765 [42.31%] G-Loss: 34.8111 D-Loss: 2.1359 Loss-g-fm: 11.1283 Loss-g-mel: 18.0665 Loss-g-dur: 1.3553 Loss-g-kl: 1.6801 lr: 0.0002 grad_norm_g: 374.1924 grad_norm_d: 6.1044
======> Epoch: 1765
Train Epoch: 1766 [38.46%] G-Loss: 20.8975 D-Loss: 2.0377 Loss-g-fm: 7.4206 Loss-g-mel: 8.3101 Loss-g-dur: 0.7315 Loss-g-kl: 1.4384 lr: 0.0002 grad_norm_g: 1453.7436 grad_norm_d: 27.3110
======> Epoch: 1766
Train Epoch: 1767 [34.62%] G-Loss: 34.4223 D-Loss: 2.1874 Loss-g-fm: 11.8153 Loss-g-mel: 17.3714 Loss-g-dur: 1.2972 Loss-g-kl: 1.5109 lr: 0.0002 grad_norm_g: 644.3904 grad_norm_d: 47.2928
======> Epoch: 1767
Train Epoch: 1768 [30.77%] G-Loss: 32.1675 D-Loss: 2.1122 Loss-g-fm: 9.4604 Loss-g-mel: 17.3043 Loss-g-dur: 1.1931 Loss-g-kl: 1.4160 lr: 0.0002 grad_norm_g: 633.9207 grad_norm_d: 56.7385
======> Epoch: 1768
Train Epoch: 1769 [26.92%] G-Loss: 32.2567 D-Loss: 2.1977 Loss-g-fm: 9.5986 Loss-g-mel: 17.6252 Loss-g-dur: 1.2238 Loss-g-kl: 1.5894 lr: 0.0002 grad_norm_g: 253.2185 grad_norm_d: 16.1725
======> Epoch: 1769
Train Epoch: 1770 [23.08%] G-Loss: 34.5286 D-Loss: 2.2071 Loss-g-fm: 10.7583 Loss-g-mel: 17.8911 Loss-g-dur: 1.2513 Loss-g-kl: 1.7998 lr: 0.0002 grad_norm_g: 755.3912 grad_norm_d: 35.9157
Saving model and optimizer state at iteration 1770 to /ZFS4T/tts/data/VITS/model_saved/G_46000.pth
Saving model and optimizer state at iteration 1770 to /ZFS4T/tts/data/VITS/model_saved/D_46000.pth
======> Epoch: 1770
Train Epoch: 1771 [19.23%] G-Loss: 32.3633 D-Loss: 2.1655 Loss-g-fm: 9.9197 Loss-g-mel: 16.9798 Loss-g-dur: 1.2171 Loss-g-kl: 1.1605 lr: 0.0002 grad_norm_g: 896.8689 grad_norm_d: 54.0581
======> Epoch: 1771
Train Epoch: 1772 [15.38%] G-Loss: 32.1385 D-Loss: 2.3854 Loss-g-fm: 9.6456 Loss-g-mel: 16.8634 Loss-g-dur: 1.1334 Loss-g-kl: 1.3683 lr: 0.0002 grad_norm_g: 702.5283 grad_norm_d: 59.9688
======> Epoch: 1772
Train Epoch: 1773 [11.54%] G-Loss: 30.5271 D-Loss: 2.3368 Loss-g-fm: 8.9172 Loss-g-mel: 16.4484 Loss-g-dur: 1.2036 Loss-g-kl: 1.1813 lr: 0.0002 grad_norm_g: 201.2316 grad_norm_d: 42.6670
======> Epoch: 1773
Train Epoch: 1774 [7.69%] G-Loss: 33.5453 D-Loss: 2.1877 Loss-g-fm: 10.4074 Loss-g-mel: 17.1830 Loss-g-dur: 1.2503 Loss-g-kl: 1.6036 lr: 0.0002 grad_norm_g: 416.8099 grad_norm_d: 36.5785
======> Epoch: 1774
Train Epoch: 1775 [3.85%] G-Loss: 36.7843 D-Loss: 2.0935 Loss-g-fm: 12.0986 Loss-g-mel: 18.8819 Loss-g-dur: 1.3002 Loss-g-kl: 1.4093 lr: 0.0002 grad_norm_g: 856.0843 grad_norm_d: 59.6610
======> Epoch: 1775
Train Epoch: 1776 [0.00%] G-Loss: 17.4103 D-Loss: 2.4133 Loss-g-fm: 5.4274 Loss-g-mel: 7.8473 Loss-g-dur: 0.6683 Loss-g-kl: 1.4023 lr: 0.0002 grad_norm_g: 649.1942 grad_norm_d: 35.7207
Train Epoch: 1776 [96.15%] G-Loss: 34.8208 D-Loss: 2.0014 Loss-g-fm: 11.8364 Loss-g-mel: 17.8051 Loss-g-dur: 1.2198 Loss-g-kl: 1.2337 lr: 0.0002 grad_norm_g: 429.1306 grad_norm_d: 30.4632
======> Epoch: 1776
Train Epoch: 1777 [92.31%] G-Loss: 32.0059 D-Loss: 2.2059 Loss-g-fm: 9.4487 Loss-g-mel: 17.3816 Loss-g-dur: 1.2115 Loss-g-kl: 1.3564 lr: 0.0002 grad_norm_g: 770.1931 grad_norm_d: 57.0430
======> Epoch: 1777
Train Epoch: 1778 [88.46%] G-Loss: 34.2911 D-Loss: 2.1090 Loss-g-fm: 10.9027 Loss-g-mel: 18.0470 Loss-g-dur: 1.2204 Loss-g-kl: 1.4237 lr: 0.0002 grad_norm_g: 950.5625 grad_norm_d: 39.4305
======> Epoch: 1778
Train Epoch: 1779 [84.62%] G-Loss: 33.3682 D-Loss: 2.2809 Loss-g-fm: 10.4961 Loss-g-mel: 17.4259 Loss-g-dur: 1.2545 Loss-g-kl: 1.4977 lr: 0.0002 grad_norm_g: 916.7344 grad_norm_d: 45.8744
======> Epoch: 1779
Train Epoch: 1780 [80.77%] G-Loss: 32.7802 D-Loss: 2.2502 Loss-g-fm: 10.1808 Loss-g-mel: 17.3146 Loss-g-dur: 1.1987 Loss-g-kl: 1.3255 lr: 0.0002 grad_norm_g: 747.3111 grad_norm_d: 80.2868
======> Epoch: 1780
Train Epoch: 1781 [76.92%] G-Loss: 34.9280 D-Loss: 2.2435 Loss-g-fm: 11.1741 Loss-g-mel: 17.9883 Loss-g-dur: 1.2301 Loss-g-kl: 1.5149 lr: 0.0002 grad_norm_g: 296.5923 grad_norm_d: 25.9855
======> Epoch: 1781
Train Epoch: 1782 [73.08%] G-Loss: 31.8148 D-Loss: 2.1527 Loss-g-fm: 9.2786 Loss-g-mel: 17.4603 Loss-g-dur: 1.1992 Loss-g-kl: 1.3617 lr: 0.0002 grad_norm_g: 687.8800 grad_norm_d: 71.6052
======> Epoch: 1782
Train Epoch: 1783 [69.23%] G-Loss: 34.9155 D-Loss: 2.1972 Loss-g-fm: 11.6469 Loss-g-mel: 17.5563 Loss-g-dur: 1.3592 Loss-g-kl: 1.7063 lr: 0.0002 grad_norm_g: 937.0957 grad_norm_d: 44.7435
======> Epoch: 1783
Train Epoch: 1784 [65.38%] G-Loss: 34.5896 D-Loss: 2.1993 Loss-g-fm: 11.0400 Loss-g-mel: 18.1068 Loss-g-dur: 1.2453 Loss-g-kl: 1.4825 lr: 0.0002 grad_norm_g: 825.8937 grad_norm_d: 54.8914
======> Epoch: 1784
Train Epoch: 1785 [61.54%] G-Loss: 31.4650 D-Loss: 2.1116 Loss-g-fm: 9.5505 Loss-g-mel: 16.7695 Loss-g-dur: 1.2328 Loss-g-kl: 1.4183 lr: 0.0002 grad_norm_g: 691.8666 grad_norm_d: 46.0619
======> Epoch: 1785
Train Epoch: 1786 [57.69%] G-Loss: 34.9604 D-Loss: 2.1933 Loss-g-fm: 11.3351 Loss-g-mel: 18.0439 Loss-g-dur: 1.2820 Loss-g-kl: 1.5405 lr: 0.0002 grad_norm_g: 488.4583 grad_norm_d: 10.6141
======> Epoch: 1786
Train Epoch: 1787 [53.85%] G-Loss: 30.7889 D-Loss: 2.2395 Loss-g-fm: 9.1291 Loss-g-mel: 16.3126 Loss-g-dur: 1.2435 Loss-g-kl: 1.5385 lr: 0.0002 grad_norm_g: 430.4201 grad_norm_d: 59.1923
======> Epoch: 1787
Train Epoch: 1788 [50.00%] G-Loss: 32.1372 D-Loss: 2.4055 Loss-g-fm: 9.4217 Loss-g-mel: 17.4100 Loss-g-dur: 1.2240 Loss-g-kl: 1.6670 lr: 0.0002 grad_norm_g: 458.5634 grad_norm_d: 111.0821
======> Epoch: 1788
Train Epoch: 1789 [46.15%] G-Loss: 34.2741 D-Loss: 2.1237 Loss-g-fm: 10.8898 Loss-g-mel: 17.9968 Loss-g-dur: 1.3107 Loss-g-kl: 1.5087 lr: 0.0002 grad_norm_g: 99.2595 grad_norm_d: 5.3191
======> Epoch: 1789
Train Epoch: 1790 [42.31%] G-Loss: 34.5274 D-Loss: 2.1350 Loss-g-fm: 11.3719 Loss-g-mel: 17.7177 Loss-g-dur: 1.3415 Loss-g-kl: 1.6245 lr: 0.0002 grad_norm_g: 866.4946 grad_norm_d: 51.9560
======> Epoch: 1790
Train Epoch: 1791 [38.46%] G-Loss: 33.4349 D-Loss: 2.1886 Loss-g-fm: 10.3256 Loss-g-mel: 17.8030 Loss-g-dur: 1.2369 Loss-g-kl: 1.5968 lr: 0.0002 grad_norm_g: 309.7058 grad_norm_d: 11.0503
======> Epoch: 1791
Train Epoch: 1792 [34.62%] G-Loss: 34.7317 D-Loss: 2.1716 Loss-g-fm: 11.2290 Loss-g-mel: 18.3259 Loss-g-dur: 1.2606 Loss-g-kl: 1.5662 lr: 0.0002 grad_norm_g: 335.4916 grad_norm_d: 25.6081
======> Epoch: 1792
Train Epoch: 1793 [30.77%] G-Loss: 33.3618 D-Loss: 2.2029 Loss-g-fm: 10.4557 Loss-g-mel: 17.6347 Loss-g-dur: 1.1748 Loss-g-kl: 1.3399 lr: 0.0002 grad_norm_g: 377.5074 grad_norm_d: 7.6558
======> Epoch: 1793
Train Epoch: 1794 [26.92%] G-Loss: 31.7497 D-Loss: 2.1925 Loss-g-fm: 9.4378 Loss-g-mel: 16.6826 Loss-g-dur: 1.4959 Loss-g-kl: 1.3946 lr: 0.0002 grad_norm_g: 70.0438 grad_norm_d: 55.8899
======> Epoch: 1794
Train Epoch: 1795 [23.08%] G-Loss: 34.0658 D-Loss: 2.2077 Loss-g-fm: 10.3122 Loss-g-mel: 18.2192 Loss-g-dur: 1.2548 Loss-g-kl: 1.5585 lr: 0.0002 grad_norm_g: 286.9603 grad_norm_d: 46.7305
======> Epoch: 1795
Train Epoch: 1796 [19.23%] G-Loss: 34.6093 D-Loss: 2.1627 Loss-g-fm: 11.0677 Loss-g-mel: 17.9665 Loss-g-dur: 1.3489 Loss-g-kl: 1.5411 lr: 0.0002 grad_norm_g: 948.1990 grad_norm_d: 67.1724
======> Epoch: 1796
Train Epoch: 1797 [15.38%] G-Loss: 35.7805 D-Loss: 2.2138 Loss-g-fm: 11.4291 Loss-g-mel: 18.4314 Loss-g-dur: 1.2921 Loss-g-kl: 1.6231 lr: 0.0002 grad_norm_g: 929.8429 grad_norm_d: 47.4873
======> Epoch: 1797
Train Epoch: 1798 [11.54%] G-Loss: 32.7865 D-Loss: 2.2835 Loss-g-fm: 10.0763 Loss-g-mel: 17.4292 Loss-g-dur: 1.1957 Loss-g-kl: 1.4496 lr: 0.0002 grad_norm_g: 1004.3782 grad_norm_d: 89.2998
======> Epoch: 1798
Train Epoch: 1799 [7.69%] G-Loss: 35.6614 D-Loss: 2.2245 Loss-g-fm: 12.2141 Loss-g-mel: 17.8760 Loss-g-dur: 1.2735 Loss-g-kl: 1.6248 lr: 0.0002 grad_norm_g: 711.0403 grad_norm_d: 21.4237
======> Epoch: 1799
Train Epoch: 1800 [3.85%] G-Loss: 30.6073 D-Loss: 2.2702 Loss-g-fm: 8.8672 Loss-g-mel: 16.8670 Loss-g-dur: 1.1655 Loss-g-kl: 1.2345 lr: 0.0002 grad_norm_g: 765.5055 grad_norm_d: 75.5404
======> Epoch: 1800
Train Epoch: 1801 [0.00%] G-Loss: 37.1627 D-Loss: 2.0815 Loss-g-fm: 12.8515 Loss-g-mel: 18.6646 Loss-g-dur: 1.3666 Loss-g-kl: 1.4848 lr: 0.0002 grad_norm_g: 949.5177 grad_norm_d: 43.8658
Train Epoch: 1801 [96.15%] G-Loss: 34.8649 D-Loss: 2.2041 Loss-g-fm: 11.1018 Loss-g-mel: 17.8924 Loss-g-dur: 1.2842 Loss-g-kl: 1.7406 lr: 0.0002 grad_norm_g: 445.7812 grad_norm_d: 34.8691
======> Epoch: 1801
Train Epoch: 1802 [92.31%] G-Loss: 36.7342 D-Loss: 2.2388 Loss-g-fm: 12.0108 Loss-g-mel: 18.9793 Loss-g-dur: 1.3047 Loss-g-kl: 1.6699 lr: 0.0002 grad_norm_g: 851.7730 grad_norm_d: 36.9996
======> Epoch: 1802
Train Epoch: 1803 [88.46%] G-Loss: 35.2607 D-Loss: 2.1619 Loss-g-fm: 11.2011 Loss-g-mel: 18.2603 Loss-g-dur: 1.3456 Loss-g-kl: 1.7523 lr: 0.0002 grad_norm_g: 940.9758 grad_norm_d: 57.9807
======> Epoch: 1803
Train Epoch: 1804 [84.62%] G-Loss: 32.9451 D-Loss: 2.3021 Loss-g-fm: 10.2021 Loss-g-mel: 17.2539 Loss-g-dur: 1.2095 Loss-g-kl: 1.4335 lr: 0.0002 grad_norm_g: 833.0340 grad_norm_d: 122.3359
======> Epoch: 1804
Train Epoch: 1805 [80.77%] G-Loss: 35.3455 D-Loss: 2.0737 Loss-g-fm: 11.5798 Loss-g-mel: 18.0980 Loss-g-dur: 1.2619 Loss-g-kl: 1.5992 lr: 0.0002 grad_norm_g: 1025.5537 grad_norm_d: 57.9670
======> Epoch: 1805
Train Epoch: 1806 [76.92%] G-Loss: 32.8836 D-Loss: 2.1032 Loss-g-fm: 10.0048 Loss-g-mel: 17.3964 Loss-g-dur: 1.1760 Loss-g-kl: 1.4330 lr: 0.0002 grad_norm_g: 981.8860 grad_norm_d: 46.2952
======> Epoch: 1806
Train Epoch: 1807 [73.08%] G-Loss: 33.5568 D-Loss: 2.2726 Loss-g-fm: 11.0949 Loss-g-mel: 17.1540 Loss-g-dur: 1.1432 Loss-g-kl: 1.1457 lr: 0.0002 grad_norm_g: 896.0467 grad_norm_d: 24.4493
======> Epoch: 1807
Train Epoch: 1808 [69.23%] G-Loss: 34.4030 D-Loss: 2.2219 Loss-g-fm: 10.9113 Loss-g-mel: 17.5202 Loss-g-dur: 1.2542 Loss-g-kl: 1.7362 lr: 0.0002 grad_norm_g: 879.8913 grad_norm_d: 36.8411
======> Epoch: 1808
Train Epoch: 1809 [65.38%] G-Loss: 35.9875 D-Loss: 2.1910 Loss-g-fm: 11.8381 Loss-g-mel: 18.6959 Loss-g-dur: 1.2350 Loss-g-kl: 1.5654 lr: 0.0002 grad_norm_g: 330.9511 grad_norm_d: 53.3723
======> Epoch: 1809
Train Epoch: 1810 [61.54%] G-Loss: 33.4439 D-Loss: 2.1597 Loss-g-fm: 10.6593 Loss-g-mel: 17.6461 Loss-g-dur: 1.2088 Loss-g-kl: 1.3831 lr: 0.0002 grad_norm_g: 541.9937 grad_norm_d: 51.4640
======> Epoch: 1810
Train Epoch: 1811 [57.69%] G-Loss: 19.3818 D-Loss: 2.2797 Loss-g-fm: 6.5446 Loss-g-mel: 8.2548 Loss-g-dur: 0.7130 Loss-g-kl: 1.3051 lr: 0.0002 grad_norm_g: 1594.9181 grad_norm_d: 68.2453
======> Epoch: 1811
Train Epoch: 1812 [53.85%] G-Loss: 33.5264 D-Loss: 2.1087 Loss-g-fm: 10.7484 Loss-g-mel: 17.2836 Loss-g-dur: 1.2636 Loss-g-kl: 1.4063 lr: 0.0002 grad_norm_g: 582.9747 grad_norm_d: 34.6798
======> Epoch: 1812
Train Epoch: 1813 [50.00%] G-Loss: 28.0066 D-Loss: 2.2230 Loss-g-fm: 8.5906 Loss-g-mel: 14.1516 Loss-g-dur: 1.0466 Loss-g-kl: 1.4181 lr: 0.0002 grad_norm_g: 722.4682 grad_norm_d: 73.9312
======> Epoch: 1813
Train Epoch: 1814 [46.15%] G-Loss: 37.0327 D-Loss: 2.2745 Loss-g-fm: 12.8628 Loss-g-mel: 18.6568 Loss-g-dur: 1.3112 Loss-g-kl: 1.5087 lr: 0.0002 grad_norm_g: 559.3614 grad_norm_d: 11.4426
======> Epoch: 1814
Train Epoch: 1815 [42.31%] G-Loss: 33.3414 D-Loss: 2.1251 Loss-g-fm: 10.7506 Loss-g-mel: 17.3156 Loss-g-dur: 1.1539 Loss-g-kl: 1.1796 lr: 0.0002 grad_norm_g: 1015.6063 grad_norm_d: 87.6740
======> Epoch: 1815
Train Epoch: 1816 [38.46%] G-Loss: 35.0461 D-Loss: 2.0986 Loss-g-fm: 11.5780 Loss-g-mel: 18.0056 Loss-g-dur: 1.1614 Loss-g-kl: 1.5148 lr: 0.0002 grad_norm_g: 565.1380 grad_norm_d: 18.2673
======> Epoch: 1816
Train Epoch: 1817 [34.62%] G-Loss: 27.8339 D-Loss: 2.2831 Loss-g-fm: 8.8877 Loss-g-mel: 14.0095 Loss-g-dur: 0.9558 Loss-g-kl: 1.5160 lr: 0.0002 grad_norm_g: 857.0261 grad_norm_d: 45.3634
======> Epoch: 1817
Train Epoch: 1818 [30.77%] G-Loss: 36.9048 D-Loss: 2.0264 Loss-g-fm: 12.4137 Loss-g-mel: 18.8185 Loss-g-dur: 1.3676 Loss-g-kl: 1.4668 lr: 0.0002 grad_norm_g: 787.5744 grad_norm_d: 28.2194
======> Epoch: 1818
Train Epoch: 1819 [26.92%] G-Loss: 35.2741 D-Loss: 2.1142 Loss-g-fm: 11.9146 Loss-g-mel: 17.4297 Loss-g-dur: 1.3197 Loss-g-kl: 1.6290 lr: 0.0002 grad_norm_g: 588.0483 grad_norm_d: 15.6577
======> Epoch: 1819
Train Epoch: 1820 [23.08%] G-Loss: 32.5529 D-Loss: 2.1232 Loss-g-fm: 9.9707 Loss-g-mel: 17.1440 Loss-g-dur: 1.1719 Loss-g-kl: 1.2078 lr: 0.0002 grad_norm_g: 881.0235 grad_norm_d: 13.4512
======> Epoch: 1820
Train Epoch: 1821 [19.23%] G-Loss: 33.4213 D-Loss: 2.1583 Loss-g-fm: 11.2839 Loss-g-mel: 16.8607 Loss-g-dur: 1.1833 Loss-g-kl: 1.4319 lr: 0.0002 grad_norm_g: 842.2795 grad_norm_d: 79.4532
======> Epoch: 1821
Train Epoch: 1822 [15.38%] G-Loss: 31.3511 D-Loss: 2.1851 Loss-g-fm: 9.0284 Loss-g-mel: 17.2423 Loss-g-dur: 1.1806 Loss-g-kl: 1.4057 lr: 0.0002 grad_norm_g: 282.5272 grad_norm_d: 22.8325
======> Epoch: 1822
Train Epoch: 1823 [11.54%] G-Loss: 26.3604 D-Loss: 2.3383 Loss-g-fm: 7.9146 Loss-g-mel: 13.8518 Loss-g-dur: 0.9882 Loss-g-kl: 1.3738 lr: 0.0002 grad_norm_g: 334.4792 grad_norm_d: 20.8670
======> Epoch: 1823
Train Epoch: 1824 [7.69%] G-Loss: 32.2019 D-Loss: 2.1628 Loss-g-fm: 9.7797 Loss-g-mel: 17.3498 Loss-g-dur: 1.1895 Loss-g-kl: 1.2677 lr: 0.0002 grad_norm_g: 808.3361 grad_norm_d: 38.9662
======> Epoch: 1824
Train Epoch: 1825 [3.85%] G-Loss: 32.6505 D-Loss: 2.2534 Loss-g-fm: 10.1574 Loss-g-mel: 16.8717 Loss-g-dur: 1.1654 Loss-g-kl: 1.4418 lr: 0.0002 grad_norm_g: 974.7922 grad_norm_d: 69.5874
======> Epoch: 1825
Train Epoch: 1826 [0.00%] G-Loss: 29.5886 D-Loss: 2.0670 Loss-g-fm: 9.7539 Loss-g-mel: 14.7238 Loss-g-dur: 1.0168 Loss-g-kl: 1.6067 lr: 0.0002 grad_norm_g: 436.2752 grad_norm_d: 12.8131
Train Epoch: 1826 [96.15%] G-Loss: 35.2462 D-Loss: 2.1161 Loss-g-fm: 11.4085 Loss-g-mel: 18.0824 Loss-g-dur: 1.3539 Loss-g-kl: 1.5895 lr: 0.0002 grad_norm_g: 926.0137 grad_norm_d: 40.1762
======> Epoch: 1826
Train Epoch: 1827 [92.31%] G-Loss: 36.1713 D-Loss: 2.3198 Loss-g-fm: 11.3044 Loss-g-mel: 18.5943 Loss-g-dur: 1.3344 Loss-g-kl: 1.6703 lr: 0.0002 grad_norm_g: 117.5738 grad_norm_d: 23.5036
======> Epoch: 1827
Train Epoch: 1828 [88.46%] G-Loss: 33.3177 D-Loss: 2.2126 Loss-g-fm: 10.7207 Loss-g-mel: 17.3609 Loss-g-dur: 1.2760 Loss-g-kl: 1.3593 lr: 0.0002 grad_norm_g: 974.6333 grad_norm_d: 57.2953
======> Epoch: 1828
Train Epoch: 1829 [84.62%] G-Loss: 33.2107 D-Loss: 2.1864 Loss-g-fm: 10.5587 Loss-g-mel: 17.3493 Loss-g-dur: 1.2684 Loss-g-kl: 1.6367 lr: 0.0002 grad_norm_g: 624.9015 grad_norm_d: 50.0674
======> Epoch: 1829
Train Epoch: 1830 [80.77%] G-Loss: 36.0819 D-Loss: 1.9961 Loss-g-fm: 12.5233 Loss-g-mel: 17.7663 Loss-g-dur: 1.3448 Loss-g-kl: 1.5633 lr: 0.0002 grad_norm_g: 451.9162 grad_norm_d: 28.8008
======> Epoch: 1830
Train Epoch: 1831 [76.92%] G-Loss: 31.2057 D-Loss: 2.3517 Loss-g-fm: 9.3068 Loss-g-mel: 17.0089 Loss-g-dur: 1.1655 Loss-g-kl: 1.3150 lr: 0.0002 grad_norm_g: 975.3477 grad_norm_d: 94.1804
======> Epoch: 1831
Train Epoch: 1832 [73.08%] G-Loss: 33.1172 D-Loss: 2.1220 Loss-g-fm: 10.5344 Loss-g-mel: 17.2160 Loss-g-dur: 1.1978 Loss-g-kl: 1.3779 lr: 0.0002 grad_norm_g: 703.7541 grad_norm_d: 59.6852
======> Epoch: 1832
Train Epoch: 1833 [69.23%] G-Loss: 33.1782 D-Loss: 2.1806 Loss-g-fm: 10.2574 Loss-g-mel: 17.5259 Loss-g-dur: 1.2618 Loss-g-kl: 1.4746 lr: 0.0002 grad_norm_g: 504.6486 grad_norm_d: 21.1400
======> Epoch: 1833
Train Epoch: 1834 [65.38%] G-Loss: 35.1003 D-Loss: 2.3640 Loss-g-fm: 11.3922 Loss-g-mel: 17.9278 Loss-g-dur: 1.2843 Loss-g-kl: 1.5086 lr: 0.0002 grad_norm_g: 609.7991 grad_norm_d: 101.2385
======> Epoch: 1834
Train Epoch: 1835 [61.54%] G-Loss: 31.2273 D-Loss: 2.3210 Loss-g-fm: 9.3003 Loss-g-mel: 17.0707 Loss-g-dur: 1.2004 Loss-g-kl: 1.2845 lr: 0.0002 grad_norm_g: 754.8402 grad_norm_d: 59.7101
======> Epoch: 1835
Train Epoch: 1836 [57.69%] G-Loss: 31.8227 D-Loss: 2.2941 Loss-g-fm: 9.2345 Loss-g-mel: 17.2845 Loss-g-dur: 1.1485 Loss-g-kl: 1.4944 lr: 0.0002 grad_norm_g: 517.3809 grad_norm_d: 5.4315
======> Epoch: 1836
Train Epoch: 1837 [53.85%] G-Loss: 34.4412 D-Loss: 2.1757 Loss-g-fm: 11.6099 Loss-g-mel: 17.5060 Loss-g-dur: 1.2016 Loss-g-kl: 1.3957 lr: 0.0002 grad_norm_g: 854.5821 grad_norm_d: 67.9475
======> Epoch: 1837
Train Epoch: 1838 [50.00%] G-Loss: 34.1983 D-Loss: 2.0723 Loss-g-fm: 11.0261 Loss-g-mel: 17.4323 Loss-g-dur: 1.2099 Loss-g-kl: 1.4429 lr: 0.0002 grad_norm_g: 531.3095 grad_norm_d: 49.0611
======> Epoch: 1838
Train Epoch: 1839 [46.15%] G-Loss: 35.9131 D-Loss: 2.2201 Loss-g-fm: 11.7626 Loss-g-mel: 18.2199 Loss-g-dur: 1.3495 Loss-g-kl: 1.6516 lr: 0.0002 grad_norm_g: 1041.9943 grad_norm_d: 78.4814
======> Epoch: 1839
Train Epoch: 1840 [42.31%] G-Loss: 34.0050 D-Loss: 2.4524 Loss-g-fm: 10.2891 Loss-g-mel: 17.9844 Loss-g-dur: 1.3784 Loss-g-kl: 1.7639 lr: 0.0002 grad_norm_g: 133.7052 grad_norm_d: 69.9907
======> Epoch: 1840
Train Epoch: 1841 [38.46%] G-Loss: 33.2763 D-Loss: 2.2213 Loss-g-fm: 10.2328 Loss-g-mel: 17.7111 Loss-g-dur: 1.3028 Loss-g-kl: 1.4409 lr: 0.0002 grad_norm_g: 815.4090 grad_norm_d: 58.7331
======> Epoch: 1841
Train Epoch: 1842 [34.62%] G-Loss: 34.9542 D-Loss: 2.2298 Loss-g-fm: 11.2581 Loss-g-mel: 18.1679 Loss-g-dur: 1.2875 Loss-g-kl: 1.4706 lr: 0.0002 grad_norm_g: 372.3772 grad_norm_d: 9.6018
======> Epoch: 1842
Train Epoch: 1843 [30.77%] G-Loss: 34.8308 D-Loss: 2.2535 Loss-g-fm: 10.8065 Loss-g-mel: 18.2690 Loss-g-dur: 1.3323 Loss-g-kl: 1.6440 lr: 0.0002 grad_norm_g: 950.9388 grad_norm_d: 57.0304
======> Epoch: 1843
Train Epoch: 1844 [26.92%] G-Loss: 32.9069 D-Loss: 2.1617 Loss-g-fm: 10.7169 Loss-g-mel: 17.0905 Loss-g-dur: 1.1724 Loss-g-kl: 1.3222 lr: 0.0002 grad_norm_g: 991.4992 grad_norm_d: 68.2772
======> Epoch: 1844
Train Epoch: 1845 [23.08%] G-Loss: 31.9171 D-Loss: 2.1602 Loss-g-fm: 9.6525 Loss-g-mel: 17.3781 Loss-g-dur: 1.1581 Loss-g-kl: 1.2633 lr: 0.0002 grad_norm_g: 465.6331 grad_norm_d: 30.1599
======> Epoch: 1845
Train Epoch: 1846 [19.23%] G-Loss: 33.4626 D-Loss: 2.1637 Loss-g-fm: 10.7312 Loss-g-mel: 17.5939 Loss-g-dur: 1.1662 Loss-g-kl: 1.3096 lr: 0.0002 grad_norm_g: 632.9033 grad_norm_d: 72.1773
======> Epoch: 1846
Train Epoch: 1847 [15.38%] G-Loss: 32.3239 D-Loss: 2.3406 Loss-g-fm: 10.5639 Loss-g-mel: 17.0295 Loss-g-dur: 1.1591 Loss-g-kl: 1.2031 lr: 0.0002 grad_norm_g: 329.2019 grad_norm_d: 16.3324
Saving model and optimizer state at iteration 1847 to /ZFS4T/tts/data/VITS/model_saved/G_48000.pth
Saving model and optimizer state at iteration 1847 to /ZFS4T/tts/data/VITS/model_saved/D_48000.pth
======> Epoch: 1847
Train Epoch: 1848 [11.54%] G-Loss: 34.7715 D-Loss: 2.0927 Loss-g-fm: 11.4536 Loss-g-mel: 17.8091 Loss-g-dur: 1.1765 Loss-g-kl: 1.5396 lr: 0.0002 grad_norm_g: 951.1551 grad_norm_d: 41.8647
======> Epoch: 1848
Train Epoch: 1849 [7.69%] G-Loss: 31.1350 D-Loss: 2.1286 Loss-g-fm: 9.4790 Loss-g-mel: 16.6799 Loss-g-dur: 1.1885 Loss-g-kl: 1.3777 lr: 0.0002 grad_norm_g: 544.6190 grad_norm_d: 36.8226
======> Epoch: 1849
Train Epoch: 1850 [3.85%] G-Loss: 31.6565 D-Loss: 2.2051 Loss-g-fm: 9.7897 Loss-g-mel: 16.7556 Loss-g-dur: 1.1673 Loss-g-kl: 1.2737 lr: 0.0002 grad_norm_g: 1031.9442 grad_norm_d: 56.5698
======> Epoch: 1850
Train Epoch: 1851 [0.00%] G-Loss: 36.1571 D-Loss: 2.0621 Loss-g-fm: 12.1345 Loss-g-mel: 18.2963 Loss-g-dur: 1.2570 Loss-g-kl: 1.5837 lr: 0.0002 grad_norm_g: 1030.8354 grad_norm_d: 50.6945
Train Epoch: 1851 [96.15%] G-Loss: 34.0433 D-Loss: 2.1014 Loss-g-fm: 10.7296 Loss-g-mel: 17.5156 Loss-g-dur: 1.2418 Loss-g-kl: 1.4980 lr: 0.0002 grad_norm_g: 889.1460 grad_norm_d: 72.1689
======> Epoch: 1851
Train Epoch: 1852 [92.31%] G-Loss: 33.7118 D-Loss: 2.1575 Loss-g-fm: 10.4089 Loss-g-mel: 17.7157 Loss-g-dur: 1.3054 Loss-g-kl: 1.3024 lr: 0.0002 grad_norm_g: 902.0406 grad_norm_d: 60.0890
======> Epoch: 1852
Train Epoch: 1853 [88.46%] G-Loss: 32.7145 D-Loss: 2.1633 Loss-g-fm: 9.5632 Loss-g-mel: 17.2172 Loss-g-dur: 1.2904 Loss-g-kl: 1.5508 lr: 0.0002 grad_norm_g: 529.9537 grad_norm_d: 35.7615
======> Epoch: 1853
Train Epoch: 1854 [84.62%] G-Loss: 33.6330 D-Loss: 2.1809 Loss-g-fm: 11.0194 Loss-g-mel: 17.9216 Loss-g-dur: 1.1938 Loss-g-kl: 1.1449 lr: 0.0002 grad_norm_g: 852.2646 grad_norm_d: 60.1963
======> Epoch: 1854
Train Epoch: 1855 [80.77%] G-Loss: 33.2182 D-Loss: 2.1852 Loss-g-fm: 10.6418 Loss-g-mel: 17.4276 Loss-g-dur: 1.1906 Loss-g-kl: 1.3774 lr: 0.0002 grad_norm_g: 905.5606 grad_norm_d: 70.3745
======> Epoch: 1855
Train Epoch: 1856 [76.92%] G-Loss: 30.4935 D-Loss: 2.2469 Loss-g-fm: 8.8032 Loss-g-mel: 16.4637 Loss-g-dur: 1.2215 Loss-g-kl: 1.4222 lr: 0.0002 grad_norm_g: 1156.8292 grad_norm_d: 72.0670
======> Epoch: 1856
Train Epoch: 1857 [73.08%] G-Loss: 26.1053 D-Loss: 2.2720 Loss-g-fm: 7.8573 Loss-g-mel: 13.5811 Loss-g-dur: 1.0150 Loss-g-kl: 1.3060 lr: 0.0002 grad_norm_g: 931.3365 grad_norm_d: 49.4782
======> Epoch: 1857
Train Epoch: 1858 [69.23%] G-Loss: 32.7067 D-Loss: 2.0778 Loss-g-fm: 10.2066 Loss-g-mel: 17.0118 Loss-g-dur: 1.2225 Loss-g-kl: 1.4265 lr: 0.0002 grad_norm_g: 992.1559 grad_norm_d: 85.2743
======> Epoch: 1858
Train Epoch: 1859 [65.38%] G-Loss: 35.8611 D-Loss: 2.1228 Loss-g-fm: 11.7216 Loss-g-mel: 18.6097 Loss-g-dur: 1.2785 Loss-g-kl: 1.6215 lr: 0.0002 grad_norm_g: 212.2343 grad_norm_d: 27.5784
======> Epoch: 1859
Train Epoch: 1860 [61.54%] G-Loss: 31.3820 D-Loss: 2.3727 Loss-g-fm: 9.3235 Loss-g-mel: 16.8222 Loss-g-dur: 1.2031 Loss-g-kl: 1.3520 lr: 0.0002 grad_norm_g: 264.8594 grad_norm_d: 67.4887
======> Epoch: 1860
Train Epoch: 1861 [57.69%] G-Loss: 32.8143 D-Loss: 2.0380 Loss-g-fm: 10.7257 Loss-g-mel: 16.9048 Loss-g-dur: 1.1462 Loss-g-kl: 1.2984 lr: 0.0002 grad_norm_g: 475.4994 grad_norm_d: 25.2110
======> Epoch: 1861
Train Epoch: 1862 [53.85%] G-Loss: 32.9205 D-Loss: 2.2353 Loss-g-fm: 10.6396 Loss-g-mel: 16.9237 Loss-g-dur: 1.1506 Loss-g-kl: 1.4860 lr: 0.0002 grad_norm_g: 252.6374 grad_norm_d: 15.2136
======> Epoch: 1862
Train Epoch: 1863 [50.00%] G-Loss: 35.9309 D-Loss: 2.1398 Loss-g-fm: 12.0668 Loss-g-mel: 18.4448 Loss-g-dur: 1.2508 Loss-g-kl: 1.5483 lr: 0.0002 grad_norm_g: 402.7211 grad_norm_d: 27.2425
======> Epoch: 1863
Train Epoch: 1864 [46.15%] G-Loss: 35.5406 D-Loss: 2.0403 Loss-g-fm: 11.7623 Loss-g-mel: 18.0309 Loss-g-dur: 1.3369 Loss-g-kl: 1.6877 lr: 0.0002 grad_norm_g: 812.9008 grad_norm_d: 43.3091
======> Epoch: 1864
Train Epoch: 1865 [42.31%] G-Loss: 33.1685 D-Loss: 2.1802 Loss-g-fm: 10.2492 Loss-g-mel: 17.4463 Loss-g-dur: 1.2841 Loss-g-kl: 1.6290 lr: 0.0002 grad_norm_g: 762.0351 grad_norm_d: 33.7098
======> Epoch: 1865
Train Epoch: 1866 [38.46%] G-Loss: 32.5829 D-Loss: 2.2714 Loss-g-fm: 9.8928 Loss-g-mel: 17.5936 Loss-g-dur: 1.1612 Loss-g-kl: 1.1511 lr: 0.0002 grad_norm_g: 826.1014 grad_norm_d: 99.9613
======> Epoch: 1866
Train Epoch: 1867 [34.62%] G-Loss: 34.0192 D-Loss: 2.2405 Loss-g-fm: 10.2245 Loss-g-mel: 18.4807 Loss-g-dur: 1.3358 Loss-g-kl: 1.4354 lr: 0.0002 grad_norm_g: 468.5367 grad_norm_d: 38.9415
======> Epoch: 1867
Train Epoch: 1868 [30.77%] G-Loss: 35.1600 D-Loss: 2.1451 Loss-g-fm: 11.2315 Loss-g-mel: 18.1517 Loss-g-dur: 1.3252 Loss-g-kl: 1.6374 lr: 0.0002 grad_norm_g: 526.6119 grad_norm_d: 36.4544
======> Epoch: 1868
Train Epoch: 1869 [26.92%] G-Loss: 32.0665 D-Loss: 2.2795 Loss-g-fm: 9.7954 Loss-g-mel: 16.5880 Loss-g-dur: 1.1451 Loss-g-kl: 1.4801 lr: 0.0002 grad_norm_g: 1019.0083 grad_norm_d: 73.5540
======> Epoch: 1869
Train Epoch: 1870 [23.08%] G-Loss: 32.5103 D-Loss: 2.1920 Loss-g-fm: 10.1019 Loss-g-mel: 16.8917 Loss-g-dur: 1.2476 Loss-g-kl: 1.7361 lr: 0.0002 grad_norm_g: 1010.0931 grad_norm_d: 69.1625
======> Epoch: 1870
Train Epoch: 1871 [19.23%] G-Loss: 33.5804 D-Loss: 2.2806 Loss-g-fm: 10.4215 Loss-g-mel: 17.9398 Loss-g-dur: 1.2358 Loss-g-kl: 1.3505 lr: 0.0002 grad_norm_g: 219.0894 grad_norm_d: 36.1667
======> Epoch: 1871
Train Epoch: 1872 [15.38%] G-Loss: 32.1650 D-Loss: 2.2728 Loss-g-fm: 9.8272 Loss-g-mel: 17.1350 Loss-g-dur: 1.1945 Loss-g-kl: 1.2235 lr: 0.0002 grad_norm_g: 992.0417 grad_norm_d: 87.4921
======> Epoch: 1872
Train Epoch: 1873 [11.54%] G-Loss: 33.1453 D-Loss: 2.3290 Loss-g-fm: 9.8708 Loss-g-mel: 17.6829 Loss-g-dur: 1.2534 Loss-g-kl: 1.4860 lr: 0.0002 grad_norm_g: 845.7410 grad_norm_d: 73.9618
======> Epoch: 1873
Train Epoch: 1874 [7.69%] G-Loss: 18.0267 D-Loss: 2.4524 Loss-g-fm: 5.5617 Loss-g-mel: 7.9496 Loss-g-dur: 0.6682 Loss-g-kl: 1.3973 lr: 0.0002 grad_norm_g: 949.3275 grad_norm_d: 28.2811
======> Epoch: 1874
Train Epoch: 1875 [3.85%] G-Loss: 32.9757 D-Loss: 2.3352 Loss-g-fm: 10.0449 Loss-g-mel: 17.2280 Loss-g-dur: 1.1554 Loss-g-kl: 1.5122 lr: 0.0002 grad_norm_g: 168.4714 grad_norm_d: 39.3609
======> Epoch: 1875
Train Epoch: 1876 [0.00%] G-Loss: 19.2205 D-Loss: 2.3827 Loss-g-fm: 5.8464 Loss-g-mel: 8.6243 Loss-g-dur: 0.7441 Loss-g-kl: 1.4478 lr: 0.0002 grad_norm_g: 1050.7101 grad_norm_d: 41.5647
Train Epoch: 1876 [96.15%] G-Loss: 33.2542 D-Loss: 2.1569 Loss-g-fm: 10.8444 Loss-g-mel: 17.3491 Loss-g-dur: 1.1684 Loss-g-kl: 1.2593 lr: 0.0002 grad_norm_g: 661.2852 grad_norm_d: 45.8538
======> Epoch: 1876
Train Epoch: 1877 [92.31%] G-Loss: 36.1423 D-Loss: 2.1166 Loss-g-fm: 12.0898 Loss-g-mel: 18.5096 Loss-g-dur: 1.2805 Loss-g-kl: 1.5018 lr: 0.0002 grad_norm_g: 700.3263 grad_norm_d: 24.0218
======> Epoch: 1877
Train Epoch: 1878 [88.46%] G-Loss: 32.8458 D-Loss: 2.2646 Loss-g-fm: 10.3360 Loss-g-mel: 17.0943 Loss-g-dur: 1.2452 Loss-g-kl: 1.5025 lr: 0.0002 grad_norm_g: 176.3508 grad_norm_d: 13.5204
======> Epoch: 1878
Train Epoch: 1879 [84.62%] G-Loss: 32.0759 D-Loss: 2.2601 Loss-g-fm: 10.1221 Loss-g-mel: 16.5736 Loss-g-dur: 1.1539 Loss-g-kl: 1.5217 lr: 0.0002 grad_norm_g: 508.0964 grad_norm_d: 18.0356
======> Epoch: 1879
Train Epoch: 1880 [80.77%] G-Loss: 32.6150 D-Loss: 2.1264 Loss-g-fm: 10.1879 Loss-g-mel: 17.0045 Loss-g-dur: 1.1389 Loss-g-kl: 1.4439 lr: 0.0002 grad_norm_g: 444.4108 grad_norm_d: 32.0254
======> Epoch: 1880
Train Epoch: 1881 [76.92%] G-Loss: 30.4574 D-Loss: 2.3316 Loss-g-fm: 8.9146 Loss-g-mel: 16.5410 Loss-g-dur: 1.2030 Loss-g-kl: 1.2958 lr: 0.0002 grad_norm_g: 133.2183 grad_norm_d: 40.5451
======> Epoch: 1881
Train Epoch: 1882 [73.08%] G-Loss: 32.7373 D-Loss: 2.1870 Loss-g-fm: 10.5651 Loss-g-mel: 16.8975 Loss-g-dur: 1.1968 Loss-g-kl: 1.5304 lr: 0.0002 grad_norm_g: 946.5480 grad_norm_d: 85.7523
======> Epoch: 1882
Train Epoch: 1883 [69.23%] G-Loss: 35.6968 D-Loss: 2.3680 Loss-g-fm: 11.0006 Loss-g-mel: 18.5582 Loss-g-dur: 1.3194 Loss-g-kl: 1.5242 lr: 0.0002 grad_norm_g: 948.1804 grad_norm_d: 59.6669
======> Epoch: 1883
Train Epoch: 1884 [65.38%] G-Loss: 35.3508 D-Loss: 2.2106 Loss-g-fm: 12.0889 Loss-g-mel: 17.7118 Loss-g-dur: 1.2659 Loss-g-kl: 1.6482 lr: 0.0002 grad_norm_g: 727.6218 grad_norm_d: 50.5203
======> Epoch: 1884
Train Epoch: 1885 [61.54%] G-Loss: 32.9397 D-Loss: 2.3331 Loss-g-fm: 9.9821 Loss-g-mel: 17.2988 Loss-g-dur: 1.2479 Loss-g-kl: 1.5291 lr: 0.0002 grad_norm_g: 767.5208 grad_norm_d: 79.7886
======> Epoch: 1885
Train Epoch: 1886 [57.69%] G-Loss: 35.3264 D-Loss: 2.1817 Loss-g-fm: 11.5370 Loss-g-mel: 18.5764 Loss-g-dur: 1.3474 Loss-g-kl: 1.6622 lr: 0.0002 grad_norm_g: 329.9291 grad_norm_d: 14.3205
======> Epoch: 1886
Train Epoch: 1887 [53.85%] G-Loss: 33.5912 D-Loss: 2.1243 Loss-g-fm: 10.7733 Loss-g-mel: 17.4220 Loss-g-dur: 1.1783 Loss-g-kl: 1.4315 lr: 0.0002 grad_norm_g: 190.4550 grad_norm_d: 12.6748
======> Epoch: 1887
Train Epoch: 1888 [50.00%] G-Loss: 34.3774 D-Loss: 2.2726 Loss-g-fm: 10.7413 Loss-g-mel: 18.3204 Loss-g-dur: 1.2309 Loss-g-kl: 1.5615 lr: 0.0002 grad_norm_g: 810.0063 grad_norm_d: 32.1697
======> Epoch: 1888
Train Epoch: 1889 [46.15%] G-Loss: 34.7766 D-Loss: 2.2248 Loss-g-fm: 11.1612 Loss-g-mel: 18.2313 Loss-g-dur: 1.1929 Loss-g-kl: 1.7034 lr: 0.0002 grad_norm_g: 860.6040 grad_norm_d: 42.0365
======> Epoch: 1889
Train Epoch: 1890 [42.31%] G-Loss: 34.7211 D-Loss: 2.2877 Loss-g-fm: 11.3487 Loss-g-mel: 17.7564 Loss-g-dur: 1.2437 Loss-g-kl: 1.6469 lr: 0.0002 grad_norm_g: 978.1797 grad_norm_d: 73.5046
======> Epoch: 1890
Train Epoch: 1891 [38.46%] G-Loss: 18.5094 D-Loss: 2.4807 Loss-g-fm: 5.9163 Loss-g-mel: 8.2403 Loss-g-dur: 0.6359 Loss-g-kl: 1.3910 lr: 0.0002 grad_norm_g: 1157.5803 grad_norm_d: 78.8674
======> Epoch: 1891
Train Epoch: 1892 [34.62%] G-Loss: 21.0384 D-Loss: 2.4313 Loss-g-fm: 6.9788 Loss-g-mel: 9.1635 Loss-g-dur: 0.7982 Loss-g-kl: 1.4238 lr: 0.0002 grad_norm_g: 118.1172 grad_norm_d: 41.0400
======> Epoch: 1892
Train Epoch: 1893 [30.77%] G-Loss: 35.7203 D-Loss: 2.3336 Loss-g-fm: 12.1725 Loss-g-mel: 18.0615 Loss-g-dur: 1.2698 Loss-g-kl: 1.5867 lr: 0.0002 grad_norm_g: 750.0639 grad_norm_d: 21.6663
======> Epoch: 1893
Train Epoch: 1894 [26.92%] G-Loss: 34.3284 D-Loss: 2.1816 Loss-g-fm: 11.0404 Loss-g-mel: 17.8795 Loss-g-dur: 1.2470 Loss-g-kl: 1.4926 lr: 0.0002 grad_norm_g: 662.6719 grad_norm_d: 67.6592
======> Epoch: 1894
Train Epoch: 1895 [23.08%] G-Loss: 32.3387 D-Loss: 2.1087 Loss-g-fm: 10.2516 Loss-g-mel: 17.2469 Loss-g-dur: 1.1764 Loss-g-kl: 1.2398 lr: 0.0002 grad_norm_g: 184.2616 grad_norm_d: 19.2175
======> Epoch: 1895
Train Epoch: 1896 [19.23%] G-Loss: 28.2617 D-Loss: 2.2815 Loss-g-fm: 9.4936 Loss-g-mel: 13.5632 Loss-g-dur: 1.0907 Loss-g-kl: 1.4427 lr: 0.0002 grad_norm_g: 664.5203 grad_norm_d: 47.5584
======> Epoch: 1896
Train Epoch: 1897 [15.38%] G-Loss: 26.8594 D-Loss: 2.3204 Loss-g-fm: 8.2293 Loss-g-mel: 13.5047 Loss-g-dur: 0.9855 Loss-g-kl: 1.3612 lr: 0.0002 grad_norm_g: 965.2985 grad_norm_d: 53.3370
======> Epoch: 1897
Train Epoch: 1898 [11.54%] G-Loss: 32.6795 D-Loss: 2.2889 Loss-g-fm: 10.2175 Loss-g-mel: 17.3888 Loss-g-dur: 1.1564 Loss-g-kl: 1.3934 lr: 0.0002 grad_norm_g: 151.4012 grad_norm_d: 26.1511
======> Epoch: 1898
Train Epoch: 1899 [7.69%] G-Loss: 32.5435 D-Loss: 2.2324 Loss-g-fm: 10.7661 Loss-g-mel: 16.5109 Loss-g-dur: 1.1651 Loss-g-kl: 1.4441 lr: 0.0002 grad_norm_g: 905.8387 grad_norm_d: 59.4195
======> Epoch: 1899
Train Epoch: 1900 [3.85%] G-Loss: 32.4495 D-Loss: 2.2959 Loss-g-fm: 10.2864 Loss-g-mel: 16.9452 Loss-g-dur: 1.1591 Loss-g-kl: 1.2588 lr: 0.0002 grad_norm_g: 837.1304 grad_norm_d: 38.7776
======> Epoch: 1900
Train Epoch: 1901 [0.00%] G-Loss: 35.4210 D-Loss: 2.2605 Loss-g-fm: 11.8518 Loss-g-mel: 17.8920 Loss-g-dur: 1.3143 Loss-g-kl: 1.6355 lr: 0.0002 grad_norm_g: 739.6968 grad_norm_d: 41.2576
Train Epoch: 1901 [96.15%] G-Loss: 33.4737 D-Loss: 2.1465 Loss-g-fm: 10.2481 Loss-g-mel: 17.7119 Loss-g-dur: 1.2807 Loss-g-kl: 1.5520 lr: 0.0002 grad_norm_g: 422.0431 grad_norm_d: 79.7874
======> Epoch: 1901
Train Epoch: 1902 [92.31%] G-Loss: 37.4845 D-Loss: 2.3393 Loss-g-fm: 12.6876 Loss-g-mel: 18.7803 Loss-g-dur: 1.2835 Loss-g-kl: 1.7211 lr: 0.0002 grad_norm_g: 835.3821 grad_norm_d: 41.8826
======> Epoch: 1902
Train Epoch: 1903 [88.46%] G-Loss: 32.3204 D-Loss: 2.2568 Loss-g-fm: 9.9034 Loss-g-mel: 17.1148 Loss-g-dur: 1.1377 Loss-g-kl: 1.4499 lr: 0.0002 grad_norm_g: 873.3693 grad_norm_d: 72.3102
======> Epoch: 1903
Train Epoch: 1904 [84.62%] G-Loss: 34.0415 D-Loss: 2.1475 Loss-g-fm: 10.3164 Loss-g-mel: 18.0460 Loss-g-dur: 1.2890 Loss-g-kl: 1.5287 lr: 0.0002 grad_norm_g: 449.0557 grad_norm_d: 28.3320
======> Epoch: 1904
Train Epoch: 1905 [80.77%] G-Loss: 32.8823 D-Loss: 2.1327 Loss-g-fm: 10.2248 Loss-g-mel: 17.5800 Loss-g-dur: 1.1518 Loss-g-kl: 1.1933 lr: 0.0002 grad_norm_g: 771.1942 grad_norm_d: 62.8190
======> Epoch: 1905
Train Epoch: 1906 [76.92%] G-Loss: 34.6995 D-Loss: 2.1046 Loss-g-fm: 11.9405 Loss-g-mel: 17.2784 Loss-g-dur: 1.2541 Loss-g-kl: 1.5020 lr: 0.0002 grad_norm_g: 1007.2041 grad_norm_d: 42.0991
======> Epoch: 1906
Train Epoch: 1907 [73.08%] G-Loss: 33.0004 D-Loss: 2.2566 Loss-g-fm: 10.6998 Loss-g-mel: 16.9428 Loss-g-dur: 1.1901 Loss-g-kl: 1.3941 lr: 0.0002 grad_norm_g: 570.2448 grad_norm_d: 87.1528
======> Epoch: 1907
Train Epoch: 1908 [69.23%] G-Loss: 29.4501 D-Loss: 2.2565 Loss-g-fm: 8.3378 Loss-g-mel: 15.9716 Loss-g-dur: 1.1546 Loss-g-kl: 1.5930 lr: 0.0002 grad_norm_g: 682.7039 grad_norm_d: 69.5196
======> Epoch: 1908
Train Epoch: 1909 [65.38%] G-Loss: 33.4627 D-Loss: 2.2009 Loss-g-fm: 10.6091 Loss-g-mel: 17.7301 Loss-g-dur: 1.1828 Loss-g-kl: 1.4910 lr: 0.0002 grad_norm_g: 171.1161 grad_norm_d: 11.8252
======> Epoch: 1909
Train Epoch: 1910 [61.54%] G-Loss: 34.6687 D-Loss: 2.1009 Loss-g-fm: 11.0871 Loss-g-mel: 18.2639 Loss-g-dur: 1.2229 Loss-g-kl: 1.6134 lr: 0.0002 grad_norm_g: 93.3741 grad_norm_d: 27.2254
======> Epoch: 1910
Train Epoch: 1911 [57.69%] G-Loss: 35.7830 D-Loss: 1.9982 Loss-g-fm: 11.8932 Loss-g-mel: 18.2544 Loss-g-dur: 1.3331 Loss-g-kl: 1.5241 lr: 0.0002 grad_norm_g: 817.9901 grad_norm_d: 45.5906
======> Epoch: 1911
Train Epoch: 1912 [53.85%] G-Loss: 33.6462 D-Loss: 2.2490 Loss-g-fm: 10.4663 Loss-g-mel: 17.5093 Loss-g-dur: 1.3016 Loss-g-kl: 1.8597 lr: 0.0002 grad_norm_g: 888.8119 grad_norm_d: 69.9854
======> Epoch: 1912
Train Epoch: 1913 [50.00%] G-Loss: 34.7160 D-Loss: 2.2972 Loss-g-fm: 10.8958 Loss-g-mel: 18.5563 Loss-g-dur: 1.2880 Loss-g-kl: 1.7302 lr: 0.0002 grad_norm_g: 538.2890 grad_norm_d: 19.8887
======> Epoch: 1913
Train Epoch: 1914 [46.15%] G-Loss: 33.1782 D-Loss: 2.3104 Loss-g-fm: 10.1782 Loss-g-mel: 17.5541 Loss-g-dur: 1.3322 Loss-g-kl: 1.6329 lr: 0.0002 grad_norm_g: 260.9623 grad_norm_d: 17.5436
======> Epoch: 1914
Train Epoch: 1915 [42.31%] G-Loss: 37.3475 D-Loss: 2.2019 Loss-g-fm: 12.5710 Loss-g-mel: 18.7740 Loss-g-dur: 1.3314 Loss-g-kl: 1.6696 lr: 0.0002 grad_norm_g: 1032.5336 grad_norm_d: 52.9530
======> Epoch: 1915
Train Epoch: 1916 [38.46%] G-Loss: 32.1266 D-Loss: 2.1751 Loss-g-fm: 9.7750 Loss-g-mel: 17.2975 Loss-g-dur: 1.1700 Loss-g-kl: 1.5238 lr: 0.0002 grad_norm_g: 301.0811 grad_norm_d: 51.8535
======> Epoch: 1916
Train Epoch: 1917 [34.62%] G-Loss: 33.1610 D-Loss: 2.1826 Loss-g-fm: 10.6967 Loss-g-mel: 17.0785 Loss-g-dur: 1.1729 Loss-g-kl: 1.4557 lr: 0.0002 grad_norm_g: 881.3862 grad_norm_d: 58.7809
======> Epoch: 1917
Train Epoch: 1918 [30.77%] G-Loss: 35.0743 D-Loss: 2.2794 Loss-g-fm: 10.7709 Loss-g-mel: 18.4715 Loss-g-dur: 1.3694 Loss-g-kl: 1.6025 lr: 0.0002 grad_norm_g: 958.2804 grad_norm_d: 87.5162
======> Epoch: 1918
Train Epoch: 1919 [26.92%] G-Loss: 31.1903 D-Loss: 2.1419 Loss-g-fm: 9.8408 Loss-g-mel: 16.2467 Loss-g-dur: 1.1050 Loss-g-kl: 1.4351 lr: 0.0002 grad_norm_g: 945.2520 grad_norm_d: 91.2154
======> Epoch: 1919
Train Epoch: 1920 [23.08%] G-Loss: 29.0294 D-Loss: 2.1327 Loss-g-fm: 8.9111 Loss-g-mel: 14.7910 Loss-g-dur: 1.0486 Loss-g-kl: 1.6939 lr: 0.0002 grad_norm_g: 849.4811 grad_norm_d: 80.5449
======> Epoch: 1920
Train Epoch: 1921 [19.23%] G-Loss: 35.5919 D-Loss: 2.1512 Loss-g-fm: 11.6443 Loss-g-mel: 18.2124 Loss-g-dur: 1.3258 Loss-g-kl: 1.4814 lr: 0.0002 grad_norm_g: 954.3479 grad_norm_d: 49.8446
======> Epoch: 1921
Train Epoch: 1922 [15.38%] G-Loss: 32.9727 D-Loss: 2.1166 Loss-g-fm: 9.4606 Loss-g-mel: 18.0559 Loss-g-dur: 1.2270 Loss-g-kl: 1.4030 lr: 0.0002 grad_norm_g: 486.6861 grad_norm_d: 81.0186
======> Epoch: 1922
Train Epoch: 1923 [11.54%] G-Loss: 33.2463 D-Loss: 2.1836 Loss-g-fm: 10.6312 Loss-g-mel: 17.4085 Loss-g-dur: 1.1342 Loss-g-kl: 1.5675 lr: 0.0002 grad_norm_g: 405.7131 grad_norm_d: 17.6272
======> Epoch: 1923
Train Epoch: 1924 [7.69%] G-Loss: 33.9499 D-Loss: 2.0961 Loss-g-fm: 11.0289 Loss-g-mel: 17.3025 Loss-g-dur: 1.1963 Loss-g-kl: 1.6064 lr: 0.0002 grad_norm_g: 678.2347 grad_norm_d: 21.6762
Saving model and optimizer state at iteration 1924 to /ZFS4T/tts/data/VITS/model_saved/G_50000.pth
Saving model and optimizer state at iteration 1924 to /ZFS4T/tts/data/VITS/model_saved/D_50000.pth
======> Epoch: 1924
Train Epoch: 1925 [3.85%] G-Loss: 33.0875 D-Loss: 2.0881 Loss-g-fm: 10.3954 Loss-g-mel: 17.3108 Loss-g-dur: 1.1584 Loss-g-kl: 1.6401 lr: 0.0002 grad_norm_g: 180.6143 grad_norm_d: 8.0063
======> Epoch: 1925
Train Epoch: 1926 [0.00%] G-Loss: 34.7794 D-Loss: 2.2128 Loss-g-fm: 11.3890 Loss-g-mel: 17.8087 Loss-g-dur: 1.2432 Loss-g-kl: 1.5815 lr: 0.0002 grad_norm_g: 935.7803 grad_norm_d: 56.2533
Train Epoch: 1926 [96.15%] G-Loss: 33.6836 D-Loss: 2.2923 Loss-g-fm: 10.5127 Loss-g-mel: 17.4978 Loss-g-dur: 1.2486 Loss-g-kl: 1.5153 lr: 0.0002 grad_norm_g: 968.9391 grad_norm_d: 46.4300
======> Epoch: 1926
Train Epoch: 1927 [92.31%] G-Loss: 30.7390 D-Loss: 2.3706 Loss-g-fm: 9.2263 Loss-g-mel: 16.3513 Loss-g-dur: 1.2438 Loss-g-kl: 1.4813 lr: 0.0002 grad_norm_g: 602.8712 grad_norm_d: 52.5139
======> Epoch: 1927
Train Epoch: 1928 [88.46%] G-Loss: 35.7348 D-Loss: 2.1221 Loss-g-fm: 11.8280 Loss-g-mel: 18.4119 Loss-g-dur: 1.2357 Loss-g-kl: 1.4432 lr: 0.0002 grad_norm_g: 734.6489 grad_norm_d: 48.2296
======> Epoch: 1928
Train Epoch: 1929 [84.62%] G-Loss: 31.5255 D-Loss: 2.1931 Loss-g-fm: 9.0494 Loss-g-mel: 17.3748 Loss-g-dur: 1.1900 Loss-g-kl: 1.4319 lr: 0.0002 grad_norm_g: 555.7783 grad_norm_d: 29.8093
======> Epoch: 1929
Train Epoch: 1930 [80.77%] G-Loss: 35.0639 D-Loss: 2.1730 Loss-g-fm: 11.6254 Loss-g-mel: 18.0384 Loss-g-dur: 1.3014 Loss-g-kl: 1.6783 lr: 0.0002 grad_norm_g: 540.0746 grad_norm_d: 25.2754
======> Epoch: 1930
Train Epoch: 1931 [76.92%] G-Loss: 27.5847 D-Loss: 2.2114 Loss-g-fm: 8.7210 Loss-g-mel: 13.9319 Loss-g-dur: 0.9952 Loss-g-kl: 1.4605 lr: 0.0002 grad_norm_g: 200.2513 grad_norm_d: 10.9833
======> Epoch: 1931
Train Epoch: 1932 [73.08%] G-Loss: 25.6529 D-Loss: 2.2304 Loss-g-fm: 7.5234 Loss-g-mel: 13.2066 Loss-g-dur: 0.9669 Loss-g-kl: 1.4218 lr: 0.0002 grad_norm_g: 464.8248 grad_norm_d: 46.6900
======> Epoch: 1932
Train Epoch: 1933 [69.23%] G-Loss: 33.7828 D-Loss: 2.3633 Loss-g-fm: 10.6135 Loss-g-mel: 18.1728 Loss-g-dur: 1.2621 Loss-g-kl: 1.4163 lr: 0.0002 grad_norm_g: 169.9692 grad_norm_d: 11.5924
======> Epoch: 1933
Train Epoch: 1934 [65.38%] G-Loss: 32.0047 D-Loss: 2.3001 Loss-g-fm: 9.6631 Loss-g-mel: 17.3953 Loss-g-dur: 1.1785 Loss-g-kl: 1.3473 lr: 0.0002 grad_norm_g: 283.3774 grad_norm_d: 52.1599
======> Epoch: 1934
Train Epoch: 1935 [61.54%] G-Loss: 18.0733 D-Loss: 2.6093 Loss-g-fm: 5.1765 Loss-g-mel: 8.6065 Loss-g-dur: 0.7303 Loss-g-kl: 1.3820 lr: 0.0002 grad_norm_g: 475.6927 grad_norm_d: 29.9866
======> Epoch: 1935
Train Epoch: 1936 [57.69%] G-Loss: 28.3811 D-Loss: 2.1739 Loss-g-fm: 9.2707 Loss-g-mel: 14.1681 Loss-g-dur: 0.9977 Loss-g-kl: 1.2910 lr: 0.0002 grad_norm_g: 938.4238 grad_norm_d: 61.8022
======> Epoch: 1936
Train Epoch: 1937 [53.85%] G-Loss: 32.5808 D-Loss: 2.1498 Loss-g-fm: 10.6583 Loss-g-mel: 16.6532 Loss-g-dur: 1.1823 Loss-g-kl: 1.5341 lr: 0.0002 grad_norm_g: 179.6920 grad_norm_d: 26.6418
======> Epoch: 1937
Train Epoch: 1938 [50.00%] G-Loss: 33.6041 D-Loss: 2.1148 Loss-g-fm: 11.1291 Loss-g-mel: 17.1265 Loss-g-dur: 1.1505 Loss-g-kl: 1.5595 lr: 0.0002 grad_norm_g: 871.8953 grad_norm_d: 60.9256
======> Epoch: 1938
Train Epoch: 1939 [46.15%] G-Loss: 36.7004 D-Loss: 2.1884 Loss-g-fm: 12.3168 Loss-g-mel: 18.3820 Loss-g-dur: 1.2648 Loss-g-kl: 1.6762 lr: 0.0002 grad_norm_g: 878.8802 grad_norm_d: 60.6432
======> Epoch: 1939
Train Epoch: 1940 [42.31%] G-Loss: 30.1619 D-Loss: 2.1820 Loss-g-fm: 8.6903 Loss-g-mel: 16.6190 Loss-g-dur: 1.1247 Loss-g-kl: 1.0642 lr: 0.0002 grad_norm_g: 728.7844 grad_norm_d: 56.8842
======> Epoch: 1940
Train Epoch: 1941 [38.46%] G-Loss: 32.3775 D-Loss: 2.1786 Loss-g-fm: 9.5087 Loss-g-mel: 17.7009 Loss-g-dur: 1.2253 Loss-g-kl: 1.2895 lr: 0.0002 grad_norm_g: 138.2264 grad_norm_d: 39.9548
======> Epoch: 1941
Train Epoch: 1942 [34.62%] G-Loss: 34.6940 D-Loss: 2.2221 Loss-g-fm: 11.7848 Loss-g-mel: 17.1973 Loss-g-dur: 1.3565 Loss-g-kl: 1.4828 lr: 0.0002 grad_norm_g: 374.2831 grad_norm_d: 10.6618
======> Epoch: 1942
Train Epoch: 1943 [30.77%] G-Loss: 36.0930 D-Loss: 2.0758 Loss-g-fm: 11.8938 Loss-g-mel: 18.3988 Loss-g-dur: 1.1955 Loss-g-kl: 1.6339 lr: 0.0002 grad_norm_g: 861.4911 grad_norm_d: 103.7160
======> Epoch: 1943
Train Epoch: 1944 [26.92%] G-Loss: 33.9039 D-Loss: 2.1974 Loss-g-fm: 11.1543 Loss-g-mel: 17.4554 Loss-g-dur: 1.1427 Loss-g-kl: 1.5854 lr: 0.0002 grad_norm_g: 750.1748 grad_norm_d: 49.0240
======> Epoch: 1944
Train Epoch: 1945 [23.08%] G-Loss: 19.1708 D-Loss: 2.4278 Loss-g-fm: 5.8687 Loss-g-mel: 8.7585 Loss-g-dur: 0.7429 Loss-g-kl: 1.4319 lr: 0.0002 grad_norm_g: 1142.0588 grad_norm_d: 47.6876
======> Epoch: 1945
Train Epoch: 1946 [19.23%] G-Loss: 32.6285 D-Loss: 2.0963 Loss-g-fm: 10.1168 Loss-g-mel: 17.0590 Loss-g-dur: 1.1581 Loss-g-kl: 1.5589 lr: 0.0002 grad_norm_g: 241.4665 grad_norm_d: 13.1184
======> Epoch: 1946
Train Epoch: 1947 [15.38%] G-Loss: 33.1449 D-Loss: 2.2155 Loss-g-fm: 10.0648 Loss-g-mel: 17.5070 Loss-g-dur: 1.3616 Loss-g-kl: 1.4806 lr: 0.0002 grad_norm_g: 837.4706 grad_norm_d: 47.7433
======> Epoch: 1947
Train Epoch: 1948 [11.54%] G-Loss: 32.6686 D-Loss: 2.2931 Loss-g-fm: 9.8551 Loss-g-mel: 17.8816 Loss-g-dur: 1.1316 Loss-g-kl: 1.3120 lr: 0.0002 grad_norm_g: 47.1890 grad_norm_d: 28.6472
======> Epoch: 1948
Train Epoch: 1949 [7.69%] G-Loss: 31.1350 D-Loss: 2.1778 Loss-g-fm: 9.4718 Loss-g-mel: 16.5020 Loss-g-dur: 1.1698 Loss-g-kl: 1.4391 lr: 0.0002 grad_norm_g: 708.5516 grad_norm_d: 56.0053
======> Epoch: 1949
Train Epoch: 1950 [3.85%] G-Loss: 34.8573 D-Loss: 2.1923 Loss-g-fm: 11.6431 Loss-g-mel: 17.5385 Loss-g-dur: 1.2412 Loss-g-kl: 1.6280 lr: 0.0002 grad_norm_g: 242.3147 grad_norm_d: 13.1538
======> Epoch: 1950
Train Epoch: 1951 [0.00%] G-Loss: 33.6967 D-Loss: 2.3614 Loss-g-fm: 10.8573 Loss-g-mel: 17.8458 Loss-g-dur: 1.1895 Loss-g-kl: 1.4577 lr: 0.0002 grad_norm_g: 780.9576 grad_norm_d: 50.8267
Train Epoch: 1951 [96.15%] G-Loss: 34.2385 D-Loss: 2.2270 Loss-g-fm: 10.9730 Loss-g-mel: 17.8706 Loss-g-dur: 1.2639 Loss-g-kl: 1.5020 lr: 0.0002 grad_norm_g: 991.4749 grad_norm_d: 68.2901
======> Epoch: 1951
Train Epoch: 1952 [92.31%] G-Loss: 29.8111 D-Loss: 2.2154 Loss-g-fm: 9.9531 Loss-g-mel: 14.1048 Loss-g-dur: 1.0520 Loss-g-kl: 1.5252 lr: 0.0002 grad_norm_g: 146.4445 grad_norm_d: 40.7215
======> Epoch: 1952
Train Epoch: 1953 [88.46%] G-Loss: 31.3921 D-Loss: 2.3844 Loss-g-fm: 9.4220 Loss-g-mel: 17.0976 Loss-g-dur: 1.1534 Loss-g-kl: 1.3140 lr: 0.0002 grad_norm_g: 70.8501 grad_norm_d: 46.5443
======> Epoch: 1953
Train Epoch: 1954 [84.62%] G-Loss: 32.9203 D-Loss: 2.1364 Loss-g-fm: 10.6352 Loss-g-mel: 16.7915 Loss-g-dur: 1.2276 Loss-g-kl: 1.6250 lr: 0.0002 grad_norm_g: 387.7021 grad_norm_d: 15.8002
======> Epoch: 1954
Train Epoch: 1955 [80.77%] G-Loss: 36.5592 D-Loss: 1.9933 Loss-g-fm: 12.6639 Loss-g-mel: 18.3001 Loss-g-dur: 1.2958 Loss-g-kl: 1.5905 lr: 0.0002 grad_norm_g: 639.9599 grad_norm_d: 47.5750
======> Epoch: 1955
Train Epoch: 1956 [76.92%] G-Loss: 20.8440 D-Loss: 2.2066 Loss-g-fm: 7.4475 Loss-g-mel: 8.1912 Loss-g-dur: 0.6273 Loss-g-kl: 1.4069 lr: 0.0002 grad_norm_g: 1209.1709 grad_norm_d: 40.2302
======> Epoch: 1956
Train Epoch: 1957 [73.08%] G-Loss: 34.1879 D-Loss: 2.2889 Loss-g-fm: 10.3768 Loss-g-mel: 18.1588 Loss-g-dur: 1.3488 Loss-g-kl: 1.6885 lr: 0.0002 grad_norm_g: 225.1343 grad_norm_d: 9.2489
======> Epoch: 1957
Train Epoch: 1958 [69.23%] G-Loss: 35.4386 D-Loss: 2.0896 Loss-g-fm: 11.7875 Loss-g-mel: 18.2134 Loss-g-dur: 1.3131 Loss-g-kl: 1.4272 lr: 0.0002 grad_norm_g: 147.9962 grad_norm_d: 15.4106
======> Epoch: 1958
Train Epoch: 1959 [65.38%] G-Loss: 33.6317 D-Loss: 2.1906 Loss-g-fm: 10.6523 Loss-g-mel: 17.7163 Loss-g-dur: 1.1868 Loss-g-kl: 1.4042 lr: 0.0002 grad_norm_g: 919.7327 grad_norm_d: 46.9928
======> Epoch: 1959
Train Epoch: 1960 [61.54%] G-Loss: 33.7027 D-Loss: 2.1548 Loss-g-fm: 10.7400 Loss-g-mel: 17.3551 Loss-g-dur: 1.2406 Loss-g-kl: 1.4881 lr: 0.0002 grad_norm_g: 852.4304 grad_norm_d: 75.7838
======> Epoch: 1960
Train Epoch: 1961 [57.69%] G-Loss: 34.9812 D-Loss: 2.4256 Loss-g-fm: 10.9554 Loss-g-mel: 18.1124 Loss-g-dur: 1.3003 Loss-g-kl: 1.6538 lr: 0.0002 grad_norm_g: 925.9895 grad_norm_d: 75.9395
======> Epoch: 1961
Train Epoch: 1962 [53.85%] G-Loss: 32.6692 D-Loss: 2.3982 Loss-g-fm: 10.3109 Loss-g-mel: 17.3019 Loss-g-dur: 1.1430 Loss-g-kl: 1.4099 lr: 0.0002 grad_norm_g: 607.3759 grad_norm_d: 50.3905
======> Epoch: 1962
Train Epoch: 1963 [50.00%] G-Loss: 29.7160 D-Loss: 2.3260 Loss-g-fm: 8.4124 Loss-g-mel: 16.0644 Loss-g-dur: 1.1165 Loss-g-kl: 1.2308 lr: 0.0002 grad_norm_g: 608.1618 grad_norm_d: 41.0620
======> Epoch: 1963
Train Epoch: 1964 [46.15%] G-Loss: 32.7183 D-Loss: 2.1690 Loss-g-fm: 10.1639 Loss-g-mel: 17.0642 Loss-g-dur: 1.1289 Loss-g-kl: 1.4270 lr: 0.0002 grad_norm_g: 862.6622 grad_norm_d: 50.4922
======> Epoch: 1964
Train Epoch: 1965 [42.31%] G-Loss: 26.5271 D-Loss: 2.2773 Loss-g-fm: 8.1813 Loss-g-mel: 13.5220 Loss-g-dur: 0.9936 Loss-g-kl: 1.2768 lr: 0.0002 grad_norm_g: 284.9296 grad_norm_d: 15.1789
======> Epoch: 1965
Train Epoch: 1966 [38.46%] G-Loss: 34.0250 D-Loss: 2.1037 Loss-g-fm: 11.0569 Loss-g-mel: 17.4581 Loss-g-dur: 1.2330 Loss-g-kl: 1.5250 lr: 0.0002 grad_norm_g: 327.7603 grad_norm_d: 31.1844
======> Epoch: 1966
Train Epoch: 1967 [34.62%] G-Loss: 33.1936 D-Loss: 2.2425 Loss-g-fm: 10.6257 Loss-g-mel: 17.5993 Loss-g-dur: 1.1473 Loss-g-kl: 1.3278 lr: 0.0002 grad_norm_g: 162.6077 grad_norm_d: 12.8077
======> Epoch: 1967
Train Epoch: 1968 [30.77%] G-Loss: 33.1895 D-Loss: 2.0818 Loss-g-fm: 11.1944 Loss-g-mel: 16.8459 Loss-g-dur: 1.1605 Loss-g-kl: 1.2264 lr: 0.0002 grad_norm_g: 810.4528 grad_norm_d: 32.8974
======> Epoch: 1968
Train Epoch: 1969 [26.92%] G-Loss: 29.4529 D-Loss: 2.3372 Loss-g-fm: 8.2489 Loss-g-mel: 16.3182 Loss-g-dur: 1.1371 Loss-g-kl: 1.1445 lr: 0.0002 grad_norm_g: 638.4080 grad_norm_d: 39.0364
======> Epoch: 1969
Train Epoch: 1970 [23.08%] G-Loss: 20.4045 D-Loss: 2.4640 Loss-g-fm: 6.4211 Loss-g-mel: 9.4745 Loss-g-dur: 0.7969 Loss-g-kl: 1.4408 lr: 0.0002 grad_norm_g: 951.3960 grad_norm_d: 42.2045
======> Epoch: 1970
Train Epoch: 1971 [19.23%] G-Loss: 34.4733 D-Loss: 2.0755 Loss-g-fm: 10.9133 Loss-g-mel: 17.8984 Loss-g-dur: 1.2606 Loss-g-kl: 1.5716 lr: 0.0002 grad_norm_g: 36.2405 grad_norm_d: 12.7009
======> Epoch: 1971
Train Epoch: 1972 [15.38%] G-Loss: 36.3748 D-Loss: 2.0383 Loss-g-fm: 12.8091 Loss-g-mel: 18.1278 Loss-g-dur: 1.2868 Loss-g-kl: 1.4014 lr: 0.0002 grad_norm_g: 380.2052 grad_norm_d: 27.8183
======> Epoch: 1972
Train Epoch: 1973 [11.54%] G-Loss: 34.7799 D-Loss: 2.1918 Loss-g-fm: 10.7731 Loss-g-mel: 18.3813 Loss-g-dur: 1.2126 Loss-g-kl: 1.6820 lr: 0.0002 grad_norm_g: 367.6744 grad_norm_d: 30.5826
======> Epoch: 1973
Train Epoch: 1974 [7.69%] G-Loss: 34.8432 D-Loss: 2.1563 Loss-g-fm: 11.2582 Loss-g-mel: 18.0181 Loss-g-dur: 1.2268 Loss-g-kl: 1.6122 lr: 0.0002 grad_norm_g: 970.5736 grad_norm_d: 41.1009
======> Epoch: 1974
Train Epoch: 1975 [3.85%] G-Loss: 34.4153 D-Loss: 2.2665 Loss-g-fm: 10.9955 Loss-g-mel: 17.8087 Loss-g-dur: 1.2311 Loss-g-kl: 1.7939 lr: 0.0002 grad_norm_g: 657.4159 grad_norm_d: 58.4200
======> Epoch: 1975
Train Epoch: 1976 [0.00%] G-Loss: 31.2390 D-Loss: 2.0725 Loss-g-fm: 11.5379 Loss-g-mel: 14.3877 Loss-g-dur: 0.9849 Loss-g-kl: 1.5344 lr: 0.0002 grad_norm_g: 613.0110 grad_norm_d: 32.7625
Train Epoch: 1976 [96.15%] G-Loss: 34.6897 D-Loss: 2.1375 Loss-g-fm: 11.8097 Loss-g-mel: 17.4571 Loss-g-dur: 1.1470 Loss-g-kl: 1.5070 lr: 0.0002 grad_norm_g: 968.1873 grad_norm_d: 53.3376
======> Epoch: 1976
Train Epoch: 1977 [92.31%] G-Loss: 19.6803 D-Loss: 2.3747 Loss-g-fm: 6.9920 Loss-g-mel: 8.1658 Loss-g-dur: 0.6538 Loss-g-kl: 1.3859 lr: 0.0002 grad_norm_g: 1246.4200 grad_norm_d: 70.2739
======> Epoch: 1977
Train Epoch: 1978 [88.46%] G-Loss: 31.9512 D-Loss: 2.3253 Loss-g-fm: 9.3355 Loss-g-mel: 17.4272 Loss-g-dur: 1.2591 Loss-g-kl: 1.6210 lr: 0.0002 grad_norm_g: 141.7333 grad_norm_d: 38.9888
======> Epoch: 1978
Train Epoch: 1979 [84.62%] G-Loss: 33.2623 D-Loss: 2.1390 Loss-g-fm: 10.3025 Loss-g-mel: 17.6240 Loss-g-dur: 1.2030 Loss-g-kl: 1.3683 lr: 0.0002 grad_norm_g: 722.7244 grad_norm_d: 37.3846
======> Epoch: 1979
Train Epoch: 1980 [80.77%] G-Loss: 34.0232 D-Loss: 2.0733 Loss-g-fm: 11.2878 Loss-g-mel: 17.5037 Loss-g-dur: 1.1332 Loss-g-kl: 1.3941 lr: 0.0002 grad_norm_g: 576.1443 grad_norm_d: 22.2274
======> Epoch: 1980
Train Epoch: 1981 [76.92%] G-Loss: 33.2714 D-Loss: 2.0987 Loss-g-fm: 10.8734 Loss-g-mel: 17.1535 Loss-g-dur: 1.1645 Loss-g-kl: 1.4318 lr: 0.0002 grad_norm_g: 621.3151 grad_norm_d: 59.2430
======> Epoch: 1981
Train Epoch: 1982 [73.08%] G-Loss: 34.8166 D-Loss: 2.2461 Loss-g-fm: 11.0638 Loss-g-mel: 18.0961 Loss-g-dur: 1.2452 Loss-g-kl: 1.5243 lr: 0.0002 grad_norm_g: 968.7742 grad_norm_d: 63.2740
======> Epoch: 1982
Train Epoch: 1983 [69.23%] G-Loss: 34.1163 D-Loss: 2.1951 Loss-g-fm: 11.1737 Loss-g-mel: 17.6406 Loss-g-dur: 1.1821 Loss-g-kl: 1.5273 lr: 0.0002 grad_norm_g: 236.3614 grad_norm_d: 10.6711
======> Epoch: 1983
Train Epoch: 1984 [65.38%] G-Loss: 19.0970 D-Loss: 2.5545 Loss-g-fm: 5.9170 Loss-g-mel: 8.6614 Loss-g-dur: 0.6762 Loss-g-kl: 1.4746 lr: 0.0002 grad_norm_g: 1029.2537 grad_norm_d: 37.8910
======> Epoch: 1984
Train Epoch: 1985 [61.54%] G-Loss: 19.2057 D-Loss: 2.3049 Loss-g-fm: 5.9480 Loss-g-mel: 8.5362 Loss-g-dur: 0.7594 Loss-g-kl: 1.5759 lr: 0.0002 grad_norm_g: 1394.9009 grad_norm_d: 76.2965
======> Epoch: 1985
Train Epoch: 1986 [57.69%] G-Loss: 33.5723 D-Loss: 2.0450 Loss-g-fm: 10.8739 Loss-g-mel: 17.2348 Loss-g-dur: 1.1904 Loss-g-kl: 1.5908 lr: 0.0002 grad_norm_g: 706.6444 grad_norm_d: 26.8032
======> Epoch: 1986
Train Epoch: 1987 [53.85%] G-Loss: 33.8967 D-Loss: 2.5851 Loss-g-fm: 11.1342 Loss-g-mel: 17.1510 Loss-g-dur: 1.1515 Loss-g-kl: 1.6943 lr: 0.0002 grad_norm_g: 772.5083 grad_norm_d: 67.5289
======> Epoch: 1987
Train Epoch: 1988 [50.00%] G-Loss: 35.5618 D-Loss: 2.1482 Loss-g-fm: 12.0523 Loss-g-mel: 17.7514 Loss-g-dur: 1.3085 Loss-g-kl: 1.6640 lr: 0.0002 grad_norm_g: 800.5082 grad_norm_d: 32.6217
======> Epoch: 1988
Train Epoch: 1989 [46.15%] G-Loss: 32.6852 D-Loss: 2.2358 Loss-g-fm: 10.2377 Loss-g-mel: 17.3047 Loss-g-dur: 1.1479 Loss-g-kl: 1.5239 lr: 0.0002 grad_norm_g: 899.0700 grad_norm_d: 61.4790
======> Epoch: 1989
Train Epoch: 1990 [42.31%] G-Loss: 32.5625 D-Loss: 2.0882 Loss-g-fm: 10.4573 Loss-g-mel: 16.6959 Loss-g-dur: 1.1394 Loss-g-kl: 1.6547 lr: 0.0002 grad_norm_g: 869.8435 grad_norm_d: 58.0040
======> Epoch: 1990
Train Epoch: 1991 [38.46%] G-Loss: 33.0535 D-Loss: 2.3427 Loss-g-fm: 10.3994 Loss-g-mel: 17.3381 Loss-g-dur: 1.2112 Loss-g-kl: 1.5281 lr: 0.0002 grad_norm_g: 341.4686 grad_norm_d: 44.5362
======> Epoch: 1991
Train Epoch: 1992 [34.62%] G-Loss: 19.2167 D-Loss: 2.5269 Loss-g-fm: 5.8194 Loss-g-mel: 8.4730 Loss-g-dur: 0.7054 Loss-g-kl: 1.5551 lr: 0.0002 grad_norm_g: 1067.8238 grad_norm_d: 27.9120
======> Epoch: 1992
Train Epoch: 1993 [30.77%] G-Loss: 33.0626 D-Loss: 2.2082 Loss-g-fm: 10.3530 Loss-g-mel: 17.0286 Loss-g-dur: 1.2007 Loss-g-kl: 1.5934 lr: 0.0002 grad_norm_g: 535.5853 grad_norm_d: 77.9178
======> Epoch: 1993
Train Epoch: 1994 [26.92%] G-Loss: 30.3333 D-Loss: 2.1830 Loss-g-fm: 10.8375 Loss-g-mel: 14.0862 Loss-g-dur: 1.0075 Loss-g-kl: 1.6237 lr: 0.0002 grad_norm_g: 807.2176 grad_norm_d: 82.7495
======> Epoch: 1994
Train Epoch: 1995 [23.08%] G-Loss: 32.3049 D-Loss: 2.1231 Loss-g-fm: 9.7694 Loss-g-mel: 17.0851 Loss-g-dur: 1.1728 Loss-g-kl: 1.4282 lr: 0.0002 grad_norm_g: 189.4938 grad_norm_d: 29.0553
======> Epoch: 1995
Train Epoch: 1996 [19.23%] G-Loss: 35.4000 D-Loss: 2.1009 Loss-g-fm: 11.5667 Loss-g-mel: 17.7247 Loss-g-dur: 1.2142 Loss-g-kl: 1.8215 lr: 0.0002 grad_norm_g: 227.4230 grad_norm_d: 37.0894
======> Epoch: 1996
Train Epoch: 1997 [15.38%] G-Loss: 33.3594 D-Loss: 2.2174 Loss-g-fm: 10.1892 Loss-g-mel: 17.4479 Loss-g-dur: 1.2071 Loss-g-kl: 1.5403 lr: 0.0002 grad_norm_g: 767.0394 grad_norm_d: 29.1430
======> Epoch: 1997
Train Epoch: 1998 [11.54%] G-Loss: 34.2849 D-Loss: 2.2918 Loss-g-fm: 11.0806 Loss-g-mel: 17.7375 Loss-g-dur: 1.4510 Loss-g-kl: 1.7465 lr: 0.0002 grad_norm_g: 670.4397 grad_norm_d: 41.3401
======> Epoch: 1998
Train Epoch: 1999 [7.69%] G-Loss: 31.3253 D-Loss: 2.1995 Loss-g-fm: 9.9232 Loss-g-mel: 16.5078 Loss-g-dur: 1.1170 Loss-g-kl: 1.3943 lr: 0.0002 grad_norm_g: 367.4188 grad_norm_d: 10.1696
======> Epoch: 1999
Train Epoch: 2000 [3.85%] G-Loss: 37.6447 D-Loss: 2.1180 Loss-g-fm: 12.8602 Loss-g-mel: 18.6721 Loss-g-dur: 1.3424 Loss-g-kl: 1.7133 lr: 0.0002 grad_norm_g: 1010.5351 grad_norm_d: 48.3322
======> Epoch: 2000
Train Epoch: 2001 [0.00%] G-Loss: 37.3757 D-Loss: 2.1665 Loss-g-fm: 12.8943 Loss-g-mel: 18.8451 Loss-g-dur: 1.3206 Loss-g-kl: 1.4790 lr: 0.0002 grad_norm_g: 1006.6450 grad_norm_d: 53.6780
terminate called without an active exception
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f44c239b820>
Traceback (most recent call last):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 2057071) is killed by signal: Aborted. 
Saving model and optimizer state at iteration 2001 to /ZFS4T/tts/data/VITS/model_saved/G_52000.pth
Saving model and optimizer state at iteration 2001 to /ZFS4T/tts/data/VITS/model_saved/D_52000.pth
Train Epoch: 2001 [96.15%] G-Loss: 31.7269 D-Loss: 2.2891 Loss-g-fm: 9.7745 Loss-g-mel: 16.7920 Loss-g-dur: 1.1385 Loss-g-kl: 1.4562 lr: 0.0002 grad_norm_g: 591.7644 grad_norm_d: 18.8855
======> Epoch: 2001
Train Epoch: 2002 [92.31%] G-Loss: 34.2216 D-Loss: 1.9377 Loss-g-fm: 11.1691 Loss-g-mel: 17.3783 Loss-g-dur: 1.1620 Loss-g-kl: 1.3586 lr: 0.0002 grad_norm_g: 947.7884 grad_norm_d: 40.4288
======> Epoch: 2002
Train Epoch: 2003 [88.46%] G-Loss: 33.8559 D-Loss: 2.2262 Loss-g-fm: 10.2401 Loss-g-mel: 17.8046 Loss-g-dur: 1.2000 Loss-g-kl: 1.6206 lr: 0.0002 grad_norm_g: 935.9505 grad_norm_d: 34.3902
======> Epoch: 2003
Train Epoch: 2004 [84.62%] G-Loss: 34.6867 D-Loss: 2.2275 Loss-g-fm: 11.3125 Loss-g-mel: 17.7045 Loss-g-dur: 1.3229 Loss-g-kl: 1.5997 lr: 0.0002 grad_norm_g: 773.1410 grad_norm_d: 112.7014
======> Epoch: 2004
Train Epoch: 2005 [80.77%] G-Loss: 31.6746 D-Loss: 2.1858 Loss-g-fm: 10.2184 Loss-g-mel: 16.3029 Loss-g-dur: 1.2250 Loss-g-kl: 1.3934 lr: 0.0002 grad_norm_g: 644.0039 grad_norm_d: 50.7019
======> Epoch: 2005
Train Epoch: 2006 [76.92%] G-Loss: 32.7277 D-Loss: 2.1694 Loss-g-fm: 10.7001 Loss-g-mel: 16.7856 Loss-g-dur: 1.2488 Loss-g-kl: 1.2338 lr: 0.0002 grad_norm_g: 738.1810 grad_norm_d: 33.3789
======> Epoch: 2006
Train Epoch: 2007 [73.08%] G-Loss: 30.6563 D-Loss: 2.2353 Loss-g-fm: 8.9994 Loss-g-mel: 16.7272 Loss-g-dur: 1.1682 Loss-g-kl: 1.1163 lr: 0.0002 grad_norm_g: 674.1378 grad_norm_d: 52.1159
======> Epoch: 2007
Train Epoch: 2008 [69.23%] G-Loss: 35.6709 D-Loss: 2.2707 Loss-g-fm: 11.8492 Loss-g-mel: 18.0271 Loss-g-dur: 1.2085 Loss-g-kl: 1.6851 lr: 0.0002 grad_norm_g: 204.2143 grad_norm_d: 14.6642
======> Epoch: 2008
Train Epoch: 2009 [65.38%] G-Loss: 34.6988 D-Loss: 2.0728 Loss-g-fm: 11.4406 Loss-g-mel: 17.7314 Loss-g-dur: 1.1136 Loss-g-kl: 1.4606 lr: 0.0002 grad_norm_g: 997.1600 grad_norm_d: 70.1710
======> Epoch: 2009
Train Epoch: 2010 [61.54%] G-Loss: 33.4365 D-Loss: 2.2137 Loss-g-fm: 10.7839 Loss-g-mel: 17.0188 Loss-g-dur: 1.1875 Loss-g-kl: 1.6831 lr: 0.0002 grad_norm_g: 848.2210 grad_norm_d: 67.8847
======> Epoch: 2010
Train Epoch: 2011 [57.69%] G-Loss: 33.5878 D-Loss: 2.2577 Loss-g-fm: 10.9289 Loss-g-mel: 17.4927 Loss-g-dur: 1.1463 Loss-g-kl: 1.4271 lr: 0.0002 grad_norm_g: 893.5731 grad_norm_d: 55.9692
======> Epoch: 2011
Train Epoch: 2012 [53.85%] G-Loss: 33.8117 D-Loss: 2.1937 Loss-g-fm: 10.4107 Loss-g-mel: 17.6882 Loss-g-dur: 1.3353 Loss-g-kl: 1.5728 lr: 0.0002 grad_norm_g: 997.0170 grad_norm_d: 40.3033
======> Epoch: 2012
Train Epoch: 2013 [50.00%] G-Loss: 34.8539 D-Loss: 2.1046 Loss-g-fm: 11.3995 Loss-g-mel: 17.7885 Loss-g-dur: 1.2875 Loss-g-kl: 1.6136 lr: 0.0002 grad_norm_g: 832.5874 grad_norm_d: 45.2997
======> Epoch: 2013
Train Epoch: 2014 [46.15%] G-Loss: 32.6969 D-Loss: 2.1053 Loss-g-fm: 10.5923 Loss-g-mel: 17.3340 Loss-g-dur: 1.1030 Loss-g-kl: 1.2429 lr: 0.0002 grad_norm_g: 212.0738 grad_norm_d: 30.2079
======> Epoch: 2014
Train Epoch: 2015 [42.31%] G-Loss: 30.9302 D-Loss: 2.2446 Loss-g-fm: 9.3854 Loss-g-mel: 16.5262 Loss-g-dur: 1.1957 Loss-g-kl: 1.3538 lr: 0.0002 grad_norm_g: 157.0464 grad_norm_d: 34.2441
======> Epoch: 2015
Train Epoch: 2016 [38.46%] G-Loss: 34.3569 D-Loss: 2.1593 Loss-g-fm: 11.1566 Loss-g-mel: 17.8823 Loss-g-dur: 1.1824 Loss-g-kl: 1.2866 lr: 0.0002 grad_norm_g: 983.4411 grad_norm_d: 77.7607
======> Epoch: 2016
Train Epoch: 2017 [34.62%] G-Loss: 34.0890 D-Loss: 2.1221 Loss-g-fm: 11.1733 Loss-g-mel: 17.6440 Loss-g-dur: 1.2336 Loss-g-kl: 1.4848 lr: 0.0002 grad_norm_g: 392.1695 grad_norm_d: 32.9208
======> Epoch: 2017
Train Epoch: 2018 [30.77%] G-Loss: 34.0180 D-Loss: 2.0431 Loss-g-fm: 11.3970 Loss-g-mel: 17.2220 Loss-g-dur: 1.1888 Loss-g-kl: 1.3279 lr: 0.0002 grad_norm_g: 709.4679 grad_norm_d: 26.7162
======> Epoch: 2018
Train Epoch: 2019 [26.92%] G-Loss: 36.2226 D-Loss: 2.0910 Loss-g-fm: 12.2784 Loss-g-mel: 18.2230 Loss-g-dur: 1.2318 Loss-g-kl: 1.7399 lr: 0.0002 grad_norm_g: 45.7347 grad_norm_d: 8.8550
======> Epoch: 2019
Train Epoch: 2020 [23.08%] G-Loss: 34.7527 D-Loss: 2.2571 Loss-g-fm: 11.1784 Loss-g-mel: 18.0134 Loss-g-dur: 1.2232 Loss-g-kl: 1.6039 lr: 0.0002 grad_norm_g: 946.3223 grad_norm_d: 52.4947
======> Epoch: 2020
Train Epoch: 2021 [19.23%] G-Loss: 34.8735 D-Loss: 2.2211 Loss-g-fm: 11.5575 Loss-g-mel: 17.7582 Loss-g-dur: 1.2134 Loss-g-kl: 1.5471 lr: 0.0002 grad_norm_g: 298.7449 grad_norm_d: 7.0485
======> Epoch: 2021
Train Epoch: 2022 [15.38%] G-Loss: 32.8503 D-Loss: 2.2080 Loss-g-fm: 10.4260 Loss-g-mel: 17.1910 Loss-g-dur: 1.1816 Loss-g-kl: 1.5750 lr: 0.0002 grad_norm_g: 951.9918 grad_norm_d: 87.5784
======> Epoch: 2022
Train Epoch: 2023 [11.54%] G-Loss: 33.0021 D-Loss: 2.1963 Loss-g-fm: 10.2873 Loss-g-mel: 17.1954 Loss-g-dur: 1.1534 Loss-g-kl: 1.5418 lr: 0.0002 grad_norm_g: 930.7321 grad_norm_d: 63.1188
======> Epoch: 2023
Train Epoch: 2024 [7.69%] G-Loss: 37.0449 D-Loss: 1.9869 Loss-g-fm: 12.6502 Loss-g-mel: 18.4607 Loss-g-dur: 1.3005 Loss-g-kl: 1.7376 lr: 0.0002 grad_norm_g: 262.3852 grad_norm_d: 22.6861
======> Epoch: 2024
Train Epoch: 2025 [3.85%] G-Loss: 19.0602 D-Loss: 2.5024 Loss-g-fm: 6.1274 Loss-g-mel: 7.7428 Loss-g-dur: 0.6456 Loss-g-kl: 1.4005 lr: 0.0002 grad_norm_g: 1830.8413 grad_norm_d: 70.6603
======> Epoch: 2025
Train Epoch: 2026 [0.00%] G-Loss: 34.2890 D-Loss: 2.0300 Loss-g-fm: 10.6415 Loss-g-mel: 17.6088 Loss-g-dur: 1.2669 Loss-g-kl: 1.5273 lr: 0.0002 grad_norm_g: 941.7571 grad_norm_d: 82.6619
Train Epoch: 2026 [96.15%] G-Loss: 31.9976 D-Loss: 2.2451 Loss-g-fm: 9.5936 Loss-g-mel: 17.1682 Loss-g-dur: 1.1739 Loss-g-kl: 1.6054 lr: 0.0002 grad_norm_g: 764.3237 grad_norm_d: 53.3415
======> Epoch: 2026
Train Epoch: 2027 [92.31%] G-Loss: 31.3918 D-Loss: 2.3017 Loss-g-fm: 9.2664 Loss-g-mel: 17.0821 Loss-g-dur: 1.1710 Loss-g-kl: 1.3610 lr: 0.0002 grad_norm_g: 405.9672 grad_norm_d: 10.8608
======> Epoch: 2027
Train Epoch: 2028 [88.46%] G-Loss: 36.6884 D-Loss: 2.2846 Loss-g-fm: 12.3364 Loss-g-mel: 18.0668 Loss-g-dur: 1.2388 Loss-g-kl: 1.6454 lr: 0.0002 grad_norm_g: 1125.4417 grad_norm_d: 29.5265
======> Epoch: 2028
Train Epoch: 2029 [84.62%] G-Loss: 34.5265 D-Loss: 2.3929 Loss-g-fm: 11.0764 Loss-g-mel: 17.7658 Loss-g-dur: 1.2020 Loss-g-kl: 1.5955 lr: 0.0002 grad_norm_g: 780.6406 grad_norm_d: 111.3539
======> Epoch: 2029
Train Epoch: 2030 [80.77%] G-Loss: 31.7087 D-Loss: 2.3452 Loss-g-fm: 9.8161 Loss-g-mel: 16.7782 Loss-g-dur: 1.1721 Loss-g-kl: 1.4433 lr: 0.0002 grad_norm_g: 521.8446 grad_norm_d: 25.9832
======> Epoch: 2030
Train Epoch: 2031 [76.92%] G-Loss: 35.7077 D-Loss: 2.0751 Loss-g-fm: 11.8031 Loss-g-mel: 18.0679 Loss-g-dur: 1.2520 Loss-g-kl: 1.5153 lr: 0.0002 grad_norm_g: 1026.5334 grad_norm_d: 65.1061
======> Epoch: 2031
Train Epoch: 2032 [73.08%] G-Loss: 34.5608 D-Loss: 2.2845 Loss-g-fm: 10.9555 Loss-g-mel: 17.5743 Loss-g-dur: 1.2724 Loss-g-kl: 1.5724 lr: 0.0002 grad_norm_g: 826.7885 grad_norm_d: 59.3817
======> Epoch: 2032
Train Epoch: 2033 [69.23%] G-Loss: 33.5157 D-Loss: 2.2218 Loss-g-fm: 10.8116 Loss-g-mel: 17.2250 Loss-g-dur: 1.1756 Loss-g-kl: 1.4087 lr: 0.0002 grad_norm_g: 418.3718 grad_norm_d: 62.2338
======> Epoch: 2033
Train Epoch: 2034 [65.38%] G-Loss: 32.3269 D-Loss: 2.2223 Loss-g-fm: 9.8034 Loss-g-mel: 17.1576 Loss-g-dur: 1.1730 Loss-g-kl: 1.4366 lr: 0.0002 grad_norm_g: 431.8716 grad_norm_d: 12.5275
======> Epoch: 2034
Train Epoch: 2035 [61.54%] G-Loss: 36.0628 D-Loss: 2.1522 Loss-g-fm: 11.8569 Loss-g-mel: 18.2145 Loss-g-dur: 1.2528 Loss-g-kl: 1.6856 lr: 0.0002 grad_norm_g: 473.7867 grad_norm_d: 22.1145
======> Epoch: 2035
Train Epoch: 2036 [57.69%] G-Loss: 33.9694 D-Loss: 2.2647 Loss-g-fm: 10.7248 Loss-g-mel: 17.7613 Loss-g-dur: 1.2243 Loss-g-kl: 1.5782 lr: 0.0002 grad_norm_g: 239.8106 grad_norm_d: 16.7357
======> Epoch: 2036
Train Epoch: 2037 [53.85%] G-Loss: 30.5043 D-Loss: 2.0615 Loss-g-fm: 11.0238 Loss-g-mel: 14.1422 Loss-g-dur: 0.9816 Loss-g-kl: 1.5666 lr: 0.0002 grad_norm_g: 876.3441 grad_norm_d: 45.9343
======> Epoch: 2037
Train Epoch: 2038 [50.00%] G-Loss: 35.2821 D-Loss: 2.0968 Loss-g-fm: 11.6184 Loss-g-mel: 17.6003 Loss-g-dur: 1.2315 Loss-g-kl: 1.6561 lr: 0.0002 grad_norm_g: 956.5152 grad_norm_d: 59.5384
======> Epoch: 2038
Train Epoch: 2039 [46.15%] G-Loss: 35.6168 D-Loss: 2.2163 Loss-g-fm: 12.1447 Loss-g-mel: 17.2439 Loss-g-dur: 1.2098 Loss-g-kl: 1.4504 lr: 0.0002 grad_norm_g: 852.5912 grad_norm_d: 115.9405
======> Epoch: 2039
Train Epoch: 2040 [42.31%] G-Loss: 32.9815 D-Loss: 2.2928 Loss-g-fm: 9.7984 Loss-g-mel: 17.6348 Loss-g-dur: 1.1295 Loss-g-kl: 1.3171 lr: 0.0002 grad_norm_g: 300.0616 grad_norm_d: 53.7238
======> Epoch: 2040
Train Epoch: 2041 [38.46%] G-Loss: 19.4075 D-Loss: 2.4180 Loss-g-fm: 6.1450 Loss-g-mel: 8.7531 Loss-g-dur: 0.7211 Loss-g-kl: 1.3972 lr: 0.0002 grad_norm_g: 399.7159 grad_norm_d: 34.6482
======> Epoch: 2041
Train Epoch: 2042 [34.62%] G-Loss: 33.4190 D-Loss: 2.0450 Loss-g-fm: 11.2761 Loss-g-mel: 17.2460 Loss-g-dur: 1.1221 Loss-g-kl: 1.2519 lr: 0.0002 grad_norm_g: 737.4281 grad_norm_d: 54.9540
======> Epoch: 2042
Train Epoch: 2043 [30.77%] G-Loss: 35.6142 D-Loss: 2.0810 Loss-g-fm: 12.2805 Loss-g-mel: 17.9456 Loss-g-dur: 1.2250 Loss-g-kl: 1.3487 lr: 0.0002 grad_norm_g: 671.6346 grad_norm_d: 12.7155
======> Epoch: 2043
Train Epoch: 2044 [26.92%] G-Loss: 32.9098 D-Loss: 2.1538 Loss-g-fm: 10.5994 Loss-g-mel: 16.8409 Loss-g-dur: 1.1440 Loss-g-kl: 1.5075 lr: 0.0002 grad_norm_g: 922.5405 grad_norm_d: 63.9840
======> Epoch: 2044
Train Epoch: 2045 [23.08%] G-Loss: 32.9363 D-Loss: 2.2448 Loss-g-fm: 10.6983 Loss-g-mel: 16.8876 Loss-g-dur: 1.1143 Loss-g-kl: 1.4062 lr: 0.0002 grad_norm_g: 907.1069 grad_norm_d: 34.9608
======> Epoch: 2045
Train Epoch: 2046 [19.23%] G-Loss: 31.8401 D-Loss: 2.3173 Loss-g-fm: 10.0772 Loss-g-mel: 16.8223 Loss-g-dur: 1.1419 Loss-g-kl: 1.2409 lr: 0.0002 grad_norm_g: 840.3219 grad_norm_d: 79.2777
======> Epoch: 2046
Train Epoch: 2047 [15.38%] G-Loss: 33.6254 D-Loss: 2.2834 Loss-g-fm: 10.4691 Loss-g-mel: 18.0044 Loss-g-dur: 1.1962 Loss-g-kl: 1.5143 lr: 0.0002 grad_norm_g: 45.9049 grad_norm_d: 7.8712
======> Epoch: 2047
Train Epoch: 2048 [11.54%] G-Loss: 35.5215 D-Loss: 2.0862 Loss-g-fm: 11.7851 Loss-g-mel: 17.8151 Loss-g-dur: 1.3526 Loss-g-kl: 1.5538 lr: 0.0002 grad_norm_g: 528.6196 grad_norm_d: 23.4180
======> Epoch: 2048
Train Epoch: 2049 [7.69%] G-Loss: 33.3531 D-Loss: 2.1000 Loss-g-fm: 10.6910 Loss-g-mel: 17.3609 Loss-g-dur: 1.1463 Loss-g-kl: 1.5476 lr: 0.0002 grad_norm_g: 544.4953 grad_norm_d: 41.0257
======> Epoch: 2049
Train Epoch: 2050 [3.85%] G-Loss: 33.8048 D-Loss: 2.1943 Loss-g-fm: 10.4469 Loss-g-mel: 17.6316 Loss-g-dur: 1.2495 Loss-g-kl: 1.5925 lr: 0.0002 grad_norm_g: 714.9668 grad_norm_d: 32.3177
======> Epoch: 2050
Train Epoch: 2051 [0.00%] G-Loss: 36.2606 D-Loss: 2.2019 Loss-g-fm: 11.9021 Loss-g-mel: 18.1920 Loss-g-dur: 1.2177 Loss-g-kl: 1.6014 lr: 0.0002 grad_norm_g: 877.3351 grad_norm_d: 37.8189
Train Epoch: 2051 [96.15%] G-Loss: 33.0855 D-Loss: 2.1901 Loss-g-fm: 10.6544 Loss-g-mel: 17.0536 Loss-g-dur: 1.1327 Loss-g-kl: 1.5486 lr: 0.0002 grad_norm_g: 941.3266 grad_norm_d: 78.6928
======> Epoch: 2051
Train Epoch: 2052 [92.31%] G-Loss: 34.2552 D-Loss: 2.1510 Loss-g-fm: 11.1244 Loss-g-mel: 17.6674 Loss-g-dur: 1.2096 Loss-g-kl: 1.6389 lr: 0.0002 grad_norm_g: 698.1958 grad_norm_d: 42.4354
======> Epoch: 2052
Train Epoch: 2053 [88.46%] G-Loss: 34.7097 D-Loss: 2.0810 Loss-g-fm: 11.6972 Loss-g-mel: 17.4059 Loss-g-dur: 1.1343 Loss-g-kl: 1.7035 lr: 0.0002 grad_norm_g: 735.2974 grad_norm_d: 15.3012
======> Epoch: 2053
Train Epoch: 2054 [84.62%] G-Loss: 33.6839 D-Loss: 2.1504 Loss-g-fm: 11.2178 Loss-g-mel: 17.2181 Loss-g-dur: 1.1294 Loss-g-kl: 1.5565 lr: 0.0002 grad_norm_g: 483.4085 grad_norm_d: 36.4868
======> Epoch: 2054
Train Epoch: 2055 [80.77%] G-Loss: 35.8368 D-Loss: 2.1461 Loss-g-fm: 12.5077 Loss-g-mel: 17.6462 Loss-g-dur: 1.2454 Loss-g-kl: 1.6602 lr: 0.0002 grad_norm_g: 795.8627 grad_norm_d: 58.0951
======> Epoch: 2055
Train Epoch: 2056 [76.92%] G-Loss: 31.8356 D-Loss: 2.5139 Loss-g-fm: 9.8140 Loss-g-mel: 16.6159 Loss-g-dur: 1.1676 Loss-g-kl: 1.3664 lr: 0.0002 grad_norm_g: 916.6832 grad_norm_d: 157.3766
======> Epoch: 2056
Train Epoch: 2057 [73.08%] G-Loss: 33.5490 D-Loss: 2.1869 Loss-g-fm: 11.0021 Loss-g-mel: 17.1911 Loss-g-dur: 1.1683 Loss-g-kl: 1.3748 lr: 0.0002 grad_norm_g: 228.7538 grad_norm_d: 32.7341
======> Epoch: 2057
Train Epoch: 2058 [69.23%] G-Loss: 19.3554 D-Loss: 2.3137 Loss-g-fm: 6.3817 Loss-g-mel: 8.4149 Loss-g-dur: 0.6702 Loss-g-kl: 1.3830 lr: 0.0002 grad_norm_g: 169.1633 grad_norm_d: 11.1443
======> Epoch: 2058
Train Epoch: 2059 [65.38%] G-Loss: 34.1251 D-Loss: 2.2619 Loss-g-fm: 11.6618 Loss-g-mel: 17.3902 Loss-g-dur: 1.1712 Loss-g-kl: 1.4473 lr: 0.0002 grad_norm_g: 478.2774 grad_norm_d: 39.8292
======> Epoch: 2059
Train Epoch: 2060 [61.54%] G-Loss: 36.2873 D-Loss: 2.2283 Loss-g-fm: 12.5789 Loss-g-mel: 17.7348 Loss-g-dur: 1.2096 Loss-g-kl: 1.6091 lr: 0.0002 grad_norm_g: 903.3371 grad_norm_d: 49.4989
======> Epoch: 2060
Train Epoch: 2061 [57.69%] G-Loss: 36.9101 D-Loss: 2.0867 Loss-g-fm: 13.0423 Loss-g-mel: 18.0582 Loss-g-dur: 1.2258 Loss-g-kl: 1.7632 lr: 0.0002 grad_norm_g: 914.5633 grad_norm_d: 54.9307
======> Epoch: 2061
Train Epoch: 2062 [53.85%] G-Loss: 31.8975 D-Loss: 2.2180 Loss-g-fm: 10.1510 Loss-g-mel: 16.8385 Loss-g-dur: 1.1606 Loss-g-kl: 1.3550 lr: 0.0002 grad_norm_g: 396.3706 grad_norm_d: 18.5021
======> Epoch: 2062
Train Epoch: 2063 [50.00%] G-Loss: 31.5336 D-Loss: 2.2482 Loss-g-fm: 9.8981 Loss-g-mel: 16.3875 Loss-g-dur: 1.1647 Loss-g-kl: 1.6833 lr: 0.0002 grad_norm_g: 451.1176 grad_norm_d: 29.6797
======> Epoch: 2063
Train Epoch: 2064 [46.15%] G-Loss: 28.2836 D-Loss: 2.0721 Loss-g-fm: 9.3686 Loss-g-mel: 13.7580 Loss-g-dur: 0.9504 Loss-g-kl: 1.4868 lr: 0.0002 grad_norm_g: 138.9378 grad_norm_d: 8.8019
======> Epoch: 2064
Train Epoch: 2065 [42.31%] G-Loss: 18.2916 D-Loss: 2.6231 Loss-g-fm: 5.8501 Loss-g-mel: 8.1868 Loss-g-dur: 0.6862 Loss-g-kl: 1.6080 lr: 0.0002 grad_norm_g: 1019.6449 grad_norm_d: 32.1087
======> Epoch: 2065
Train Epoch: 2066 [38.46%] G-Loss: 35.7043 D-Loss: 2.1142 Loss-g-fm: 12.1770 Loss-g-mel: 18.0797 Loss-g-dur: 1.2198 Loss-g-kl: 1.4979 lr: 0.0002 grad_norm_g: 625.5998 grad_norm_d: 31.8246
======> Epoch: 2066
Train Epoch: 2067 [34.62%] G-Loss: 31.7128 D-Loss: 2.1252 Loss-g-fm: 9.8530 Loss-g-mel: 16.6975 Loss-g-dur: 1.2300 Loss-g-kl: 1.2789 lr: 0.0002 grad_norm_g: 330.0021 grad_norm_d: 20.7747
======> Epoch: 2067
Train Epoch: 2068 [30.77%] G-Loss: 18.9410 D-Loss: 2.3448 Loss-g-fm: 6.1762 Loss-g-mel: 7.9201 Loss-g-dur: 0.6176 Loss-g-kl: 1.4444 lr: 0.0002 grad_norm_g: 1233.9228 grad_norm_d: 62.6130
======> Epoch: 2068
Train Epoch: 2069 [26.92%] G-Loss: 31.9052 D-Loss: 2.2934 Loss-g-fm: 9.8385 Loss-g-mel: 16.8267 Loss-g-dur: 1.2436 Loss-g-kl: 1.4900 lr: 0.0002 grad_norm_g: 548.7489 grad_norm_d: 41.4234
======> Epoch: 2069
Train Epoch: 2070 [23.08%] G-Loss: 33.5732 D-Loss: 2.0450 Loss-g-fm: 10.7981 Loss-g-mel: 17.5472 Loss-g-dur: 1.1318 Loss-g-kl: 1.4469 lr: 0.0002 grad_norm_g: 57.9822 grad_norm_d: 18.8527
======> Epoch: 2070
Train Epoch: 2071 [19.23%] G-Loss: 34.5074 D-Loss: 2.1214 Loss-g-fm: 11.3725 Loss-g-mel: 17.5325 Loss-g-dur: 1.1413 Loss-g-kl: 1.5359 lr: 0.0002 grad_norm_g: 890.0544 grad_norm_d: 75.6708
======> Epoch: 2071
Train Epoch: 2072 [15.38%] G-Loss: 29.2097 D-Loss: 2.0194 Loss-g-fm: 10.1531 Loss-g-mel: 13.7474 Loss-g-dur: 0.9788 Loss-g-kl: 1.4769 lr: 0.0002 grad_norm_g: 895.7012 grad_norm_d: 58.9908
======> Epoch: 2072
Train Epoch: 2073 [11.54%] G-Loss: 32.4475 D-Loss: 2.1449 Loss-g-fm: 10.2844 Loss-g-mel: 16.9153 Loss-g-dur: 1.1411 Loss-g-kl: 1.5091 lr: 0.0002 grad_norm_g: 428.6636 grad_norm_d: 39.3003
======> Epoch: 2073
Train Epoch: 2074 [7.69%] G-Loss: 34.9158 D-Loss: 2.0504 Loss-g-fm: 11.3733 Loss-g-mel: 17.9069 Loss-g-dur: 1.3108 Loss-g-kl: 1.6368 lr: 0.0002 grad_norm_g: 408.6011 grad_norm_d: 22.9410
======> Epoch: 2074
Train Epoch: 2075 [3.85%] G-Loss: 34.3593 D-Loss: 2.1483 Loss-g-fm: 11.6500 Loss-g-mel: 17.2564 Loss-g-dur: 1.1362 Loss-g-kl: 1.6459 lr: 0.0002 grad_norm_g: 852.7206 grad_norm_d: 73.8453
======> Epoch: 2075
Train Epoch: 2076 [0.00%] G-Loss: 34.8514 D-Loss: 2.2275 Loss-g-fm: 10.7817 Loss-g-mel: 18.1335 Loss-g-dur: 1.2907 Loss-g-kl: 1.6281 lr: 0.0002 grad_norm_g: 102.3107 grad_norm_d: 20.3222
Train Epoch: 2076 [96.15%] G-Loss: 20.5627 D-Loss: 2.2752 Loss-g-fm: 7.1187 Loss-g-mel: 8.4844 Loss-g-dur: 0.7178 Loss-g-kl: 1.4557 lr: 0.0002 grad_norm_g: 1256.4037 grad_norm_d: 26.2058
======> Epoch: 2076
Train Epoch: 2077 [92.31%] G-Loss: 33.0050 D-Loss: 2.1136 Loss-g-fm: 10.4488 Loss-g-mel: 17.2297 Loss-g-dur: 1.1762 Loss-g-kl: 1.2902 lr: 0.0002 grad_norm_g: 833.9845 grad_norm_d: 59.6832
Saving model and optimizer state at iteration 2077 to /ZFS4T/tts/data/VITS/model_saved/G_54000.pth
Saving model and optimizer state at iteration 2077 to /ZFS4T/tts/data/VITS/model_saved/D_54000.pth
======> Epoch: 2077
Train Epoch: 2078 [88.46%] G-Loss: 18.0602 D-Loss: 2.3840 Loss-g-fm: 5.5450 Loss-g-mel: 7.9660 Loss-g-dur: 0.6346 Loss-g-kl: 1.4389 lr: 0.0002 grad_norm_g: 753.8489 grad_norm_d: 93.3403
======> Epoch: 2078
Train Epoch: 2079 [84.62%] G-Loss: 32.6208 D-Loss: 2.2377 Loss-g-fm: 9.8731 Loss-g-mel: 17.2990 Loss-g-dur: 1.1424 Loss-g-kl: 1.3608 lr: 0.0002 grad_norm_g: 53.4142 grad_norm_d: 62.9602
======> Epoch: 2079
Train Epoch: 2080 [80.77%] G-Loss: 35.2036 D-Loss: 2.0697 Loss-g-fm: 12.1391 Loss-g-mel: 17.5950 Loss-g-dur: 1.1521 Loss-g-kl: 1.6642 lr: 0.0002 grad_norm_g: 531.9735 grad_norm_d: 25.3919
======> Epoch: 2080
Train Epoch: 2081 [76.92%] G-Loss: 35.8195 D-Loss: 2.0653 Loss-g-fm: 12.2765 Loss-g-mel: 17.9618 Loss-g-dur: 1.2783 Loss-g-kl: 1.5302 lr: 0.0002 grad_norm_g: 963.5902 grad_norm_d: 64.2268
======> Epoch: 2081
Train Epoch: 2082 [73.08%] G-Loss: 31.5816 D-Loss: 2.1808 Loss-g-fm: 9.7818 Loss-g-mel: 16.8143 Loss-g-dur: 1.1424 Loss-g-kl: 1.2564 lr: 0.0002 grad_norm_g: 429.4361 grad_norm_d: 24.9546
======> Epoch: 2082
Train Epoch: 2083 [69.23%] G-Loss: 32.5478 D-Loss: 2.1096 Loss-g-fm: 10.3046 Loss-g-mel: 16.9445 Loss-g-dur: 1.1582 Loss-g-kl: 1.4545 lr: 0.0002 grad_norm_g: 763.7986 grad_norm_d: 45.1156
======> Epoch: 2083
Train Epoch: 2084 [65.38%] G-Loss: 35.8299 D-Loss: 2.1171 Loss-g-fm: 11.8786 Loss-g-mel: 18.1820 Loss-g-dur: 1.3232 Loss-g-kl: 1.6134 lr: 0.0002 grad_norm_g: 189.8339 grad_norm_d: 28.8763
======> Epoch: 2084
Train Epoch: 2085 [61.54%] G-Loss: 31.7483 D-Loss: 2.2049 Loss-g-fm: 10.1740 Loss-g-mel: 16.4806 Loss-g-dur: 1.1460 Loss-g-kl: 1.5500 lr: 0.0002 grad_norm_g: 892.9641 grad_norm_d: 73.3344
======> Epoch: 2085
Train Epoch: 2086 [57.69%] G-Loss: 30.0810 D-Loss: 2.2094 Loss-g-fm: 8.7600 Loss-g-mel: 16.2261 Loss-g-dur: 1.1320 Loss-g-kl: 1.4694 lr: 0.0002 grad_norm_g: 330.5816 grad_norm_d: 94.1137
======> Epoch: 2086
Train Epoch: 2087 [53.85%] G-Loss: 17.9715 D-Loss: 2.3310 Loss-g-fm: 5.7860 Loss-g-mel: 7.9371 Loss-g-dur: 0.6445 Loss-g-kl: 1.3049 lr: 0.0002 grad_norm_g: 1122.8374 grad_norm_d: 57.4124
======> Epoch: 2087
Train Epoch: 2088 [50.00%] G-Loss: 33.3526 D-Loss: 2.1643 Loss-g-fm: 11.0031 Loss-g-mel: 17.1454 Loss-g-dur: 1.1391 Loss-g-kl: 1.4636 lr: 0.0002 grad_norm_g: 543.9956 grad_norm_d: 5.7334
======> Epoch: 2088
Train Epoch: 2089 [46.15%] G-Loss: 30.9357 D-Loss: 2.1570 Loss-g-fm: 9.1801 Loss-g-mel: 16.6141 Loss-g-dur: 1.1585 Loss-g-kl: 1.3537 lr: 0.0002 grad_norm_g: 167.3061 grad_norm_d: 9.7379
======> Epoch: 2089
Train Epoch: 2090 [42.31%] G-Loss: 33.2637 D-Loss: 2.1311 Loss-g-fm: 10.8762 Loss-g-mel: 17.0956 Loss-g-dur: 1.1598 Loss-g-kl: 1.3414 lr: 0.0002 grad_norm_g: 753.9215 grad_norm_d: 51.2046
======> Epoch: 2090
Train Epoch: 2091 [38.46%] G-Loss: 32.5811 D-Loss: 2.1317 Loss-g-fm: 10.3927 Loss-g-mel: 16.9536 Loss-g-dur: 1.2070 Loss-g-kl: 1.5304 lr: 0.0002 grad_norm_g: 971.0186 grad_norm_d: 48.9523
======> Epoch: 2091
Train Epoch: 2092 [34.62%] G-Loss: 34.1668 D-Loss: 2.1299 Loss-g-fm: 10.9529 Loss-g-mel: 17.7648 Loss-g-dur: 1.1726 Loss-g-kl: 1.5655 lr: 0.0002 grad_norm_g: 811.4483 grad_norm_d: 57.7616
======> Epoch: 2092
Train Epoch: 2093 [30.77%] G-Loss: 35.0334 D-Loss: 2.0984 Loss-g-fm: 12.5548 Loss-g-mel: 17.3847 Loss-g-dur: 1.1712 Loss-g-kl: 1.4426 lr: 0.0002 grad_norm_g: 848.8811 grad_norm_d: 65.4439
======> Epoch: 2093
Train Epoch: 2094 [26.92%] G-Loss: 34.3898 D-Loss: 2.2288 Loss-g-fm: 10.9265 Loss-g-mel: 17.9038 Loss-g-dur: 1.2512 Loss-g-kl: 1.6450 lr: 0.0002 grad_norm_g: 273.8012 grad_norm_d: 20.1819
======> Epoch: 2094
Train Epoch: 2095 [23.08%] G-Loss: 34.7250 D-Loss: 2.2688 Loss-g-fm: 11.4171 Loss-g-mel: 17.6544 Loss-g-dur: 1.1382 Loss-g-kl: 1.6498 lr: 0.0002 grad_norm_g: 848.1775 grad_norm_d: 58.8796
======> Epoch: 2095
Train Epoch: 2096 [19.23%] G-Loss: 33.5476 D-Loss: 2.1683 Loss-g-fm: 11.3335 Loss-g-mel: 16.9668 Loss-g-dur: 1.1436 Loss-g-kl: 1.3276 lr: 0.0002 grad_norm_g: 814.8183 grad_norm_d: 48.6920
======> Epoch: 2096
Train Epoch: 2097 [15.38%] G-Loss: 33.6036 D-Loss: 2.1187 Loss-g-fm: 11.0975 Loss-g-mel: 17.2866 Loss-g-dur: 1.1360 Loss-g-kl: 1.1559 lr: 0.0002 grad_norm_g: 758.0912 grad_norm_d: 89.6337
======> Epoch: 2097
Train Epoch: 2098 [11.54%] G-Loss: 33.7286 D-Loss: 2.2344 Loss-g-fm: 10.6472 Loss-g-mel: 17.5498 Loss-g-dur: 1.1390 Loss-g-kl: 1.6122 lr: 0.0002 grad_norm_g: 897.9690 grad_norm_d: 72.2172
======> Epoch: 2098
Train Epoch: 2099 [7.69%] G-Loss: 35.6648 D-Loss: 2.1327 Loss-g-fm: 11.8966 Loss-g-mel: 18.1470 Loss-g-dur: 1.2095 Loss-g-kl: 1.5148 lr: 0.0002 grad_norm_g: 444.8509 grad_norm_d: 12.9290
======> Epoch: 2099
Train Epoch: 2100 [3.85%] G-Loss: 35.3717 D-Loss: 2.0776 Loss-g-fm: 12.0188 Loss-g-mel: 17.5096 Loss-g-dur: 1.3100 Loss-g-kl: 1.5461 lr: 0.0002 grad_norm_g: 730.3846 grad_norm_d: 55.6508
======> Epoch: 2100
Train Epoch: 2101 [0.00%] G-Loss: 19.4018 D-Loss: 2.3384 Loss-g-fm: 6.4830 Loss-g-mel: 8.0276 Loss-g-dur: 0.6626 Loss-g-kl: 1.4544 lr: 0.0002 grad_norm_g: 1065.3074 grad_norm_d: 61.2523
Train Epoch: 2101 [96.15%] G-Loss: 34.5259 D-Loss: 2.1318 Loss-g-fm: 11.2513 Loss-g-mel: 17.5713 Loss-g-dur: 1.2254 Loss-g-kl: 1.5430 lr: 0.0002 grad_norm_g: 831.8197 grad_norm_d: 87.2753
======> Epoch: 2101
Train Epoch: 2102 [92.31%] G-Loss: 32.6117 D-Loss: 2.1136 Loss-g-fm: 10.8510 Loss-g-mel: 16.5489 Loss-g-dur: 1.1463 Loss-g-kl: 1.3307 lr: 0.0002 grad_norm_g: 778.5470 grad_norm_d: 68.0967
======> Epoch: 2102
Train Epoch: 2103 [88.46%] G-Loss: 34.7942 D-Loss: 2.0954 Loss-g-fm: 11.6090 Loss-g-mel: 17.2391 Loss-g-dur: 1.2742 Loss-g-kl: 1.7050 lr: 0.0002 grad_norm_g: 667.0614 grad_norm_d: 29.9869
======> Epoch: 2103
Train Epoch: 2104 [84.62%] G-Loss: 34.5255 D-Loss: 2.1216 Loss-g-fm: 11.2384 Loss-g-mel: 17.6753 Loss-g-dur: 1.1995 Loss-g-kl: 1.6850 lr: 0.0002 grad_norm_g: 458.1898 grad_norm_d: 8.9992
======> Epoch: 2104
Train Epoch: 2105 [80.77%] G-Loss: 36.6269 D-Loss: 2.1138 Loss-g-fm: 12.0988 Loss-g-mel: 18.1887 Loss-g-dur: 1.3087 Loss-g-kl: 1.7943 lr: 0.0002 grad_norm_g: 591.8242 grad_norm_d: 31.3197
======> Epoch: 2105
Train Epoch: 2106 [76.92%] G-Loss: 33.9263 D-Loss: 2.2363 Loss-g-fm: 10.8102 Loss-g-mel: 17.5797 Loss-g-dur: 1.1406 Loss-g-kl: 1.6066 lr: 0.0002 grad_norm_g: 636.0475 grad_norm_d: 58.1451
======> Epoch: 2106
Train Epoch: 2107 [73.08%] G-Loss: 35.6181 D-Loss: 2.1732 Loss-g-fm: 11.5552 Loss-g-mel: 18.1475 Loss-g-dur: 1.2869 Loss-g-kl: 1.7932 lr: 0.0002 grad_norm_g: 865.4902 grad_norm_d: 53.7757
======> Epoch: 2107
Train Epoch: 2108 [69.23%] G-Loss: 34.2123 D-Loss: 2.1557 Loss-g-fm: 11.7063 Loss-g-mel: 17.1391 Loss-g-dur: 1.1321 Loss-g-kl: 1.1438 lr: 0.0002 grad_norm_g: 989.9783 grad_norm_d: 47.6381
======> Epoch: 2108
Train Epoch: 2109 [65.38%] G-Loss: 20.2697 D-Loss: 2.3677 Loss-g-fm: 5.9714 Loss-g-mel: 9.0455 Loss-g-dur: 0.9726 Loss-g-kl: 1.5923 lr: 0.0002 grad_norm_g: 1571.4117 grad_norm_d: 45.9231
======> Epoch: 2109
Train Epoch: 2110 [61.54%] G-Loss: 34.8608 D-Loss: 2.0985 Loss-g-fm: 11.5667 Loss-g-mel: 17.3352 Loss-g-dur: 1.1904 Loss-g-kl: 1.5339 lr: 0.0002 grad_norm_g: 1015.1949 grad_norm_d: 74.6077
======> Epoch: 2110
Train Epoch: 2111 [57.69%] G-Loss: 33.9132 D-Loss: 2.2991 Loss-g-fm: 10.4064 Loss-g-mel: 17.6760 Loss-g-dur: 1.2250 Loss-g-kl: 1.7584 lr: 0.0002 grad_norm_g: 723.4204 grad_norm_d: 49.7048
======> Epoch: 2111
Train Epoch: 2112 [53.85%] G-Loss: 33.4787 D-Loss: 2.0585 Loss-g-fm: 10.8788 Loss-g-mel: 16.9993 Loss-g-dur: 1.1367 Loss-g-kl: 1.5449 lr: 0.0002 grad_norm_g: 924.5992 grad_norm_d: 69.7816
======> Epoch: 2112
Train Epoch: 2113 [50.00%] G-Loss: 35.7313 D-Loss: 2.1703 Loss-g-fm: 11.7745 Loss-g-mel: 18.5551 Loss-g-dur: 1.2060 Loss-g-kl: 1.5491 lr: 0.0002 grad_norm_g: 430.0219 grad_norm_d: 20.4231
======> Epoch: 2113
Train Epoch: 2114 [46.15%] G-Loss: 18.3318 D-Loss: 2.3876 Loss-g-fm: 5.8428 Loss-g-mel: 7.7120 Loss-g-dur: 0.5775 Loss-g-kl: 1.4562 lr: 0.0002 grad_norm_g: 1108.5807 grad_norm_d: 37.5973
======> Epoch: 2114
Train Epoch: 2115 [42.31%] G-Loss: 32.6635 D-Loss: 2.1234 Loss-g-fm: 10.6310 Loss-g-mel: 16.8631 Loss-g-dur: 1.1399 Loss-g-kl: 1.2802 lr: 0.0002 grad_norm_g: 1012.0907 grad_norm_d: 61.8022
======> Epoch: 2115
Train Epoch: 2116 [38.46%] G-Loss: 33.8076 D-Loss: 2.3336 Loss-g-fm: 11.0600 Loss-g-mel: 17.1615 Loss-g-dur: 1.1089 Loss-g-kl: 1.4643 lr: 0.0002 grad_norm_g: 802.7008 grad_norm_d: 103.2601
======> Epoch: 2116
Train Epoch: 2117 [34.62%] G-Loss: 31.4974 D-Loss: 2.3339 Loss-g-fm: 9.8471 Loss-g-mel: 16.6097 Loss-g-dur: 1.1114 Loss-g-kl: 1.2969 lr: 0.0002 grad_norm_g: 706.6168 grad_norm_d: 81.4623
======> Epoch: 2117
Train Epoch: 2118 [30.77%] G-Loss: 17.8722 D-Loss: 2.5428 Loss-g-fm: 5.2515 Loss-g-mel: 7.7938 Loss-g-dur: 0.7560 Loss-g-kl: 1.5840 lr: 0.0002 grad_norm_g: 725.0656 grad_norm_d: 51.2772
======> Epoch: 2118
Train Epoch: 2119 [26.92%] G-Loss: 34.1087 D-Loss: 2.0930 Loss-g-fm: 11.8880 Loss-g-mel: 16.9220 Loss-g-dur: 1.1010 Loss-g-kl: 1.3559 lr: 0.0002 grad_norm_g: 950.3745 grad_norm_d: 30.8400
======> Epoch: 2119
Train Epoch: 2120 [23.08%] G-Loss: 34.1452 D-Loss: 2.3568 Loss-g-fm: 10.7238 Loss-g-mel: 17.7206 Loss-g-dur: 1.2567 Loss-g-kl: 1.7034 lr: 0.0002 grad_norm_g: 239.3864 grad_norm_d: 71.2871
======> Epoch: 2120
Train Epoch: 2121 [19.23%] G-Loss: 32.0906 D-Loss: 2.2344 Loss-g-fm: 10.2073 Loss-g-mel: 16.5711 Loss-g-dur: 1.0909 Loss-g-kl: 1.3834 lr: 0.0002 grad_norm_g: 588.9469 grad_norm_d: 29.7674
======> Epoch: 2121
Train Epoch: 2122 [15.38%] G-Loss: 32.4500 D-Loss: 2.1444 Loss-g-fm: 10.3892 Loss-g-mel: 16.7974 Loss-g-dur: 1.1510 Loss-g-kl: 1.3395 lr: 0.0002 grad_norm_g: 277.1810 grad_norm_d: 11.9412
======> Epoch: 2122
Train Epoch: 2123 [11.54%] G-Loss: 31.6586 D-Loss: 2.1643 Loss-g-fm: 9.8060 Loss-g-mel: 16.6767 Loss-g-dur: 1.1312 Loss-g-kl: 1.3484 lr: 0.0002 grad_norm_g: 647.1233 grad_norm_d: 33.9568
======> Epoch: 2123
Train Epoch: 2124 [7.69%] G-Loss: 31.4448 D-Loss: 2.1967 Loss-g-fm: 9.6469 Loss-g-mel: 16.4500 Loss-g-dur: 1.1355 Loss-g-kl: 1.4827 lr: 0.0002 grad_norm_g: 811.8471 grad_norm_d: 71.5102
======> Epoch: 2124
Train Epoch: 2125 [3.85%] G-Loss: 34.7296 D-Loss: 2.0858 Loss-g-fm: 11.5361 Loss-g-mel: 17.4405 Loss-g-dur: 1.2312 Loss-g-kl: 1.6957 lr: 0.0002 grad_norm_g: 735.2944 grad_norm_d: 31.4632
======> Epoch: 2125
Train Epoch: 2126 [0.00%] G-Loss: 34.2918 D-Loss: 1.9705 Loss-g-fm: 11.6145 Loss-g-mel: 17.1976 Loss-g-dur: 1.0953 Loss-g-kl: 1.4902 lr: 0.0002 grad_norm_g: 824.5175 grad_norm_d: 54.8555
Train Epoch: 2126 [96.15%] G-Loss: 28.2883 D-Loss: 2.2366 Loss-g-fm: 9.6729 Loss-g-mel: 13.4614 Loss-g-dur: 0.9765 Loss-g-kl: 1.4359 lr: 0.0002 grad_norm_g: 141.0193 grad_norm_d: 23.4000
======> Epoch: 2126
Train Epoch: 2127 [92.31%] G-Loss: 33.0839 D-Loss: 2.2558 Loss-g-fm: 10.7559 Loss-g-mel: 17.0187 Loss-g-dur: 1.1099 Loss-g-kl: 1.3132 lr: 0.0002 grad_norm_g: 882.5493 grad_norm_d: 72.4379
======> Epoch: 2127
Train Epoch: 2128 [88.46%] G-Loss: 18.6258 D-Loss: 2.3620 Loss-g-fm: 5.7420 Loss-g-mel: 8.1858 Loss-g-dur: 0.6959 Loss-g-kl: 1.4118 lr: 0.0002 grad_norm_g: 1100.4264 grad_norm_d: 79.8442
======> Epoch: 2128
Train Epoch: 2129 [84.62%] G-Loss: 33.1020 D-Loss: 2.1168 Loss-g-fm: 10.5111 Loss-g-mel: 17.4241 Loss-g-dur: 1.1758 Loss-g-kl: 1.3078 lr: 0.0002 grad_norm_g: 817.4848 grad_norm_d: 55.0366
======> Epoch: 2129
Train Epoch: 2130 [80.77%] G-Loss: 33.1895 D-Loss: 2.1877 Loss-g-fm: 10.4902 Loss-g-mel: 16.9654 Loss-g-dur: 1.2013 Loss-g-kl: 1.4170 lr: 0.0002 grad_norm_g: 342.5886 grad_norm_d: 16.8989
======> Epoch: 2130
Train Epoch: 2131 [76.92%] G-Loss: 32.7075 D-Loss: 2.1384 Loss-g-fm: 10.4277 Loss-g-mel: 16.6319 Loss-g-dur: 1.1845 Loss-g-kl: 1.4419 lr: 0.0002 grad_norm_g: 918.2515 grad_norm_d: 67.5703
======> Epoch: 2131
Train Epoch: 2132 [73.08%] G-Loss: 33.5246 D-Loss: 2.1845 Loss-g-fm: 11.1255 Loss-g-mel: 17.1828 Loss-g-dur: 1.1864 Loss-g-kl: 1.4616 lr: 0.0002 grad_norm_g: 1070.3658 grad_norm_d: 59.5666
======> Epoch: 2132
Train Epoch: 2133 [69.23%] G-Loss: 35.9819 D-Loss: 2.0299 Loss-g-fm: 12.0865 Loss-g-mel: 18.1008 Loss-g-dur: 1.2048 Loss-g-kl: 1.6826 lr: 0.0002 grad_norm_g: 765.5805 grad_norm_d: 28.9992
======> Epoch: 2133
Train Epoch: 2134 [65.38%] G-Loss: 18.1972 D-Loss: 2.4057 Loss-g-fm: 5.6662 Loss-g-mel: 8.0164 Loss-g-dur: 0.6722 Loss-g-kl: 1.3745 lr: 0.0002 grad_norm_g: 1128.1684 grad_norm_d: 55.8779
======> Epoch: 2134
Train Epoch: 2135 [61.54%] G-Loss: 36.9233 D-Loss: 2.0466 Loss-g-fm: 12.5699 Loss-g-mel: 18.0622 Loss-g-dur: 1.2464 Loss-g-kl: 1.7432 lr: 0.0002 grad_norm_g: 744.5585 grad_norm_d: 73.9433
======> Epoch: 2135
Train Epoch: 2136 [57.69%] G-Loss: 19.2921 D-Loss: 2.3471 Loss-g-fm: 6.0949 Loss-g-mel: 8.3438 Loss-g-dur: 0.6405 Loss-g-kl: 1.4641 lr: 0.0002 grad_norm_g: 1572.6502 grad_norm_d: 54.5719
======> Epoch: 2136
Train Epoch: 2137 [53.85%] G-Loss: 33.6833 D-Loss: 2.2723 Loss-g-fm: 10.6837 Loss-g-mel: 17.1742 Loss-g-dur: 1.3465 Loss-g-kl: 1.6658 lr: 0.0002 grad_norm_g: 287.5997 grad_norm_d: 43.5051
======> Epoch: 2137
Train Epoch: 2138 [50.00%] G-Loss: 32.1472 D-Loss: 2.2811 Loss-g-fm: 9.8533 Loss-g-mel: 17.0281 Loss-g-dur: 1.1552 Loss-g-kl: 1.4598 lr: 0.0002 grad_norm_g: 864.5941 grad_norm_d: 77.1417
======> Epoch: 2138
Train Epoch: 2139 [46.15%] G-Loss: 36.0156 D-Loss: 2.1116 Loss-g-fm: 12.4712 Loss-g-mel: 17.9011 Loss-g-dur: 1.3198 Loss-g-kl: 1.5542 lr: 0.0002 grad_norm_g: 677.3850 grad_norm_d: 50.3051
======> Epoch: 2139
Train Epoch: 2140 [42.31%] G-Loss: 31.7980 D-Loss: 2.2745 Loss-g-fm: 10.0410 Loss-g-mel: 16.7906 Loss-g-dur: 1.1509 Loss-g-kl: 1.4326 lr: 0.0002 grad_norm_g: 38.9560 grad_norm_d: 7.9801
======> Epoch: 2140
Train Epoch: 2141 [38.46%] G-Loss: 32.8812 D-Loss: 2.2365 Loss-g-fm: 10.6122 Loss-g-mel: 17.3214 Loss-g-dur: 1.1305 Loss-g-kl: 1.3798 lr: 0.0002 grad_norm_g: 428.6474 grad_norm_d: 24.5877
======> Epoch: 2141
Train Epoch: 2142 [34.62%] G-Loss: 32.1455 D-Loss: 2.2339 Loss-g-fm: 10.7584 Loss-g-mel: 16.5635 Loss-g-dur: 1.1183 Loss-g-kl: 1.2827 lr: 0.0002 grad_norm_g: 643.4264 grad_norm_d: 41.0291
======> Epoch: 2142
Train Epoch: 2143 [30.77%] G-Loss: 33.6988 D-Loss: 2.0602 Loss-g-fm: 11.4390 Loss-g-mel: 16.7563 Loss-g-dur: 1.1355 Loss-g-kl: 1.6306 lr: 0.0002 grad_norm_g: 881.9245 grad_norm_d: 59.5633
======> Epoch: 2143
Train Epoch: 2144 [26.92%] G-Loss: 28.9646 D-Loss: 2.0395 Loss-g-fm: 10.1055 Loss-g-mel: 13.6895 Loss-g-dur: 0.9694 Loss-g-kl: 1.4447 lr: 0.0002 grad_norm_g: 990.6411 grad_norm_d: 60.8698
======> Epoch: 2144
Train Epoch: 2145 [23.08%] G-Loss: 33.7825 D-Loss: 2.1851 Loss-g-fm: 10.6063 Loss-g-mel: 17.1380 Loss-g-dur: 1.2346 Loss-g-kl: 1.7188 lr: 0.0002 grad_norm_g: 896.0266 grad_norm_d: 41.6032
======> Epoch: 2145
Train Epoch: 2146 [19.23%] G-Loss: 37.2970 D-Loss: 2.0923 Loss-g-fm: 13.0162 Loss-g-mel: 18.3600 Loss-g-dur: 1.2477 Loss-g-kl: 1.5065 lr: 0.0002 grad_norm_g: 976.4549 grad_norm_d: 52.4011
======> Epoch: 2146
Train Epoch: 2147 [15.38%] G-Loss: 34.0190 D-Loss: 2.1389 Loss-g-fm: 11.3687 Loss-g-mel: 17.4339 Loss-g-dur: 1.1411 Loss-g-kl: 1.1841 lr: 0.0002 grad_norm_g: 954.6007 grad_norm_d: 66.5043
======> Epoch: 2147
Train Epoch: 2148 [11.54%] G-Loss: 34.5209 D-Loss: 2.1928 Loss-g-fm: 11.1596 Loss-g-mel: 17.7934 Loss-g-dur: 1.1916 Loss-g-kl: 1.5930 lr: 0.0002 grad_norm_g: 741.5672 grad_norm_d: 70.0509
======> Epoch: 2148
Train Epoch: 2149 [7.69%] G-Loss: 33.9266 D-Loss: 2.1732 Loss-g-fm: 11.3324 Loss-g-mel: 17.2105 Loss-g-dur: 1.1388 Loss-g-kl: 1.4792 lr: 0.0002 grad_norm_g: 720.7931 grad_norm_d: 92.5417
======> Epoch: 2149
Train Epoch: 2150 [3.85%] G-Loss: 18.0816 D-Loss: 2.4308 Loss-g-fm: 5.5663 Loss-g-mel: 7.5678 Loss-g-dur: 0.7798 Loss-g-kl: 1.5247 lr: 0.0001 grad_norm_g: 920.5928 grad_norm_d: 74.8297
======> Epoch: 2150
Train Epoch: 2151 [0.00%] G-Loss: 34.4556 D-Loss: 2.0220 Loss-g-fm: 11.8375 Loss-g-mel: 17.2697 Loss-g-dur: 1.1347 Loss-g-kl: 1.4806 lr: 0.0001 grad_norm_g: 127.1032 grad_norm_d: 5.4077
Train Epoch: 2151 [96.15%] G-Loss: 33.3132 D-Loss: 2.1462 Loss-g-fm: 10.8584 Loss-g-mel: 17.2837 Loss-g-dur: 1.1103 Loss-g-kl: 1.3516 lr: 0.0001 grad_norm_g: 521.1526 grad_norm_d: 46.5701
======> Epoch: 2151
Train Epoch: 2152 [92.31%] G-Loss: 30.3957 D-Loss: 2.1539 Loss-g-fm: 9.4267 Loss-g-mel: 15.8543 Loss-g-dur: 1.1238 Loss-g-kl: 1.3069 lr: 0.0001 grad_norm_g: 771.0226 grad_norm_d: 104.6972
======> Epoch: 2152
Train Epoch: 2153 [88.46%] G-Loss: 35.5388 D-Loss: 2.1828 Loss-g-fm: 11.9426 Loss-g-mel: 18.1030 Loss-g-dur: 1.2039 Loss-g-kl: 1.5962 lr: 0.0001 grad_norm_g: 713.9868 grad_norm_d: 34.4135
======> Epoch: 2153
Train Epoch: 2154 [84.62%] G-Loss: 34.7907 D-Loss: 2.1817 Loss-g-fm: 11.0319 Loss-g-mel: 17.5930 Loss-g-dur: 1.3221 Loss-g-kl: 1.6901 lr: 0.0001 grad_norm_g: 146.9542 grad_norm_d: 40.5679
Saving model and optimizer state at iteration 2154 to /ZFS4T/tts/data/VITS/model_saved/G_56000.pth
Saving model and optimizer state at iteration 2154 to /ZFS4T/tts/data/VITS/model_saved/D_56000.pth
======> Epoch: 2154
Train Epoch: 2155 [80.77%] G-Loss: 31.4787 D-Loss: 2.4096 Loss-g-fm: 9.0914 Loss-g-mel: 16.6755 Loss-g-dur: 1.2717 Loss-g-kl: 1.3054 lr: 0.0001 grad_norm_g: 842.9084 grad_norm_d: 75.8543
======> Epoch: 2155
Train Epoch: 2156 [76.92%] G-Loss: 33.3879 D-Loss: 2.2171 Loss-g-fm: 11.0760 Loss-g-mel: 16.7431 Loss-g-dur: 1.1820 Loss-g-kl: 1.4673 lr: 0.0001 grad_norm_g: 736.4881 grad_norm_d: 41.9653
======> Epoch: 2156
Train Epoch: 2157 [73.08%] G-Loss: 32.3719 D-Loss: 2.1690 Loss-g-fm: 10.3589 Loss-g-mel: 16.7215 Loss-g-dur: 1.1720 Loss-g-kl: 1.3208 lr: 0.0001 grad_norm_g: 360.2975 grad_norm_d: 26.1150
======> Epoch: 2157
Train Epoch: 2158 [69.23%] G-Loss: 35.0561 D-Loss: 2.1600 Loss-g-fm: 11.4867 Loss-g-mel: 17.9514 Loss-g-dur: 1.2690 Loss-g-kl: 1.6249 lr: 0.0001 grad_norm_g: 653.4967 grad_norm_d: 34.8899
======> Epoch: 2158
Train Epoch: 2159 [65.38%] G-Loss: 34.3185 D-Loss: 2.2167 Loss-g-fm: 11.1155 Loss-g-mel: 17.7546 Loss-g-dur: 1.2253 Loss-g-kl: 1.5876 lr: 0.0001 grad_norm_g: 781.6530 grad_norm_d: 54.7011
======> Epoch: 2159
Train Epoch: 2160 [61.54%] G-Loss: 35.1905 D-Loss: 2.0495 Loss-g-fm: 11.4833 Loss-g-mel: 18.2169 Loss-g-dur: 1.2408 Loss-g-kl: 1.3696 lr: 0.0001 grad_norm_g: 648.8485 grad_norm_d: 26.2736
======> Epoch: 2160
Train Epoch: 2161 [57.69%] G-Loss: 33.9384 D-Loss: 2.2236 Loss-g-fm: 10.8168 Loss-g-mel: 17.6321 Loss-g-dur: 1.1737 Loss-g-kl: 1.7242 lr: 0.0001 grad_norm_g: 725.4013 grad_norm_d: 55.3563
======> Epoch: 2161
Train Epoch: 2162 [53.85%] G-Loss: 33.0727 D-Loss: 2.1910 Loss-g-fm: 10.4792 Loss-g-mel: 16.7945 Loss-g-dur: 1.1675 Loss-g-kl: 1.4292 lr: 0.0001 grad_norm_g: 916.2530 grad_norm_d: 83.4397
======> Epoch: 2162
Train Epoch: 2163 [50.00%] G-Loss: 34.2856 D-Loss: 2.0816 Loss-g-fm: 11.4893 Loss-g-mel: 17.4964 Loss-g-dur: 1.2039 Loss-g-kl: 1.5418 lr: 0.0001 grad_norm_g: 896.0433 grad_norm_d: 59.8071
======> Epoch: 2163
Train Epoch: 2164 [46.15%] G-Loss: 29.2519 D-Loss: 2.1712 Loss-g-fm: 9.8425 Loss-g-mel: 13.7487 Loss-g-dur: 1.0225 Loss-g-kl: 1.6030 lr: 0.0001 grad_norm_g: 898.2537 grad_norm_d: 79.5174
======> Epoch: 2164
Train Epoch: 2165 [42.31%] G-Loss: 33.1736 D-Loss: 2.1431 Loss-g-fm: 11.2099 Loss-g-mel: 16.7007 Loss-g-dur: 1.1295 Loss-g-kl: 1.4755 lr: 0.0001 grad_norm_g: 617.6048 grad_norm_d: 31.2550
======> Epoch: 2165
Train Epoch: 2166 [38.46%] G-Loss: 31.1765 D-Loss: 2.3140 Loss-g-fm: 9.8955 Loss-g-mel: 16.3826 Loss-g-dur: 1.1432 Loss-g-kl: 1.2827 lr: 0.0001 grad_norm_g: 988.3624 grad_norm_d: 55.3478
======> Epoch: 2166
Train Epoch: 2167 [34.62%] G-Loss: 33.3624 D-Loss: 2.2831 Loss-g-fm: 10.3122 Loss-g-mel: 17.5863 Loss-g-dur: 1.1021 Loss-g-kl: 1.5511 lr: 0.0001 grad_norm_g: 712.8098 grad_norm_d: 114.1064
======> Epoch: 2167
Train Epoch: 2168 [30.77%] G-Loss: 33.4280 D-Loss: 2.1758 Loss-g-fm: 10.5072 Loss-g-mel: 17.3861 Loss-g-dur: 1.2225 Loss-g-kl: 1.5229 lr: 0.0001 grad_norm_g: 593.5132 grad_norm_d: 22.2722
======> Epoch: 2168
Train Epoch: 2169 [26.92%] G-Loss: 34.2256 D-Loss: 2.1414 Loss-g-fm: 11.4675 Loss-g-mel: 17.1550 Loss-g-dur: 1.1454 Loss-g-kl: 1.5538 lr: 0.0001 grad_norm_g: 402.5567 grad_norm_d: 7.4315
======> Epoch: 2169
Train Epoch: 2170 [23.08%] G-Loss: 36.6388 D-Loss: 2.0410 Loss-g-fm: 12.4445 Loss-g-mel: 18.3927 Loss-g-dur: 1.2203 Loss-g-kl: 1.8346 lr: 0.0001 grad_norm_g: 338.6229 grad_norm_d: 16.7942
======> Epoch: 2170
Train Epoch: 2171 [19.23%] G-Loss: 33.9524 D-Loss: 2.0995 Loss-g-fm: 11.1595 Loss-g-mel: 17.3555 Loss-g-dur: 1.1559 Loss-g-kl: 1.4292 lr: 0.0001 grad_norm_g: 296.6770 grad_norm_d: 45.8543
======> Epoch: 2171
Train Epoch: 2172 [15.38%] G-Loss: 33.1543 D-Loss: 2.0900 Loss-g-fm: 10.4203 Loss-g-mel: 17.2190 Loss-g-dur: 1.1735 Loss-g-kl: 1.5171 lr: 0.0001 grad_norm_g: 934.4866 grad_norm_d: 55.3807
======> Epoch: 2172
Train Epoch: 2173 [11.54%] G-Loss: 30.4761 D-Loss: 2.3012 Loss-g-fm: 9.2321 Loss-g-mel: 16.0118 Loss-g-dur: 1.1455 Loss-g-kl: 1.5746 lr: 0.0001 grad_norm_g: 282.3619 grad_norm_d: 22.0208
======> Epoch: 2173
Train Epoch: 2174 [7.69%] G-Loss: 33.2649 D-Loss: 2.1216 Loss-g-fm: 10.7807 Loss-g-mel: 16.9747 Loss-g-dur: 1.1464 Loss-g-kl: 1.5052 lr: 0.0001 grad_norm_g: 642.4548 grad_norm_d: 28.7822
======> Epoch: 2174
Train Epoch: 2175 [3.85%] G-Loss: 34.6937 D-Loss: 2.0976 Loss-g-fm: 11.1304 Loss-g-mel: 18.0123 Loss-g-dur: 1.1814 Loss-g-kl: 1.5088 lr: 0.0001 grad_norm_g: 877.1146 grad_norm_d: 56.4870
======> Epoch: 2175
Train Epoch: 2176 [0.00%] G-Loss: 34.6521 D-Loss: 2.1766 Loss-g-fm: 11.1416 Loss-g-mel: 17.8698 Loss-g-dur: 1.1989 Loss-g-kl: 1.5132 lr: 0.0001 grad_norm_g: 899.4294 grad_norm_d: 47.2559
Train Epoch: 2176 [96.15%] G-Loss: 34.7113 D-Loss: 2.1373 Loss-g-fm: 12.1646 Loss-g-mel: 17.5040 Loss-g-dur: 1.1393 Loss-g-kl: 1.4019 lr: 0.0001 grad_norm_g: 393.2360 grad_norm_d: 21.6795
======> Epoch: 2176
Train Epoch: 2177 [92.31%] G-Loss: 31.7831 D-Loss: 2.2763 Loss-g-fm: 9.8547 Loss-g-mel: 16.4922 Loss-g-dur: 1.2806 Loss-g-kl: 1.4409 lr: 0.0001 grad_norm_g: 942.9358 grad_norm_d: 54.1486
======> Epoch: 2177
Train Epoch: 2178 [88.46%] G-Loss: 34.0404 D-Loss: 2.2239 Loss-g-fm: 11.4601 Loss-g-mel: 17.2736 Loss-g-dur: 1.1313 Loss-g-kl: 1.4052 lr: 0.0001 grad_norm_g: 915.9690 grad_norm_d: 84.3856
======> Epoch: 2178
Train Epoch: 2179 [84.62%] G-Loss: 32.1845 D-Loss: 2.3255 Loss-g-fm: 10.2959 Loss-g-mel: 16.7884 Loss-g-dur: 1.1203 Loss-g-kl: 1.4193 lr: 0.0001 grad_norm_g: 770.3491 grad_norm_d: 60.1025
======> Epoch: 2179
Train Epoch: 2180 [80.77%] G-Loss: 34.7433 D-Loss: 2.2639 Loss-g-fm: 11.2385 Loss-g-mel: 18.0544 Loss-g-dur: 1.1972 Loss-g-kl: 1.5304 lr: 0.0001 grad_norm_g: 55.7153 grad_norm_d: 24.5374
======> Epoch: 2180
Train Epoch: 2181 [76.92%] G-Loss: 35.2086 D-Loss: 2.0642 Loss-g-fm: 11.8865 Loss-g-mel: 17.5943 Loss-g-dur: 1.2587 Loss-g-kl: 1.7493 lr: 0.0001 grad_norm_g: 224.6584 grad_norm_d: 46.7651
======> Epoch: 2181
Train Epoch: 2182 [73.08%] G-Loss: 35.6216 D-Loss: 2.1448 Loss-g-fm: 11.7381 Loss-g-mel: 18.1305 Loss-g-dur: 1.2399 Loss-g-kl: 1.7541 lr: 0.0001 grad_norm_g: 820.7365 grad_norm_d: 52.9558
======> Epoch: 2182
Train Epoch: 2183 [69.23%] G-Loss: 34.3894 D-Loss: 2.1413 Loss-g-fm: 10.9785 Loss-g-mel: 17.7475 Loss-g-dur: 1.2209 Loss-g-kl: 1.7275 lr: 0.0001 grad_norm_g: 757.6559 grad_norm_d: 50.3362
======> Epoch: 2183
Train Epoch: 2184 [65.38%] G-Loss: 35.6566 D-Loss: 2.1465 Loss-g-fm: 11.8497 Loss-g-mel: 18.2915 Loss-g-dur: 1.1914 Loss-g-kl: 1.7492 lr: 0.0001 grad_norm_g: 352.3851 grad_norm_d: 15.3247
======> Epoch: 2184
Train Epoch: 2185 [61.54%] G-Loss: 17.7503 D-Loss: 2.4053 Loss-g-fm: 5.2231 Loss-g-mel: 7.5274 Loss-g-dur: 0.6462 Loss-g-kl: 1.3354 lr: 0.0001 grad_norm_g: 1712.8955 grad_norm_d: 87.8671
======> Epoch: 2185
Train Epoch: 2186 [57.69%] G-Loss: 18.0279 D-Loss: 2.3862 Loss-g-fm: 5.5834 Loss-g-mel: 7.7845 Loss-g-dur: 0.6569 Loss-g-kl: 1.4559 lr: 0.0001 grad_norm_g: 1672.9970 grad_norm_d: 78.4817
======> Epoch: 2186
Train Epoch: 2187 [53.85%] G-Loss: 35.6834 D-Loss: 2.3799 Loss-g-fm: 12.1594 Loss-g-mel: 17.8359 Loss-g-dur: 1.2090 Loss-g-kl: 1.6999 lr: 0.0001 grad_norm_g: 44.5821 grad_norm_d: 16.0427
======> Epoch: 2187
Train Epoch: 2188 [50.00%] G-Loss: 34.3308 D-Loss: 2.0738 Loss-g-fm: 11.9524 Loss-g-mel: 17.0147 Loss-g-dur: 1.0998 Loss-g-kl: 1.6450 lr: 0.0001 grad_norm_g: 1086.3436 grad_norm_d: 58.5863
======> Epoch: 2188
Train Epoch: 2189 [46.15%] G-Loss: 33.8179 D-Loss: 2.2655 Loss-g-fm: 10.9489 Loss-g-mel: 17.4899 Loss-g-dur: 1.1862 Loss-g-kl: 1.5164 lr: 0.0001 grad_norm_g: 967.8050 grad_norm_d: 66.5925
======> Epoch: 2189
Train Epoch: 2190 [42.31%] G-Loss: 18.3216 D-Loss: 2.5200 Loss-g-fm: 6.1079 Loss-g-mel: 7.6999 Loss-g-dur: 0.6331 Loss-g-kl: 1.3057 lr: 0.0001 grad_norm_g: 1326.7285 grad_norm_d: 84.5799
======> Epoch: 2190
Train Epoch: 2191 [38.46%] G-Loss: 36.1830 D-Loss: 2.1709 Loss-g-fm: 12.9143 Loss-g-mel: 17.9386 Loss-g-dur: 1.2219 Loss-g-kl: 1.5729 lr: 0.0001 grad_norm_g: 393.0082 grad_norm_d: 39.6057
======> Epoch: 2191
Train Epoch: 2192 [34.62%] G-Loss: 35.8164 D-Loss: 2.2096 Loss-g-fm: 12.4765 Loss-g-mel: 18.0690 Loss-g-dur: 1.1813 Loss-g-kl: 1.5710 lr: 0.0001 grad_norm_g: 466.1963 grad_norm_d: 9.5615
======> Epoch: 2192
Train Epoch: 2193 [30.77%] G-Loss: 31.5497 D-Loss: 2.2987 Loss-g-fm: 10.1631 Loss-g-mel: 16.1398 Loss-g-dur: 1.1777 Loss-g-kl: 1.2330 lr: 0.0001 grad_norm_g: 828.9297 grad_norm_d: 50.6212
======> Epoch: 2193
Train Epoch: 2194 [26.92%] G-Loss: 34.0678 D-Loss: 2.1421 Loss-g-fm: 10.8788 Loss-g-mel: 17.7336 Loss-g-dur: 1.2446 Loss-g-kl: 1.5094 lr: 0.0001 grad_norm_g: 997.2214 grad_norm_d: 45.7710
======> Epoch: 2194
Train Epoch: 2195 [23.08%] G-Loss: 33.4614 D-Loss: 2.3450 Loss-g-fm: 11.2026 Loss-g-mel: 16.8415 Loss-g-dur: 1.1351 Loss-g-kl: 1.4544 lr: 0.0001 grad_norm_g: 845.4725 grad_norm_d: 81.5735
======> Epoch: 2195
Train Epoch: 2196 [19.23%] G-Loss: 34.2145 D-Loss: 2.1609 Loss-g-fm: 11.6987 Loss-g-mel: 17.4083 Loss-g-dur: 1.1211 Loss-g-kl: 1.5115 lr: 0.0001 grad_norm_g: 818.1805 grad_norm_d: 53.8135
======> Epoch: 2196
Train Epoch: 2197 [15.38%] G-Loss: 30.4506 D-Loss: 2.2954 Loss-g-fm: 8.8706 Loss-g-mel: 16.4064 Loss-g-dur: 1.1119 Loss-g-kl: 1.3767 lr: 0.0001 grad_norm_g: 972.6013 grad_norm_d: 71.7845
======> Epoch: 2197
Train Epoch: 2198 [11.54%] G-Loss: 33.5086 D-Loss: 2.0462 Loss-g-fm: 11.3100 Loss-g-mel: 16.7578 Loss-g-dur: 1.1725 Loss-g-kl: 1.5634 lr: 0.0001 grad_norm_g: 972.9005 grad_norm_d: 54.6771
======> Epoch: 2198
Train Epoch: 2199 [7.69%] G-Loss: 35.0516 D-Loss: 2.1756 Loss-g-fm: 11.8583 Loss-g-mel: 17.6683 Loss-g-dur: 1.2782 Loss-g-kl: 1.6903 lr: 0.0001 grad_norm_g: 781.8026 grad_norm_d: 52.9979
======> Epoch: 2199
Train Epoch: 2200 [3.85%] G-Loss: 33.4326 D-Loss: 2.1247 Loss-g-fm: 11.2618 Loss-g-mel: 16.9987 Loss-g-dur: 1.1314 Loss-g-kl: 1.3781 lr: 0.0001 grad_norm_g: 613.7304 grad_norm_d: 21.0181
======> Epoch: 2200
Train Epoch: 2201 [0.00%] G-Loss: 32.4936 D-Loss: 2.1494 Loss-g-fm: 10.0140 Loss-g-mel: 17.1401 Loss-g-dur: 1.1483 Loss-g-kl: 1.4616 lr: 0.0001 grad_norm_g: 72.1236 grad_norm_d: 30.3443
Train Epoch: 2201 [96.15%] G-Loss: 33.1850 D-Loss: 2.1014 Loss-g-fm: 11.1290 Loss-g-mel: 16.7211 Loss-g-dur: 1.1153 Loss-g-kl: 1.3030 lr: 0.0001 grad_norm_g: 1040.8848 grad_norm_d: 56.8693
======> Epoch: 2201
Train Epoch: 2202 [92.31%] G-Loss: 33.8557 D-Loss: 2.1355 Loss-g-fm: 11.3426 Loss-g-mel: 17.0502 Loss-g-dur: 1.1908 Loss-g-kl: 1.4486 lr: 0.0001 grad_norm_g: 911.1242 grad_norm_d: 100.3328
======> Epoch: 2202
Train Epoch: 2203 [88.46%] G-Loss: 38.7614 D-Loss: 2.0613 Loss-g-fm: 13.6055 Loss-g-mel: 19.1708 Loss-g-dur: 1.2313 Loss-g-kl: 1.5327 lr: 0.0001 grad_norm_g: 881.0503 grad_norm_d: 54.4769
======> Epoch: 2203
Train Epoch: 2204 [84.62%] G-Loss: 33.7673 D-Loss: 2.2371 Loss-g-fm: 11.0217 Loss-g-mel: 17.2398 Loss-g-dur: 1.1980 Loss-g-kl: 1.6130 lr: 0.0001 grad_norm_g: 329.2115 grad_norm_d: 10.0307
======> Epoch: 2204
Train Epoch: 2205 [80.77%] G-Loss: 35.4267 D-Loss: 2.1823 Loss-g-fm: 12.4687 Loss-g-mel: 17.3916 Loss-g-dur: 1.3027 Loss-g-kl: 1.6630 lr: 0.0001 grad_norm_g: 913.2751 grad_norm_d: 66.6711
======> Epoch: 2205
Train Epoch: 2206 [76.92%] G-Loss: 32.9067 D-Loss: 2.1511 Loss-g-fm: 10.5421 Loss-g-mel: 17.3033 Loss-g-dur: 1.1363 Loss-g-kl: 1.2518 lr: 0.0001 grad_norm_g: 578.9330 grad_norm_d: 84.0084
======> Epoch: 2206
Train Epoch: 2207 [73.08%] G-Loss: 33.8480 D-Loss: 2.2334 Loss-g-fm: 10.5753 Loss-g-mel: 17.8000 Loss-g-dur: 1.2228 Loss-g-kl: 1.5213 lr: 0.0001 grad_norm_g: 402.9601 grad_norm_d: 53.3533
======> Epoch: 2207
Train Epoch: 2208 [69.23%] G-Loss: 36.3621 D-Loss: 2.1113 Loss-g-fm: 12.3768 Loss-g-mel: 18.0671 Loss-g-dur: 1.1863 Loss-g-kl: 1.5949 lr: 0.0001 grad_norm_g: 1037.8321 grad_norm_d: 55.6726
======> Epoch: 2208
Train Epoch: 2209 [65.38%] G-Loss: 33.2279 D-Loss: 2.1175 Loss-g-fm: 11.1195 Loss-g-mel: 16.6572 Loss-g-dur: 1.1385 Loss-g-kl: 1.5786 lr: 0.0001 grad_norm_g: 852.6575 grad_norm_d: 59.4200
======> Epoch: 2209
Train Epoch: 2210 [61.54%] G-Loss: 31.9785 D-Loss: 2.1226 Loss-g-fm: 10.3446 Loss-g-mel: 16.2658 Loss-g-dur: 1.1305 Loss-g-kl: 1.4512 lr: 0.0001 grad_norm_g: 785.8743 grad_norm_d: 30.6017
======> Epoch: 2210
Train Epoch: 2211 [57.69%] G-Loss: 34.2236 D-Loss: 2.1467 Loss-g-fm: 11.6290 Loss-g-mel: 17.2079 Loss-g-dur: 1.1745 Loss-g-kl: 1.3823 lr: 0.0001 grad_norm_g: 772.2575 grad_norm_d: 39.7514
======> Epoch: 2211
Train Epoch: 2212 [53.85%] G-Loss: 36.6362 D-Loss: 2.0958 Loss-g-fm: 12.5217 Loss-g-mel: 18.4641 Loss-g-dur: 1.2362 Loss-g-kl: 1.8820 lr: 0.0001 grad_norm_g: 1071.7675 grad_norm_d: 63.2994
======> Epoch: 2212
Train Epoch: 2213 [50.00%] G-Loss: 32.8518 D-Loss: 2.1312 Loss-g-fm: 10.8698 Loss-g-mel: 16.7239 Loss-g-dur: 1.0875 Loss-g-kl: 1.3004 lr: 0.0001 grad_norm_g: 996.7530 grad_norm_d: 88.7070
======> Epoch: 2213
Train Epoch: 2214 [46.15%] G-Loss: 35.1224 D-Loss: 2.1260 Loss-g-fm: 11.9078 Loss-g-mel: 17.6723 Loss-g-dur: 1.1882 Loss-g-kl: 1.5772 lr: 0.0001 grad_norm_g: 716.3513 grad_norm_d: 44.5658
======> Epoch: 2214
Train Epoch: 2215 [42.31%] G-Loss: 27.8183 D-Loss: 2.1734 Loss-g-fm: 9.5270 Loss-g-mel: 13.5518 Loss-g-dur: 0.9380 Loss-g-kl: 1.4232 lr: 0.0001 grad_norm_g: 431.0938 grad_norm_d: 35.3160
======> Epoch: 2215
Train Epoch: 2216 [38.46%] G-Loss: 34.0892 D-Loss: 2.1490 Loss-g-fm: 11.1938 Loss-g-mel: 17.4057 Loss-g-dur: 1.1752 Loss-g-kl: 1.4516 lr: 0.0001 grad_norm_g: 920.2242 grad_norm_d: 46.7051
======> Epoch: 2216
Train Epoch: 2217 [34.62%] G-Loss: 34.5141 D-Loss: 2.1571 Loss-g-fm: 11.8138 Loss-g-mel: 17.0442 Loss-g-dur: 1.2169 Loss-g-kl: 1.6654 lr: 0.0001 grad_norm_g: 713.9196 grad_norm_d: 51.4309
======> Epoch: 2217
Train Epoch: 2218 [30.77%] G-Loss: 32.9379 D-Loss: 2.1783 Loss-g-fm: 11.5174 Loss-g-mel: 16.3719 Loss-g-dur: 1.2557 Loss-g-kl: 1.3869 lr: 0.0001 grad_norm_g: 979.4459 grad_norm_d: 71.6630
======> Epoch: 2218
Train Epoch: 2219 [26.92%] G-Loss: 32.5838 D-Loss: 2.0750 Loss-g-fm: 10.5983 Loss-g-mel: 16.4703 Loss-g-dur: 1.1979 Loss-g-kl: 1.5191 lr: 0.0001 grad_norm_g: 783.6560 grad_norm_d: 21.3614
======> Epoch: 2219
Train Epoch: 2220 [23.08%] G-Loss: 35.8761 D-Loss: 2.2302 Loss-g-fm: 11.8268 Loss-g-mel: 18.3137 Loss-g-dur: 1.2398 Loss-g-kl: 1.5953 lr: 0.0001 grad_norm_g: 951.9978 grad_norm_d: 51.6649
======> Epoch: 2220
Train Epoch: 2221 [19.23%] G-Loss: 33.1737 D-Loss: 2.2067 Loss-g-fm: 10.8182 Loss-g-mel: 16.9260 Loss-g-dur: 1.1231 Loss-g-kl: 1.5175 lr: 0.0001 grad_norm_g: 580.8014 grad_norm_d: 65.5779
======> Epoch: 2221
Train Epoch: 2222 [15.38%] G-Loss: 37.5797 D-Loss: 2.0724 Loss-g-fm: 13.5314 Loss-g-mel: 17.8684 Loss-g-dur: 1.2947 Loss-g-kl: 1.7676 lr: 0.0001 grad_norm_g: 1019.0092 grad_norm_d: 53.5763
======> Epoch: 2222
Train Epoch: 2223 [11.54%] G-Loss: 31.9048 D-Loss: 2.2654 Loss-g-fm: 10.1368 Loss-g-mel: 16.5753 Loss-g-dur: 1.1133 Loss-g-kl: 1.5648 lr: 0.0001 grad_norm_g: 516.2334 grad_norm_d: 80.4411
======> Epoch: 2223
Train Epoch: 2224 [7.69%] G-Loss: 35.1834 D-Loss: 2.1635 Loss-g-fm: 12.0452 Loss-g-mel: 17.6035 Loss-g-dur: 1.2048 Loss-g-kl: 1.5998 lr: 0.0001 grad_norm_g: 84.3899 grad_norm_d: 50.4283
======> Epoch: 2224
Train Epoch: 2225 [3.85%] G-Loss: 32.7432 D-Loss: 2.1970 Loss-g-fm: 10.0437 Loss-g-mel: 17.0267 Loss-g-dur: 1.1246 Loss-g-kl: 1.4391 lr: 0.0001 grad_norm_g: 942.5853 grad_norm_d: 76.1214
======> Epoch: 2225
Train Epoch: 2226 [0.00%] G-Loss: 32.6891 D-Loss: 2.2798 Loss-g-fm: 10.2235 Loss-g-mel: 17.2752 Loss-g-dur: 1.1264 Loss-g-kl: 1.4802 lr: 0.0001 grad_norm_g: 539.4030 grad_norm_d: 14.3823
Train Epoch: 2226 [96.15%] G-Loss: 18.4296 D-Loss: 2.4690 Loss-g-fm: 5.5627 Loss-g-mel: 8.5886 Loss-g-dur: 0.6325 Loss-g-kl: 1.4418 lr: 0.0001 grad_norm_g: 1168.0360 grad_norm_d: 37.2069
======> Epoch: 2226
Train Epoch: 2227 [92.31%] G-Loss: 36.2610 D-Loss: 2.1054 Loss-g-fm: 12.1973 Loss-g-mel: 18.0434 Loss-g-dur: 1.3191 Loss-g-kl: 1.5558 lr: 0.0001 grad_norm_g: 819.5131 grad_norm_d: 23.8109
======> Epoch: 2227
Train Epoch: 2228 [88.46%] G-Loss: 35.6148 D-Loss: 2.1309 Loss-g-fm: 11.9192 Loss-g-mel: 17.9443 Loss-g-dur: 1.2264 Loss-g-kl: 1.7887 lr: 0.0001 grad_norm_g: 781.9004 grad_norm_d: 63.0782
======> Epoch: 2228
Train Epoch: 2229 [84.62%] G-Loss: 35.0944 D-Loss: 2.1590 Loss-g-fm: 11.6800 Loss-g-mel: 17.6064 Loss-g-dur: 1.2194 Loss-g-kl: 1.5828 lr: 0.0001 grad_norm_g: 1030.2404 grad_norm_d: 84.2248
======> Epoch: 2229
Train Epoch: 2230 [80.77%] G-Loss: 32.9694 D-Loss: 2.1027 Loss-g-fm: 10.6813 Loss-g-mel: 17.0349 Loss-g-dur: 1.1691 Loss-g-kl: 1.2043 lr: 0.0001 grad_norm_g: 633.0501 grad_norm_d: 15.5384
======> Epoch: 2230
Train Epoch: 2231 [76.92%] G-Loss: 34.9934 D-Loss: 2.1150 Loss-g-fm: 11.7536 Loss-g-mel: 17.3561 Loss-g-dur: 1.2009 Loss-g-kl: 1.6254 lr: 0.0001 grad_norm_g: 419.4242 grad_norm_d: 59.9965
Saving model and optimizer state at iteration 2231 to /ZFS4T/tts/data/VITS/model_saved/G_58000.pth
Saving model and optimizer state at iteration 2231 to /ZFS4T/tts/data/VITS/model_saved/D_58000.pth
======> Epoch: 2231
Train Epoch: 2232 [73.08%] G-Loss: 33.2000 D-Loss: 2.3290 Loss-g-fm: 10.4882 Loss-g-mel: 17.2001 Loss-g-dur: 1.1985 Loss-g-kl: 1.5877 lr: 0.0001 grad_norm_g: 491.9520 grad_norm_d: 20.9688
======> Epoch: 2232
Train Epoch: 2233 [69.23%] G-Loss: 35.3909 D-Loss: 2.0296 Loss-g-fm: 12.1130 Loss-g-mel: 17.5513 Loss-g-dur: 1.1213 Loss-g-kl: 1.6320 lr: 0.0001 grad_norm_g: 1030.6262 grad_norm_d: 71.8420
======> Epoch: 2233
Train Epoch: 2234 [65.38%] G-Loss: 34.6368 D-Loss: 2.2718 Loss-g-fm: 11.7389 Loss-g-mel: 17.3221 Loss-g-dur: 1.2085 Loss-g-kl: 1.5595 lr: 0.0001 grad_norm_g: 731.8146 grad_norm_d: 32.5406
======> Epoch: 2234
Train Epoch: 2235 [61.54%] G-Loss: 34.3695 D-Loss: 2.0814 Loss-g-fm: 12.4019 Loss-g-mel: 16.5809 Loss-g-dur: 1.1972 Loss-g-kl: 1.4843 lr: 0.0001 grad_norm_g: 921.8334 grad_norm_d: 55.4375
======> Epoch: 2235
Train Epoch: 2236 [57.69%] G-Loss: 36.2050 D-Loss: 2.0322 Loss-g-fm: 12.6766 Loss-g-mel: 17.9574 Loss-g-dur: 1.2099 Loss-g-kl: 1.4614 lr: 0.0001 grad_norm_g: 904.2258 grad_norm_d: 48.3661
======> Epoch: 2236
Train Epoch: 2237 [53.85%] G-Loss: 34.5945 D-Loss: 2.2016 Loss-g-fm: 11.1957 Loss-g-mel: 17.7027 Loss-g-dur: 1.2163 Loss-g-kl: 1.7378 lr: 0.0001 grad_norm_g: 131.4209 grad_norm_d: 28.1167
======> Epoch: 2237
Train Epoch: 2238 [50.00%] G-Loss: 32.8542 D-Loss: 2.1350 Loss-g-fm: 10.9144 Loss-g-mel: 16.5614 Loss-g-dur: 1.1321 Loss-g-kl: 1.5128 lr: 0.0001 grad_norm_g: 795.4239 grad_norm_d: 39.6297
======> Epoch: 2238
Train Epoch: 2239 [46.15%] G-Loss: 35.3672 D-Loss: 2.1993 Loss-g-fm: 12.0315 Loss-g-mel: 17.4301 Loss-g-dur: 1.1791 Loss-g-kl: 1.7869 lr: 0.0001 grad_norm_g: 288.0590 grad_norm_d: 29.6369
======> Epoch: 2239
Train Epoch: 2240 [42.31%] G-Loss: 36.1963 D-Loss: 2.1085 Loss-g-fm: 12.2146 Loss-g-mel: 17.9868 Loss-g-dur: 1.1965 Loss-g-kl: 1.6724 lr: 0.0001 grad_norm_g: 818.8210 grad_norm_d: 38.5325
======> Epoch: 2240
Train Epoch: 2241 [38.46%] G-Loss: 32.6858 D-Loss: 2.2074 Loss-g-fm: 10.6321 Loss-g-mel: 16.9141 Loss-g-dur: 1.1041 Loss-g-kl: 1.4236 lr: 0.0001 grad_norm_g: 802.7134 grad_norm_d: 58.0375
======> Epoch: 2241
Train Epoch: 2242 [34.62%] G-Loss: 34.0762 D-Loss: 2.2065 Loss-g-fm: 10.8743 Loss-g-mel: 17.3659 Loss-g-dur: 1.2797 Loss-g-kl: 1.5753 lr: 0.0001 grad_norm_g: 906.1632 grad_norm_d: 120.0321
======> Epoch: 2242
Train Epoch: 2243 [30.77%] G-Loss: 32.8245 D-Loss: 2.3747 Loss-g-fm: 10.4991 Loss-g-mel: 16.6054 Loss-g-dur: 1.1335 Loss-g-kl: 1.5400 lr: 0.0001 grad_norm_g: 636.5004 grad_norm_d: 99.1508
======> Epoch: 2243
Train Epoch: 2244 [26.92%] G-Loss: 30.6994 D-Loss: 2.2866 Loss-g-fm: 9.5860 Loss-g-mel: 16.4351 Loss-g-dur: 1.1368 Loss-g-kl: 1.4116 lr: 0.0001 grad_norm_g: 130.0140 grad_norm_d: 17.4255
======> Epoch: 2244
Train Epoch: 2245 [23.08%] G-Loss: 36.3347 D-Loss: 2.0871 Loss-g-fm: 12.3778 Loss-g-mel: 18.2094 Loss-g-dur: 1.2106 Loss-g-kl: 1.8713 lr: 0.0001 grad_norm_g: 826.4090 grad_norm_d: 21.4516
======> Epoch: 2245
Train Epoch: 2246 [19.23%] G-Loss: 31.3338 D-Loss: 2.1539 Loss-g-fm: 9.5877 Loss-g-mel: 16.4718 Loss-g-dur: 1.0963 Loss-g-kl: 1.3521 lr: 0.0001 grad_norm_g: 611.1822 grad_norm_d: 26.7367
======> Epoch: 2246
Train Epoch: 2247 [15.38%] G-Loss: 33.9441 D-Loss: 2.2150 Loss-g-fm: 10.9920 Loss-g-mel: 17.1857 Loss-g-dur: 1.1333 Loss-g-kl: 1.4138 lr: 0.0001 grad_norm_g: 989.2830 grad_norm_d: 50.6213
======> Epoch: 2247
Train Epoch: 2248 [11.54%] G-Loss: 33.6203 D-Loss: 2.0003 Loss-g-fm: 11.3363 Loss-g-mel: 17.0850 Loss-g-dur: 1.1306 Loss-g-kl: 1.4208 lr: 0.0001 grad_norm_g: 996.5713 grad_norm_d: 65.8145
======> Epoch: 2248
Train Epoch: 2249 [7.69%] G-Loss: 36.0132 D-Loss: 2.2901 Loss-g-fm: 12.0523 Loss-g-mel: 18.0379 Loss-g-dur: 1.1861 Loss-g-kl: 1.7526 lr: 0.0001 grad_norm_g: 134.9403 grad_norm_d: 23.9694
======> Epoch: 2249
Train Epoch: 2250 [3.85%] G-Loss: 35.5222 D-Loss: 2.0829 Loss-g-fm: 12.1282 Loss-g-mel: 17.8781 Loss-g-dur: 1.1513 Loss-g-kl: 1.6670 lr: 0.0001 grad_norm_g: 452.5840 grad_norm_d: 7.6834
======> Epoch: 2250
Train Epoch: 2251 [0.00%] G-Loss: 31.9964 D-Loss: 2.2057 Loss-g-fm: 10.3227 Loss-g-mel: 16.8011 Loss-g-dur: 1.1393 Loss-g-kl: 1.4382 lr: 0.0001 grad_norm_g: 732.8834 grad_norm_d: 47.7600
Train Epoch: 2251 [96.15%] G-Loss: 27.0972 D-Loss: 2.1464 Loss-g-fm: 8.6570 Loss-g-mel: 13.1342 Loss-g-dur: 0.9575 Loss-g-kl: 1.4497 lr: 0.0001 grad_norm_g: 921.7136 grad_norm_d: 52.0364
======> Epoch: 2251
Train Epoch: 2252 [92.31%] G-Loss: 33.3257 D-Loss: 1.9816 Loss-g-fm: 11.5145 Loss-g-mel: 16.3962 Loss-g-dur: 1.1254 Loss-g-kl: 1.3853 lr: 0.0001 grad_norm_g: 534.4841 grad_norm_d: 71.8177
======> Epoch: 2252
Train Epoch: 2253 [88.46%] G-Loss: 34.6333 D-Loss: 2.1961 Loss-g-fm: 11.3673 Loss-g-mel: 17.4668 Loss-g-dur: 1.2956 Loss-g-kl: 1.6586 lr: 0.0001 grad_norm_g: 436.8326 grad_norm_d: 18.3525
======> Epoch: 2253
Train Epoch: 2254 [84.62%] G-Loss: 33.6576 D-Loss: 2.1404 Loss-g-fm: 11.0245 Loss-g-mel: 17.4107 Loss-g-dur: 1.1401 Loss-g-kl: 1.2729 lr: 0.0001 grad_norm_g: 706.5054 grad_norm_d: 103.6116
======> Epoch: 2254
Train Epoch: 2255 [80.77%] G-Loss: 32.8024 D-Loss: 2.0802 Loss-g-fm: 10.3726 Loss-g-mel: 16.7594 Loss-g-dur: 1.1395 Loss-g-kl: 1.5245 lr: 0.0001 grad_norm_g: 566.9850 grad_norm_d: 7.3493
======> Epoch: 2255
Train Epoch: 2256 [76.92%] G-Loss: 27.4638 D-Loss: 2.1916 Loss-g-fm: 8.9200 Loss-g-mel: 13.4020 Loss-g-dur: 0.9370 Loss-g-kl: 1.3689 lr: 0.0001 grad_norm_g: 588.0194 grad_norm_d: 62.3858
======> Epoch: 2256
Train Epoch: 2257 [73.08%] G-Loss: 33.0441 D-Loss: 2.1326 Loss-g-fm: 10.9259 Loss-g-mel: 16.7439 Loss-g-dur: 1.1136 Loss-g-kl: 1.5001 lr: 0.0001 grad_norm_g: 793.5261 grad_norm_d: 35.2986
======> Epoch: 2257
Train Epoch: 2258 [69.23%] G-Loss: 33.5959 D-Loss: 2.2952 Loss-g-fm: 11.2229 Loss-g-mel: 16.9186 Loss-g-dur: 1.1314 Loss-g-kl: 1.5517 lr: 0.0001 grad_norm_g: 794.3214 grad_norm_d: 58.2940
======> Epoch: 2258
Train Epoch: 2259 [65.38%] G-Loss: 32.8767 D-Loss: 2.2094 Loss-g-fm: 10.8564 Loss-g-mel: 16.9786 Loss-g-dur: 1.0899 Loss-g-kl: 1.4182 lr: 0.0001 grad_norm_g: 290.0562 grad_norm_d: 8.2840
======> Epoch: 2259
Train Epoch: 2260 [61.54%] G-Loss: 35.2928 D-Loss: 2.1377 Loss-g-fm: 11.8447 Loss-g-mel: 18.2844 Loss-g-dur: 1.1944 Loss-g-kl: 1.5136 lr: 0.0001 grad_norm_g: 103.2406 grad_norm_d: 36.8324
======> Epoch: 2260
Train Epoch: 2261 [57.69%] G-Loss: 33.7408 D-Loss: 2.1450 Loss-g-fm: 11.5841 Loss-g-mel: 17.1154 Loss-g-dur: 1.1589 Loss-g-kl: 1.2980 lr: 0.0001 grad_norm_g: 719.3491 grad_norm_d: 43.9745
======> Epoch: 2261
Train Epoch: 2262 [53.85%] G-Loss: 33.3425 D-Loss: 2.1744 Loss-g-fm: 11.2254 Loss-g-mel: 17.0473 Loss-g-dur: 1.1186 Loss-g-kl: 1.3992 lr: 0.0001 grad_norm_g: 854.3626 grad_norm_d: 49.3323
======> Epoch: 2262
Train Epoch: 2263 [50.00%] G-Loss: 32.4630 D-Loss: 2.1710 Loss-g-fm: 10.6258 Loss-g-mel: 16.6281 Loss-g-dur: 1.0958 Loss-g-kl: 1.6358 lr: 0.0001 grad_norm_g: 492.1768 grad_norm_d: 35.0189
======> Epoch: 2263
Train Epoch: 2264 [46.15%] G-Loss: 32.9026 D-Loss: 2.2502 Loss-g-fm: 10.4261 Loss-g-mel: 16.9442 Loss-g-dur: 1.1192 Loss-g-kl: 1.5322 lr: 0.0001 grad_norm_g: 418.9818 grad_norm_d: 107.1760
======> Epoch: 2264
Train Epoch: 2265 [42.31%] G-Loss: 36.3864 D-Loss: 2.0698 Loss-g-fm: 13.2629 Loss-g-mel: 17.7050 Loss-g-dur: 1.1505 Loss-g-kl: 1.7159 lr: 0.0001 grad_norm_g: 778.3667 grad_norm_d: 54.7033
======> Epoch: 2265
Train Epoch: 2266 [38.46%] G-Loss: 32.1599 D-Loss: 2.3417 Loss-g-fm: 10.1754 Loss-g-mel: 16.9471 Loss-g-dur: 1.1175 Loss-g-kl: 1.5050 lr: 0.0001 grad_norm_g: 602.0610 grad_norm_d: 89.7327
======> Epoch: 2266
Train Epoch: 2267 [34.62%] G-Loss: 35.9766 D-Loss: 2.0903 Loss-g-fm: 12.6160 Loss-g-mel: 17.9993 Loss-g-dur: 1.1820 Loss-g-kl: 1.4514 lr: 0.0001 grad_norm_g: 227.4988 grad_norm_d: 34.4887
======> Epoch: 2267
Train Epoch: 2268 [30.77%] G-Loss: 34.3815 D-Loss: 2.1203 Loss-g-fm: 11.3067 Loss-g-mel: 17.3756 Loss-g-dur: 1.2380 Loss-g-kl: 1.7149 lr: 0.0001 grad_norm_g: 230.6076 grad_norm_d: 10.9457
======> Epoch: 2268
Train Epoch: 2269 [26.92%] G-Loss: 32.2169 D-Loss: 2.0793 Loss-g-fm: 10.1950 Loss-g-mel: 16.6752 Loss-g-dur: 1.1412 Loss-g-kl: 1.4740 lr: 0.0001 grad_norm_g: 887.1259 grad_norm_d: 47.7414
======> Epoch: 2269
Train Epoch: 2270 [23.08%] G-Loss: 35.7995 D-Loss: 2.1167 Loss-g-fm: 12.2490 Loss-g-mel: 17.6816 Loss-g-dur: 1.4046 Loss-g-kl: 1.3657 lr: 0.0001 grad_norm_g: 210.1886 grad_norm_d: 20.8417
======> Epoch: 2270
Train Epoch: 2271 [19.23%] G-Loss: 35.6196 D-Loss: 1.9896 Loss-g-fm: 12.0203 Loss-g-mel: 17.5936 Loss-g-dur: 1.2039 Loss-g-kl: 1.7359 lr: 0.0001 grad_norm_g: 572.9151 grad_norm_d: 68.6223
======> Epoch: 2271
Train Epoch: 2272 [15.38%] G-Loss: 19.6808 D-Loss: 2.2151 Loss-g-fm: 6.5497 Loss-g-mel: 8.4625 Loss-g-dur: 0.6185 Loss-g-kl: 1.4937 lr: 0.0001 grad_norm_g: 1362.9698 grad_norm_d: 94.8752
======> Epoch: 2272
Train Epoch: 2273 [11.54%] G-Loss: 33.0272 D-Loss: 2.1573 Loss-g-fm: 11.0010 Loss-g-mel: 16.4340 Loss-g-dur: 1.1296 Loss-g-kl: 1.2819 lr: 0.0001 grad_norm_g: 372.3366 grad_norm_d: 48.6127
======> Epoch: 2273
Train Epoch: 2274 [7.69%] G-Loss: 32.3691 D-Loss: 2.2848 Loss-g-fm: 10.7103 Loss-g-mel: 16.6312 Loss-g-dur: 1.1043 Loss-g-kl: 1.4816 lr: 0.0001 grad_norm_g: 647.7116 grad_norm_d: 69.5716
======> Epoch: 2274
Train Epoch: 2275 [3.85%] G-Loss: 35.9098 D-Loss: 2.0950 Loss-g-fm: 12.0814 Loss-g-mel: 17.8239 Loss-g-dur: 1.1301 Loss-g-kl: 1.6684 lr: 0.0001 grad_norm_g: 279.3633 grad_norm_d: 9.5813
======> Epoch: 2275
Train Epoch: 2276 [0.00%] G-Loss: 34.1247 D-Loss: 2.1653 Loss-g-fm: 11.5505 Loss-g-mel: 17.1753 Loss-g-dur: 1.1666 Loss-g-kl: 1.6195 lr: 0.0001 grad_norm_g: 645.5953 grad_norm_d: 59.7596
Train Epoch: 2276 [96.15%] G-Loss: 32.9617 D-Loss: 2.0892 Loss-g-fm: 10.7776 Loss-g-mel: 16.9081 Loss-g-dur: 1.1304 Loss-g-kl: 1.5334 lr: 0.0001 grad_norm_g: 758.8912 grad_norm_d: 40.0234
======> Epoch: 2276
Train Epoch: 2277 [92.31%] G-Loss: 34.0751 D-Loss: 2.0633 Loss-g-fm: 11.3410 Loss-g-mel: 17.2114 Loss-g-dur: 1.2163 Loss-g-kl: 1.5632 lr: 0.0001 grad_norm_g: 447.8776 grad_norm_d: 5.4052
======> Epoch: 2277
Train Epoch: 2278 [88.46%] G-Loss: 34.3796 D-Loss: 2.1337 Loss-g-fm: 11.6798 Loss-g-mel: 17.1913 Loss-g-dur: 1.1776 Loss-g-kl: 1.5519 lr: 0.0001 grad_norm_g: 856.9046 grad_norm_d: 54.9509
======> Epoch: 2278
Train Epoch: 2279 [84.62%] G-Loss: 35.2804 D-Loss: 2.1259 Loss-g-fm: 11.8974 Loss-g-mel: 17.5491 Loss-g-dur: 1.2326 Loss-g-kl: 1.8220 lr: 0.0001 grad_norm_g: 318.2838 grad_norm_d: 35.3141
======> Epoch: 2279
Train Epoch: 2280 [80.77%] G-Loss: 33.4407 D-Loss: 2.2458 Loss-g-fm: 11.4373 Loss-g-mel: 16.7474 Loss-g-dur: 1.1155 Loss-g-kl: 1.4071 lr: 0.0001 grad_norm_g: 296.1843 grad_norm_d: 37.8833
======> Epoch: 2280
Train Epoch: 2281 [76.92%] G-Loss: 27.8167 D-Loss: 2.0988 Loss-g-fm: 9.6841 Loss-g-mel: 13.2561 Loss-g-dur: 0.9646 Loss-g-kl: 1.3984 lr: 0.0001 grad_norm_g: 486.4117 grad_norm_d: 10.2097
======> Epoch: 2281
Train Epoch: 2282 [73.08%] G-Loss: 19.8169 D-Loss: 2.2716 Loss-g-fm: 6.2933 Loss-g-mel: 8.7647 Loss-g-dur: 0.7317 Loss-g-kl: 1.4364 lr: 0.0001 grad_norm_g: 1356.3022 grad_norm_d: 61.3466
======> Epoch: 2282
Train Epoch: 2283 [69.23%] G-Loss: 37.2835 D-Loss: 2.0939 Loss-g-fm: 13.4172 Loss-g-mel: 17.8351 Loss-g-dur: 1.3373 Loss-g-kl: 1.7951 lr: 0.0001 grad_norm_g: 972.3510 grad_norm_d: 41.2493
======> Epoch: 2283
Train Epoch: 2284 [65.38%] G-Loss: 30.6354 D-Loss: 2.2798 Loss-g-fm: 11.3050 Loss-g-mel: 13.8910 Loss-g-dur: 0.9664 Loss-g-kl: 1.6286 lr: 0.0001 grad_norm_g: 1113.5795 grad_norm_d: 89.3704
======> Epoch: 2284
Train Epoch: 2285 [61.54%] G-Loss: 35.7955 D-Loss: 2.1236 Loss-g-fm: 12.2590 Loss-g-mel: 17.2670 Loss-g-dur: 1.2500 Loss-g-kl: 1.8514 lr: 0.0001 grad_norm_g: 771.7792 grad_norm_d: 49.0100
======> Epoch: 2285
Train Epoch: 2286 [57.69%] G-Loss: 36.1923 D-Loss: 2.1907 Loss-g-fm: 12.1532 Loss-g-mel: 18.3081 Loss-g-dur: 1.2540 Loss-g-kl: 1.5674 lr: 0.0001 grad_norm_g: 147.1268 grad_norm_d: 12.7248
======> Epoch: 2286
Train Epoch: 2287 [53.85%] G-Loss: 31.7516 D-Loss: 2.2382 Loss-g-fm: 10.1385 Loss-g-mel: 16.4101 Loss-g-dur: 1.0993 Loss-g-kl: 1.4542 lr: 0.0001 grad_norm_g: 725.8663 grad_norm_d: 54.7675
======> Epoch: 2287
Train Epoch: 2288 [50.00%] G-Loss: 31.4985 D-Loss: 2.3158 Loss-g-fm: 9.8825 Loss-g-mel: 16.5052 Loss-g-dur: 1.0802 Loss-g-kl: 1.3102 lr: 0.0001 grad_norm_g: 152.7003 grad_norm_d: 5.1445
======> Epoch: 2288
Train Epoch: 2289 [46.15%] G-Loss: 34.3754 D-Loss: 2.1530 Loss-g-fm: 11.6796 Loss-g-mel: 16.7278 Loss-g-dur: 1.1152 Loss-g-kl: 1.7390 lr: 0.0001 grad_norm_g: 78.6601 grad_norm_d: 15.9615
======> Epoch: 2289
Train Epoch: 2290 [42.31%] G-Loss: 34.8529 D-Loss: 1.9971 Loss-g-fm: 11.8594 Loss-g-mel: 17.4835 Loss-g-dur: 1.1858 Loss-g-kl: 1.5164 lr: 0.0001 grad_norm_g: 858.4718 grad_norm_d: 52.0958
======> Epoch: 2290
Train Epoch: 2291 [38.46%] G-Loss: 32.3675 D-Loss: 2.1052 Loss-g-fm: 10.5138 Loss-g-mel: 16.6509 Loss-g-dur: 1.0832 Loss-g-kl: 1.1756 lr: 0.0001 grad_norm_g: 860.8580 grad_norm_d: 91.2730
======> Epoch: 2291
Train Epoch: 2292 [34.62%] G-Loss: 33.7140 D-Loss: 2.1317 Loss-g-fm: 11.6187 Loss-g-mel: 17.0049 Loss-g-dur: 1.0808 Loss-g-kl: 1.4485 lr: 0.0001 grad_norm_g: 237.7440 grad_norm_d: 8.3415
======> Epoch: 2292
terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f45045144d7 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f45044de36b in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f45045b0b58 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x10b4b9e (0x7f4505722b9e in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4d59a6 (0x7f455b4649a6 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7f45044f9e77 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7f45044f269e in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f45044f27b9 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x75ab08 (0x7f455b6e9b08 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f455b6e9e95 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x122c4b (0x5606fd33ec4b in /home/rex/anaconda3/envs/torch2/bin/python)
frame #11: <unknown function> + 0x13d6ea (0x5606fd3596ea in /home/rex/anaconda3/envs/torch2/bin/python)
frame #12: <unknown function> + 0x20db0f (0x5606fd429b0f in /home/rex/anaconda3/envs/torch2/bin/python)
frame #13: _PyEval_EvalFrameDefault + 0x67db (0x5606fd35270b in /home/rex/anaconda3/envs/torch2/bin/python)
frame #14: <unknown function> + 0x1407c2 (0x5606fd35c7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #15: _PyEval_EvalFrameDefault + 0x3e2 (0x5606fd34c312 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #16: <unknown function> + 0x1407c2 (0x5606fd35c7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #17: _PyEval_EvalFrameDefault + 0x37cb (0x5606fd34f6fb in /home/rex/anaconda3/envs/torch2/bin/python)
frame #18: <unknown function> + 0x1407c2 (0x5606fd35c7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #19: _PyEval_EvalFrameDefault + 0x37cb (0x5606fd34f6fb in /home/rex/anaconda3/envs/torch2/bin/python)
frame #20: <unknown function> + 0x1407c2 (0x5606fd35c7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #21: _PyEval_EvalFrameDefault + 0x696 (0x5606fd34c5c6 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #22: <unknown function> + 0x12f094 (0x5606fd34b094 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #23: _PyFunction_Vectorcall + 0xd9 (0x5606fd35c519 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x696 (0x5606fd34c5c6 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #25: <unknown function> + 0x1407c2 (0x5606fd35c7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x3e2 (0x5606fd34c312 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #27: <unknown function> + 0x12f094 (0x5606fd34b094 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #28: _PyFunction_Vectorcall + 0xd9 (0x5606fd35c519 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x11f8 (0x5606fd34d128 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #30: <unknown function> + 0x12f094 (0x5606fd34b094 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #31: _PyEval_EvalCodeWithName + 0x48 (0x5606fd34ad68 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #32: PyEval_EvalCodeEx + 0x39 (0x5606fd34ad19 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #33: PyEval_EvalCode + 0x1b (0x5606fd3f807b in /home/rex/anaconda3/envs/torch2/bin/python)
frame #34: <unknown function> + 0x208fca (0x5606fd424fca in /home/rex/anaconda3/envs/torch2/bin/python)
frame #35: <unknown function> + 0x205353 (0x5606fd421353 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #36: PyRun_StringFlags + 0x9a (0x5606fd41916a in /home/rex/anaconda3/envs/torch2/bin/python)
frame #37: PyRun_SimpleStringFlags + 0x3c (0x5606fd41905c in /home/rex/anaconda3/envs/torch2/bin/python)
frame #38: Py_RunMain + 0x267 (0x5606fd418297 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #39: Py_BytesMain + 0x37 (0x5606fd3ebf07 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #40: __libc_start_main + 0xf3 (0x7f457d87c083 in /lib/x86_64-linux-gnu/libc.so.6)
frame #41: <unknown function> + 0x1cfe01 (0x5606fd3ebe01 in /home/rex/anaconda3/envs/torch2/bin/python)

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f63d9c224d7 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f63d9bec36b in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f63d9cbeb58 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x10b4b9e (0x7f63dae30b9e in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4d59a6 (0x7f6430b729a6 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7f63d9c07e77 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7f63d9c0069e in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f63d9c007b9 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x75ab08 (0x7f6430df7b08 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x305 (0x7f6430df7e95 in /home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x122c4b (0x562539e8cc4b in /home/rex/anaconda3/envs/torch2/bin/python)
frame #11: <unknown function> + 0x13d6ea (0x562539ea76ea in /home/rex/anaconda3/envs/torch2/bin/python)
frame #12: <unknown function> + 0x20db0f (0x562539f77b0f in /home/rex/anaconda3/envs/torch2/bin/python)
frame #13: _PyEval_EvalFrameDefault + 0x67db (0x562539ea070b in /home/rex/anaconda3/envs/torch2/bin/python)
frame #14: <unknown function> + 0x1407c2 (0x562539eaa7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #15: _PyEval_EvalFrameDefault + 0x3e2 (0x562539e9a312 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #16: <unknown function> + 0x1407c2 (0x562539eaa7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #17: _PyEval_EvalFrameDefault + 0x37cb (0x562539e9d6fb in /home/rex/anaconda3/envs/torch2/bin/python)
frame #18: <unknown function> + 0x1407c2 (0x562539eaa7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #19: _PyEval_EvalFrameDefault + 0x37cb (0x562539e9d6fb in /home/rex/anaconda3/envs/torch2/bin/python)
frame #20: <unknown function> + 0x1407c2 (0x562539eaa7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #21: _PyEval_EvalFrameDefault + 0x696 (0x562539e9a5c6 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #22: <unknown function> + 0x12f094 (0x562539e99094 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #23: _PyFunction_Vectorcall + 0xd9 (0x562539eaa519 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x696 (0x562539e9a5c6 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #25: <unknown function> + 0x1407c2 (0x562539eaa7c2 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x3e2 (0x562539e9a312 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #27: <unknown function> + 0x12f094 (0x562539e99094 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #28: _PyFunction_Vectorcall + 0xd9 (0x562539eaa519 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x11f8 (0x562539e9b128 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #30: <unknown function> + 0x12f094 (0x562539e99094 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #31: _PyEval_EvalCodeWithName + 0x48 (0x562539e98d68 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #32: PyEval_EvalCodeEx + 0x39 (0x562539e98d19 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #33: PyEval_EvalCode + 0x1b (0x562539f4607b in /home/rex/anaconda3/envs/torch2/bin/python)
frame #34: <unknown function> + 0x208fca (0x562539f72fca in /home/rex/anaconda3/envs/torch2/bin/python)
frame #35: <unknown function> + 0x205353 (0x562539f6f353 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #36: PyRun_StringFlags + 0x9a (0x562539f6716a in /home/rex/anaconda3/envs/torch2/bin/python)
frame #37: PyRun_SimpleStringFlags + 0x3c (0x562539f6705c in /home/rex/anaconda3/envs/torch2/bin/python)
frame #38: Py_RunMain + 0x267 (0x562539f66297 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #39: Py_BytesMain + 0x37 (0x562539f39f07 in /home/rex/anaconda3/envs/torch2/bin/python)
frame #40: __libc_start_main + 0xf3 (0x7f6452f8a083 in /lib/x86_64-linux-gnu/libc.so.6)
frame #41: <unknown function> + 0x1cfe01 (0x562539f39e01 in /home/rex/anaconda3/envs/torch2/bin/python)

nohup: ignoring input
rank =  0
**************************************** /ZFS4T/tts/data/VITS/model_saved/G_58000.pth ****************************************
**************************************** /ZFS4T/tts/data/VITS/model_saved/D_58000.pth ****************************************
rank =  1
**************************************** /ZFS4T/tts/data/VITS/model_saved/G_58000.pth ****************************************
**************************************** /ZFS4T/tts/data/VITS/model_saved/D_58000.pth ****************************************
/mnt/disk3/huangyao/tts/vits/mel_processing.py:57: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/mnt/disk3/huangyao/tts/vits/mel_processing.py:57: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/mnt/disk3/huangyao/tts/vits/mel_processing.py:75: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/mnt/disk3/huangyao/tts/vits/mel_processing.py:75: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error
  mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.
Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.
Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/functional.py:641: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 9, 96], strides() = [17760, 96, 1]
bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 9, 96], strides() = [17376, 96, 1]
bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Train Epoch: 2231 [76.92%] G-Loss: 35.1148 D-Loss: 2.0911 Loss-g-fm: 12.0707 Loss-g-mel: 17.3632 Loss-g-dur: 1.1786 Loss-g-kl: 1.5646 lr: 0.00014846 grad_norm_g: 915.5983 grad_norm_d: 71.8350
Saving model and optimizer state at iteration 2231 to /ZFS4T/tts/data/VITS/model_saved/G_58000.pth
Saving model and optimizer state at iteration 2231 to /ZFS4T/tts/data/VITS/model_saved/D_58000.pth
======> Epoch: 2231
Train Epoch: 2232 [73.08%] G-Loss: 33.7992 D-Loss: 2.2357 Loss-g-fm: 10.6953 Loss-g-mel: 17.5913 Loss-g-dur: 1.1670 Loss-g-kl: 1.5533 lr: 0.00014844 grad_norm_g: 222.1795 grad_norm_d: 23.4090
======> Epoch: 2232
Train Epoch: 2233 [69.23%] G-Loss: 34.9008 D-Loss: 2.0973 Loss-g-fm: 12.0253 Loss-g-mel: 17.3178 Loss-g-dur: 1.1084 Loss-g-kl: 1.4312 lr: 0.00014842 grad_norm_g: 535.8136 grad_norm_d: 25.6895
======> Epoch: 2233
Train Epoch: 2234 [65.38%] G-Loss: 34.3778 D-Loss: 2.0720 Loss-g-fm: 11.6216 Loss-g-mel: 17.4675 Loss-g-dur: 1.2109 Loss-g-kl: 1.5915 lr: 0.00014840 grad_norm_g: 178.0756 grad_norm_d: 35.1051
======> Epoch: 2234
Train Epoch: 2235 [61.54%] G-Loss: 32.4561 D-Loss: 2.2615 Loss-g-fm: 10.6008 Loss-g-mel: 16.5017 Loss-g-dur: 1.1983 Loss-g-kl: 1.5630 lr: 0.00014838 grad_norm_g: 200.1086 grad_norm_d: 30.2554
======> Epoch: 2235
Train Epoch: 2236 [57.69%] G-Loss: 36.0348 D-Loss: 2.1042 Loss-g-fm: 12.6009 Loss-g-mel: 17.6253 Loss-g-dur: 1.1975 Loss-g-kl: 1.6529 lr: 0.00014836 grad_norm_g: 1072.5633 grad_norm_d: 67.2256
======> Epoch: 2236
Train Epoch: 2237 [53.85%] G-Loss: 35.5358 D-Loss: 2.1285 Loss-g-fm: 11.8343 Loss-g-mel: 17.7259 Loss-g-dur: 1.2010 Loss-g-kl: 1.7364 lr: 0.00014835 grad_norm_g: 688.1828 grad_norm_d: 7.1148
======> Epoch: 2237
Train Epoch: 2238 [50.00%] G-Loss: 32.2147 D-Loss: 2.2956 Loss-g-fm: 10.9264 Loss-g-mel: 15.9361 Loss-g-dur: 1.1218 Loss-g-kl: 1.4160 lr: 0.00014833 grad_norm_g: 868.1665 grad_norm_d: 79.7077
======> Epoch: 2238
Train Epoch: 2239 [46.15%] G-Loss: 36.8086 D-Loss: 2.2413 Loss-g-fm: 13.0930 Loss-g-mel: 17.5155 Loss-g-dur: 1.2160 Loss-g-kl: 1.8401 lr: 0.00014831 grad_norm_g: 571.3804 grad_norm_d: 65.1296
======> Epoch: 2239
Train Epoch: 2240 [42.31%] G-Loss: 34.7685 D-Loss: 2.2107 Loss-g-fm: 11.1166 Loss-g-mel: 17.6876 Loss-g-dur: 1.2375 Loss-g-kl: 1.7339 lr: 0.00014829 grad_norm_g: 35.7173 grad_norm_d: 22.9798
======> Epoch: 2240
Train Epoch: 2241 [38.46%] G-Loss: 32.4178 D-Loss: 2.1833 Loss-g-fm: 10.3066 Loss-g-mel: 16.9368 Loss-g-dur: 1.0900 Loss-g-kl: 1.5933 lr: 0.00014827 grad_norm_g: 457.2338 grad_norm_d: 6.2388
======> Epoch: 2241
Train Epoch: 2242 [34.62%] G-Loss: 35.1272 D-Loss: 2.1068 Loss-g-fm: 11.7474 Loss-g-mel: 17.5513 Loss-g-dur: 1.1606 Loss-g-kl: 1.6187 lr: 0.00014825 grad_norm_g: 1135.4426 grad_norm_d: 74.1531
======> Epoch: 2242
Train Epoch: 2243 [30.77%] G-Loss: 33.7410 D-Loss: 2.1695 Loss-g-fm: 10.7119 Loss-g-mel: 17.2979 Loss-g-dur: 1.1399 Loss-g-kl: 1.4753 lr: 0.00014823 grad_norm_g: 122.6731 grad_norm_d: 52.0544
======> Epoch: 2243
Train Epoch: 2244 [26.92%] G-Loss: 31.8894 D-Loss: 2.3588 Loss-g-fm: 10.5113 Loss-g-mel: 16.4812 Loss-g-dur: 1.1419 Loss-g-kl: 1.4170 lr: 0.00014822 grad_norm_g: 413.6195 grad_norm_d: 91.9771
======> Epoch: 2244
Train Epoch: 2245 [23.08%] G-Loss: 35.8726 D-Loss: 2.1431 Loss-g-fm: 12.1376 Loss-g-mel: 18.0297 Loss-g-dur: 1.1956 Loss-g-kl: 1.7381 lr: 0.00014820 grad_norm_g: 464.1402 grad_norm_d: 42.3008
======> Epoch: 2245
Train Epoch: 2246 [19.23%] G-Loss: 32.7142 D-Loss: 2.1315 Loss-g-fm: 10.7132 Loss-g-mel: 16.8388 Loss-g-dur: 1.0826 Loss-g-kl: 1.4326 lr: 0.00014818 grad_norm_g: 43.0266 grad_norm_d: 16.3532
======> Epoch: 2246
Train Epoch: 2247 [15.38%] G-Loss: 32.0251 D-Loss: 2.2700 Loss-g-fm: 9.4044 Loss-g-mel: 17.0884 Loss-g-dur: 1.1460 Loss-g-kl: 1.4724 lr: 0.00014816 grad_norm_g: 376.1860 grad_norm_d: 31.3035
======> Epoch: 2247
Train Epoch: 2248 [11.54%] G-Loss: 32.0225 D-Loss: 2.1811 Loss-g-fm: 10.8950 Loss-g-mel: 16.4111 Loss-g-dur: 1.1172 Loss-g-kl: 1.3823 lr: 0.00014814 grad_norm_g: 948.6942 grad_norm_d: 60.3664
======> Epoch: 2248
Train Epoch: 2249 [7.69%] G-Loss: 38.0419 D-Loss: 2.1809 Loss-g-fm: 13.6155 Loss-g-mel: 18.5862 Loss-g-dur: 1.1814 Loss-g-kl: 1.7764 lr: 0.00014812 grad_norm_g: 1003.5271 grad_norm_d: 52.8261
======> Epoch: 2249
Train Epoch: 2250 [3.85%] G-Loss: 36.0416 D-Loss: 2.1664 Loss-g-fm: 12.4405 Loss-g-mel: 18.2193 Loss-g-dur: 1.1908 Loss-g-kl: 1.5524 lr: 0.00014811 grad_norm_g: 340.3517 grad_norm_d: 23.6058
======> Epoch: 2250
Train Epoch: 2251 [0.00%] G-Loss: 32.8039 D-Loss: 2.1148 Loss-g-fm: 10.7655 Loss-g-mel: 16.7907 Loss-g-dur: 1.1346 Loss-g-kl: 1.6899 lr: 0.00014809 grad_norm_g: 830.6285 grad_norm_d: 80.4286
Train Epoch: 2251 [96.15%] G-Loss: 29.9704 D-Loss: 2.0951 Loss-g-fm: 10.7582 Loss-g-mel: 13.7762 Loss-g-dur: 0.9770 Loss-g-kl: 1.5141 lr: 0.00014809 grad_norm_g: 1123.8125 grad_norm_d: 70.6484
======> Epoch: 2251
Train Epoch: 2252 [92.31%] G-Loss: 32.6140 D-Loss: 2.2107 Loss-g-fm: 11.0226 Loss-g-mel: 16.2775 Loss-g-dur: 1.1277 Loss-g-kl: 1.4665 lr: 0.00014807 grad_norm_g: 795.4985 grad_norm_d: 78.9731
======> Epoch: 2252
Train Epoch: 2253 [88.46%] G-Loss: 34.8047 D-Loss: 2.1068 Loss-g-fm: 11.4543 Loss-g-mel: 17.4681 Loss-g-dur: 1.2802 Loss-g-kl: 1.7468 lr: 0.00014805 grad_norm_g: 192.5802 grad_norm_d: 48.9713
======> Epoch: 2253
Train Epoch: 2254 [84.62%] G-Loss: 33.9371 D-Loss: 2.1281 Loss-g-fm: 11.3745 Loss-g-mel: 17.2299 Loss-g-dur: 1.1255 Loss-g-kl: 1.5023 lr: 0.00014803 grad_norm_g: 311.5344 grad_norm_d: 9.6812
======> Epoch: 2254
Train Epoch: 2255 [80.77%] G-Loss: 32.8827 D-Loss: 2.2194 Loss-g-fm: 10.1745 Loss-g-mel: 16.9910 Loss-g-dur: 1.1440 Loss-g-kl: 1.4951 lr: 0.00014801 grad_norm_g: 760.0407 grad_norm_d: 34.5068
======> Epoch: 2255
Train Epoch: 2256 [76.92%] G-Loss: 28.7374 D-Loss: 2.0719 Loss-g-fm: 10.2980 Loss-g-mel: 13.5254 Loss-g-dur: 0.9539 Loss-g-kl: 1.3345 lr: 0.00014799 grad_norm_g: 928.4180 grad_norm_d: 55.9422
======> Epoch: 2256
Train Epoch: 2257 [73.08%] G-Loss: 33.9620 D-Loss: 2.0908 Loss-g-fm: 11.1435 Loss-g-mel: 17.1155 Loss-g-dur: 1.1060 Loss-g-kl: 1.6146 lr: 0.00014798 grad_norm_g: 904.5676 grad_norm_d: 83.5828
======> Epoch: 2257
Train Epoch: 2258 [69.23%] G-Loss: 33.5258 D-Loss: 2.2033 Loss-g-fm: 11.5600 Loss-g-mel: 16.6161 Loss-g-dur: 1.1030 Loss-g-kl: 1.5473 lr: 0.00014796 grad_norm_g: 909.0829 grad_norm_d: 56.6706
======> Epoch: 2258
Train Epoch: 2259 [65.38%] G-Loss: 33.1892 D-Loss: 2.0981 Loss-g-fm: 11.0289 Loss-g-mel: 16.6401 Loss-g-dur: 1.1029 Loss-g-kl: 1.5381 lr: 0.00014794 grad_norm_g: 756.2938 grad_norm_d: 37.1139
======> Epoch: 2259
Train Epoch: 2260 [61.54%] G-Loss: 36.4640 D-Loss: 2.1309 Loss-g-fm: 12.4764 Loss-g-mel: 18.3870 Loss-g-dur: 1.2246 Loss-g-kl: 1.6157 lr: 0.00014792 grad_norm_g: 205.5545 grad_norm_d: 8.9575
======> Epoch: 2260
Train Epoch: 2261 [57.69%] G-Loss: 33.4938 D-Loss: 2.1153 Loss-g-fm: 10.8964 Loss-g-mel: 17.1305 Loss-g-dur: 1.1139 Loss-g-kl: 1.5897 lr: 0.00014790 grad_norm_g: 869.6450 grad_norm_d: 54.2010
======> Epoch: 2261
Train Epoch: 2262 [53.85%] G-Loss: 35.3256 D-Loss: 2.0798 Loss-g-fm: 12.2276 Loss-g-mel: 17.4975 Loss-g-dur: 1.1516 Loss-g-kl: 1.6317 lr: 0.00014788 grad_norm_g: 849.6845 grad_norm_d: 50.0732
======> Epoch: 2262
Train Epoch: 2263 [50.00%] G-Loss: 33.8725 D-Loss: 2.1193 Loss-g-fm: 11.4253 Loss-g-mel: 16.9333 Loss-g-dur: 1.1655 Loss-g-kl: 1.5080 lr: 0.00014786 grad_norm_g: 631.3208 grad_norm_d: 27.9804
======> Epoch: 2263
Train Epoch: 2264 [46.15%] G-Loss: 32.4601 D-Loss: 2.1265 Loss-g-fm: 10.3321 Loss-g-mel: 16.8165 Loss-g-dur: 1.1277 Loss-g-kl: 1.5259 lr: 0.00014785 grad_norm_g: 745.8198 grad_norm_d: 29.1488
======> Epoch: 2264
Train Epoch: 2265 [42.31%] G-Loss: 37.7028 D-Loss: 2.0997 Loss-g-fm: 14.5218 Loss-g-mel: 17.7311 Loss-g-dur: 1.1504 Loss-g-kl: 1.6161 lr: 0.00014783 grad_norm_g: 1127.1672 grad_norm_d: 66.5003
======> Epoch: 2265
Train Epoch: 2266 [38.46%] G-Loss: 33.9432 D-Loss: 2.0555 Loss-g-fm: 11.7808 Loss-g-mel: 16.7871 Loss-g-dur: 1.1423 Loss-g-kl: 1.4012 lr: 0.00014781 grad_norm_g: 625.6561 grad_norm_d: 8.8250
======> Epoch: 2266
Train Epoch: 2267 [34.62%] G-Loss: 36.4096 D-Loss: 2.0622 Loss-g-fm: 12.6180 Loss-g-mel: 18.1430 Loss-g-dur: 1.1881 Loss-g-kl: 1.6922 lr: 0.00014779 grad_norm_g: 762.5842 grad_norm_d: 47.5656
======> Epoch: 2267
Train Epoch: 2268 [30.77%] G-Loss: 35.5136 D-Loss: 2.2020 Loss-g-fm: 11.5019 Loss-g-mel: 18.3579 Loss-g-dur: 1.1994 Loss-g-kl: 1.6046 lr: 0.00014777 grad_norm_g: 665.7535 grad_norm_d: 44.6058
======> Epoch: 2268
Train Epoch: 2269 [26.92%] G-Loss: 34.0346 D-Loss: 2.1929 Loss-g-fm: 11.8026 Loss-g-mel: 16.9903 Loss-g-dur: 1.1173 Loss-g-kl: 1.3758 lr: 0.00014775 grad_norm_g: 814.1969 grad_norm_d: 75.1883
======> Epoch: 2269
Train Epoch: 2270 [23.08%] G-Loss: 35.8854 D-Loss: 2.1153 Loss-g-fm: 12.4778 Loss-g-mel: 17.3048 Loss-g-dur: 1.1288 Loss-g-kl: 1.7442 lr: 0.00014774 grad_norm_g: 239.3779 grad_norm_d: 24.0213
======> Epoch: 2270
Train Epoch: 2271 [19.23%] G-Loss: 34.8896 D-Loss: 2.1373 Loss-g-fm: 11.7283 Loss-g-mel: 17.2364 Loss-g-dur: 1.2527 Loss-g-kl: 1.7684 lr: 0.00014772 grad_norm_g: 552.3985 grad_norm_d: 37.5380
======> Epoch: 2271
Train Epoch: 2272 [15.38%] G-Loss: 18.1724 D-Loss: 2.4038 Loss-g-fm: 5.4253 Loss-g-mel: 8.2806 Loss-g-dur: 0.6271 Loss-g-kl: 1.5121 lr: 0.00014770 grad_norm_g: 218.5066 grad_norm_d: 31.3678
======> Epoch: 2272
Train Epoch: 2273 [11.54%] G-Loss: 33.6401 D-Loss: 2.1677 Loss-g-fm: 11.0786 Loss-g-mel: 16.7119 Loss-g-dur: 1.0951 Loss-g-kl: 1.5793 lr: 0.00014768 grad_norm_g: 871.1801 grad_norm_d: 31.2422
======> Epoch: 2273
Train Epoch: 2274 [7.69%] G-Loss: 32.4458 D-Loss: 2.2336 Loss-g-fm: 10.6080 Loss-g-mel: 16.6256 Loss-g-dur: 1.1092 Loss-g-kl: 1.4787 lr: 0.00014766 grad_norm_g: 725.0282 grad_norm_d: 73.7733
======> Epoch: 2274
Train Epoch: 2275 [3.85%] G-Loss: 36.8429 D-Loss: 2.1557 Loss-g-fm: 12.5382 Loss-g-mel: 18.3192 Loss-g-dur: 1.1451 Loss-g-kl: 1.4414 lr: 0.00014764 grad_norm_g: 964.5005 grad_norm_d: 40.6319
======> Epoch: 2275
Train Epoch: 2276 [0.00%] G-Loss: 33.9573 D-Loss: 2.1833 Loss-g-fm: 11.5886 Loss-g-mel: 16.9556 Loss-g-dur: 1.1926 Loss-g-kl: 1.5445 lr: 0.00014762 grad_norm_g: 952.7855 grad_norm_d: 64.5868
Train Epoch: 2276 [96.15%] G-Loss: 34.3626 D-Loss: 2.2868 Loss-g-fm: 11.8646 Loss-g-mel: 17.2384 Loss-g-dur: 1.1218 Loss-g-kl: 1.5915 lr: 0.00014762 grad_norm_g: 696.0530 grad_norm_d: 61.1699
======> Epoch: 2276
Train Epoch: 2277 [92.31%] G-Loss: 33.3246 D-Loss: 2.2150 Loss-g-fm: 10.5147 Loss-g-mel: 17.0991 Loss-g-dur: 1.2364 Loss-g-kl: 1.6310 lr: 0.00014761 grad_norm_g: 721.0351 grad_norm_d: 46.4191
======> Epoch: 2277
Train Epoch: 2278 [88.46%] G-Loss: 36.5899 D-Loss: 2.1016 Loss-g-fm: 12.9144 Loss-g-mel: 17.7237 Loss-g-dur: 1.2281 Loss-g-kl: 1.6967 lr: 0.00014759 grad_norm_g: 894.5468 grad_norm_d: 38.4588
======> Epoch: 2278
Train Epoch: 2279 [84.62%] G-Loss: 34.4898 D-Loss: 2.2101 Loss-g-fm: 11.4460 Loss-g-mel: 17.5704 Loss-g-dur: 1.2491 Loss-g-kl: 1.6560 lr: 0.00014757 grad_norm_g: 851.6878 grad_norm_d: 47.3137
======> Epoch: 2279
Train Epoch: 2280 [80.77%] G-Loss: 34.6827 D-Loss: 1.9950 Loss-g-fm: 12.2126 Loss-g-mel: 16.8870 Loss-g-dur: 1.1250 Loss-g-kl: 1.4615 lr: 0.00014755 grad_norm_g: 926.2446 grad_norm_d: 67.1834
======> Epoch: 2280
Train Epoch: 2281 [76.92%] G-Loss: 27.5435 D-Loss: 2.1574 Loss-g-fm: 9.4821 Loss-g-mel: 12.9569 Loss-g-dur: 0.9879 Loss-g-kl: 1.4273 lr: 0.00014753 grad_norm_g: 922.8398 grad_norm_d: 60.8028
======> Epoch: 2281
Train Epoch: 2282 [73.08%] G-Loss: 17.9478 D-Loss: 2.4883 Loss-g-fm: 5.4831 Loss-g-mel: 8.1355 Loss-g-dur: 0.6324 Loss-g-kl: 1.3833 lr: 0.00014751 grad_norm_g: 583.0095 grad_norm_d: 45.7068
======> Epoch: 2282
Train Epoch: 2283 [69.23%] G-Loss: 36.7796 D-Loss: 2.0785 Loss-g-fm: 12.9943 Loss-g-mel: 17.8402 Loss-g-dur: 1.2648 Loss-g-kl: 1.7104 lr: 0.00014750 grad_norm_g: 611.1143 grad_norm_d: 20.6057
======> Epoch: 2283
Train Epoch: 2284 [65.38%] G-Loss: 28.8454 D-Loss: 2.2353 Loss-g-fm: 9.8224 Loss-g-mel: 13.7792 Loss-g-dur: 0.9559 Loss-g-kl: 1.7029 lr: 0.00014748 grad_norm_g: 859.3016 grad_norm_d: 58.4939
======> Epoch: 2284
Train Epoch: 2285 [61.54%] G-Loss: 36.9703 D-Loss: 2.1126 Loss-g-fm: 13.4394 Loss-g-mel: 17.7796 Loss-g-dur: 1.1974 Loss-g-kl: 1.6605 lr: 0.00014746 grad_norm_g: 215.0699 grad_norm_d: 19.9833
======> Epoch: 2285
Train Epoch: 2286 [57.69%] G-Loss: 34.8915 D-Loss: 2.1857 Loss-g-fm: 11.3431 Loss-g-mel: 17.7955 Loss-g-dur: 1.2517 Loss-g-kl: 1.7401 lr: 0.00014744 grad_norm_g: 892.6533 grad_norm_d: 93.3273
======> Epoch: 2286
Train Epoch: 2287 [53.85%] G-Loss: 32.1549 D-Loss: 2.2150 Loss-g-fm: 10.5182 Loss-g-mel: 16.5276 Loss-g-dur: 1.1281 Loss-g-kl: 1.3484 lr: 0.00014742 grad_norm_g: 945.7142 grad_norm_d: 53.8551
======> Epoch: 2287
Train Epoch: 2288 [50.00%] G-Loss: 31.4384 D-Loss: 2.3570 Loss-g-fm: 9.8947 Loss-g-mel: 16.3945 Loss-g-dur: 1.1220 Loss-g-kl: 1.3491 lr: 0.00014740 grad_norm_g: 775.0366 grad_norm_d: 81.1907
======> Epoch: 2288
Train Epoch: 2289 [46.15%] G-Loss: 31.5064 D-Loss: 2.3156 Loss-g-fm: 9.9562 Loss-g-mel: 16.2373 Loss-g-dur: 1.1362 Loss-g-kl: 1.4224 lr: 0.00014739 grad_norm_g: 431.8451 grad_norm_d: 47.8517
======> Epoch: 2289
Train Epoch: 2290 [42.31%] G-Loss: 34.5499 D-Loss: 2.1591 Loss-g-fm: 11.7934 Loss-g-mel: 17.2710 Loss-g-dur: 1.1608 Loss-g-kl: 1.8063 lr: 0.00014737 grad_norm_g: 192.7669 grad_norm_d: 23.4775
======> Epoch: 2290
Train Epoch: 2291 [38.46%] G-Loss: 32.5188 D-Loss: 2.1831 Loss-g-fm: 10.5031 Loss-g-mel: 16.7296 Loss-g-dur: 1.1073 Loss-g-kl: 1.2483 lr: 0.00014735 grad_norm_g: 886.0953 grad_norm_d: 60.3866
======> Epoch: 2291
Train Epoch: 2292 [34.62%] G-Loss: 33.4130 D-Loss: 2.1950 Loss-g-fm: 11.4405 Loss-g-mel: 16.7973 Loss-g-dur: 1.1518 Loss-g-kl: 1.6188 lr: 0.00014733 grad_norm_g: 972.5792 grad_norm_d: 58.9454
======> Epoch: 2292
Train Epoch: 2293 [30.77%] G-Loss: 33.8003 D-Loss: 2.0578 Loss-g-fm: 11.2533 Loss-g-mel: 17.1866 Loss-g-dur: 1.1227 Loss-g-kl: 1.3926 lr: 0.00014731 grad_norm_g: 969.7691 grad_norm_d: 62.5698
======> Epoch: 2293
Train Epoch: 2294 [26.92%] G-Loss: 33.1315 D-Loss: 2.1797 Loss-g-fm: 10.8670 Loss-g-mel: 16.8102 Loss-g-dur: 1.1108 Loss-g-kl: 1.4231 lr: 0.00014729 grad_norm_g: 88.7320 grad_norm_d: 26.0836
======> Epoch: 2294
Train Epoch: 2295 [23.08%] G-Loss: 33.2436 D-Loss: 2.1723 Loss-g-fm: 10.9682 Loss-g-mel: 16.8732 Loss-g-dur: 1.0687 Loss-g-kl: 1.5297 lr: 0.00014727 grad_norm_g: 932.8753 grad_norm_d: 62.1272
======> Epoch: 2295
Train Epoch: 2296 [19.23%] G-Loss: 35.6701 D-Loss: 2.1684 Loss-g-fm: 12.4967 Loss-g-mel: 17.7978 Loss-g-dur: 1.1960 Loss-g-kl: 1.6024 lr: 0.00014726 grad_norm_g: 897.7765 grad_norm_d: 39.4996
======> Epoch: 2296
Train Epoch: 2297 [15.38%] G-Loss: 32.3635 D-Loss: 2.1142 Loss-g-fm: 10.7890 Loss-g-mel: 16.3976 Loss-g-dur: 1.1120 Loss-g-kl: 1.2960 lr: 0.00014724 grad_norm_g: 966.2824 grad_norm_d: 85.6443
======> Epoch: 2297
Train Epoch: 2298 [11.54%] G-Loss: 34.8448 D-Loss: 2.3666 Loss-g-fm: 11.5379 Loss-g-mel: 17.4875 Loss-g-dur: 1.2215 Loss-g-kl: 1.7639 lr: 0.00014722 grad_norm_g: 560.5780 grad_norm_d: 112.7480
======> Epoch: 2298
Train Epoch: 2299 [7.69%] G-Loss: 35.1832 D-Loss: 2.2040 Loss-g-fm: 12.2096 Loss-g-mel: 17.2768 Loss-g-dur: 1.2663 Loss-g-kl: 1.7296 lr: 0.00014720 grad_norm_g: 205.8080 grad_norm_d: 24.5086
======> Epoch: 2299
Train Epoch: 2300 [3.85%] G-Loss: 36.5248 D-Loss: 2.0190 Loss-g-fm: 13.0811 Loss-g-mel: 17.2678 Loss-g-dur: 1.1601 Loss-g-kl: 1.7154 lr: 0.00014718 grad_norm_g: 888.4523 grad_norm_d: 27.7446
======> Epoch: 2300
Train Epoch: 2301 [0.00%] G-Loss: 32.4695 D-Loss: 2.0594 Loss-g-fm: 10.8293 Loss-g-mel: 16.5488 Loss-g-dur: 1.1021 Loss-g-kl: 1.2920 lr: 0.00014716 grad_norm_g: 987.1370 grad_norm_d: 91.9794
Train Epoch: 2301 [96.15%] G-Loss: 36.3894 D-Loss: 2.1275 Loss-g-fm: 12.5846 Loss-g-mel: 17.9164 Loss-g-dur: 1.2402 Loss-g-kl: 1.6561 lr: 0.00014716 grad_norm_g: 882.5732 grad_norm_d: 67.6444
======> Epoch: 2301
Train Epoch: 2302 [92.31%] G-Loss: 37.0421 D-Loss: 2.0518 Loss-g-fm: 13.5846 Loss-g-mel: 17.6094 Loss-g-dur: 1.2552 Loss-g-kl: 1.7271 lr: 0.00014715 grad_norm_g: 767.9596 grad_norm_d: 68.1000
======> Epoch: 2302
Train Epoch: 2303 [88.46%] G-Loss: 32.8507 D-Loss: 2.3099 Loss-g-fm: 10.3734 Loss-g-mel: 16.8617 Loss-g-dur: 1.2264 Loss-g-kl: 1.5525 lr: 0.00014713 grad_norm_g: 189.0833 grad_norm_d: 18.7595
======> Epoch: 2303
Train Epoch: 2304 [84.62%] G-Loss: 36.1575 D-Loss: 2.0353 Loss-g-fm: 13.2058 Loss-g-mel: 17.5556 Loss-g-dur: 1.1311 Loss-g-kl: 1.4359 lr: 0.00014711 grad_norm_g: 166.1519 grad_norm_d: 28.1827
======> Epoch: 2304
Train Epoch: 2305 [80.77%] G-Loss: 36.0785 D-Loss: 2.0978 Loss-g-fm: 12.1750 Loss-g-mel: 18.1042 Loss-g-dur: 1.2375 Loss-g-kl: 1.6413 lr: 0.00014709 grad_norm_g: 506.2277 grad_norm_d: 6.8177
======> Epoch: 2305
Train Epoch: 2306 [76.92%] G-Loss: 33.6798 D-Loss: 2.0945 Loss-g-fm: 11.5154 Loss-g-mel: 16.7319 Loss-g-dur: 1.1137 Loss-g-kl: 1.4067 lr: 0.00014707 grad_norm_g: 896.9528 grad_norm_d: 62.3319
======> Epoch: 2306
Train Epoch: 2307 [73.08%] G-Loss: 36.6345 D-Loss: 2.1161 Loss-g-fm: 12.6970 Loss-g-mel: 18.0998 Loss-g-dur: 1.2688 Loss-g-kl: 1.8250 lr: 0.00014705 grad_norm_g: 1001.1106 grad_norm_d: 56.6734
======> Epoch: 2307
Train Epoch: 2308 [69.23%] G-Loss: 33.1840 D-Loss: 2.1912 Loss-g-fm: 11.1024 Loss-g-mel: 16.7821 Loss-g-dur: 1.1087 Loss-g-kl: 1.5637 lr: 0.00014704 grad_norm_g: 192.8658 grad_norm_d: 44.8853
terminate called without an active exception
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc41c02a820>
Traceback (most recent call last):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 102269) is killed by signal: Aborted. 
Saving model and optimizer state at iteration 2308 to /ZFS4T/tts/data/VITS/model_saved/G_60000.pth
Saving model and optimizer state at iteration 2308 to /ZFS4T/tts/data/VITS/model_saved/D_60000.pth
======> Epoch: 2308
Train Epoch: 2309 [65.38%] G-Loss: 34.7258 D-Loss: 2.0044 Loss-g-fm: 11.9213 Loss-g-mel: 17.2018 Loss-g-dur: 1.1262 Loss-g-kl: 1.6085 lr: 0.00014702 grad_norm_g: 817.0945 grad_norm_d: 61.6845
======> Epoch: 2309
Train Epoch: 2310 [61.54%] G-Loss: 34.4708 D-Loss: 2.1407 Loss-g-fm: 11.9873 Loss-g-mel: 17.1380 Loss-g-dur: 1.0908 Loss-g-kl: 1.4750 lr: 0.00014700 grad_norm_g: 1058.8304 grad_norm_d: 83.4535
======> Epoch: 2310
Train Epoch: 2311 [57.69%] G-Loss: 34.6746 D-Loss: 2.3182 Loss-g-fm: 11.5283 Loss-g-mel: 17.5821 Loss-g-dur: 1.1920 Loss-g-kl: 1.6718 lr: 0.00014698 grad_norm_g: 628.0392 grad_norm_d: 90.0932
======> Epoch: 2311
Train Epoch: 2312 [53.85%] G-Loss: 34.4625 D-Loss: 2.1644 Loss-g-fm: 11.3884 Loss-g-mel: 17.6730 Loss-g-dur: 1.2288 Loss-g-kl: 1.4008 lr: 0.00014696 grad_norm_g: 320.3661 grad_norm_d: 34.5304
======> Epoch: 2312
Train Epoch: 2313 [50.00%] G-Loss: 36.5743 D-Loss: 2.0723 Loss-g-fm: 12.8768 Loss-g-mel: 18.1878 Loss-g-dur: 1.1944 Loss-g-kl: 1.5406 lr: 0.00014694 grad_norm_g: 89.6033 grad_norm_d: 9.4063
======> Epoch: 2313
Train Epoch: 2314 [46.15%] G-Loss: 17.8912 D-Loss: 2.4097 Loss-g-fm: 5.6557 Loss-g-mel: 7.7720 Loss-g-dur: 0.6726 Loss-g-kl: 1.3980 lr: 0.00014693 grad_norm_g: 592.0812 grad_norm_d: 15.2833
======> Epoch: 2314
Train Epoch: 2315 [42.31%] G-Loss: 34.6046 D-Loss: 1.9835 Loss-g-fm: 11.8293 Loss-g-mel: 17.1675 Loss-g-dur: 1.1581 Loss-g-kl: 1.6240 lr: 0.00014691 grad_norm_g: 874.0247 grad_norm_d: 44.2828
======> Epoch: 2315
Train Epoch: 2316 [38.46%] G-Loss: 34.3953 D-Loss: 2.2932 Loss-g-fm: 10.9765 Loss-g-mel: 17.6348 Loss-g-dur: 1.2556 Loss-g-kl: 1.7752 lr: 0.00014689 grad_norm_g: 654.2307 grad_norm_d: 32.6940
======> Epoch: 2316
Train Epoch: 2317 [34.62%] G-Loss: 35.4152 D-Loss: 2.1106 Loss-g-fm: 12.1111 Loss-g-mel: 17.6863 Loss-g-dur: 1.1996 Loss-g-kl: 1.7328 lr: 0.00014687 grad_norm_g: 730.7073 grad_norm_d: 98.7695
======> Epoch: 2317
Train Epoch: 2318 [30.77%] G-Loss: 19.1357 D-Loss: 2.3038 Loss-g-fm: 6.2098 Loss-g-mel: 8.2911 Loss-g-dur: 0.6297 Loss-g-kl: 1.3841 lr: 0.00014685 grad_norm_g: 1350.1783 grad_norm_d: 79.3740
======> Epoch: 2318
Train Epoch: 2319 [26.92%] G-Loss: 35.4030 D-Loss: 2.1524 Loss-g-fm: 12.2766 Loss-g-mel: 17.6033 Loss-g-dur: 1.1885 Loss-g-kl: 1.5143 lr: 0.00014683 grad_norm_g: 1023.5213 grad_norm_d: 58.8896
======> Epoch: 2319
Train Epoch: 2320 [23.08%] G-Loss: 33.2882 D-Loss: 2.0814 Loss-g-fm: 11.5967 Loss-g-mel: 16.4514 Loss-g-dur: 1.0500 Loss-g-kl: 1.5320 lr: 0.00014681 grad_norm_g: 952.9163 grad_norm_d: 55.1352
======> Epoch: 2320
Train Epoch: 2321 [19.23%] G-Loss: 36.8702 D-Loss: 2.0409 Loss-g-fm: 12.9926 Loss-g-mel: 18.1311 Loss-g-dur: 1.2763 Loss-g-kl: 1.7034 lr: 0.00014680 grad_norm_g: 828.6499 grad_norm_d: 65.0061
======> Epoch: 2321
Train Epoch: 2322 [15.38%] G-Loss: 33.5341 D-Loss: 2.1941 Loss-g-fm: 11.3614 Loss-g-mel: 17.0017 Loss-g-dur: 1.1001 Loss-g-kl: 1.4424 lr: 0.00014678 grad_norm_g: 701.9481 grad_norm_d: 31.8671
======> Epoch: 2322
Train Epoch: 2323 [11.54%] G-Loss: 32.5551 D-Loss: 2.3431 Loss-g-fm: 10.3995 Loss-g-mel: 17.1183 Loss-g-dur: 1.1022 Loss-g-kl: 1.5228 lr: 0.00014676 grad_norm_g: 538.1493 grad_norm_d: 80.6510
======> Epoch: 2323
Train Epoch: 2324 [7.69%] G-Loss: 34.2962 D-Loss: 2.1991 Loss-g-fm: 12.0222 Loss-g-mel: 16.8183 Loss-g-dur: 1.1312 Loss-g-kl: 1.6847 lr: 0.00014674 grad_norm_g: 970.6136 grad_norm_d: 51.0117
======> Epoch: 2324
Train Epoch: 2325 [3.85%] G-Loss: 34.9379 D-Loss: 2.1142 Loss-g-fm: 12.2661 Loss-g-mel: 17.2883 Loss-g-dur: 1.0830 Loss-g-kl: 1.5990 lr: 0.00014672 grad_norm_g: 432.7947 grad_norm_d: 12.9333
======> Epoch: 2325
Train Epoch: 2326 [0.00%] G-Loss: 33.7500 D-Loss: 2.0809 Loss-g-fm: 11.4876 Loss-g-mel: 16.9357 Loss-g-dur: 1.1356 Loss-g-kl: 1.4533 lr: 0.00014670 grad_norm_g: 663.1670 grad_norm_d: 67.0343
Train Epoch: 2326 [96.15%] G-Loss: 35.1600 D-Loss: 2.0897 Loss-g-fm: 12.5382 Loss-g-mel: 17.1591 Loss-g-dur: 1.1314 Loss-g-kl: 1.5171 lr: 0.00014670 grad_norm_g: 863.5874 grad_norm_d: 66.8259
======> Epoch: 2326
Train Epoch: 2327 [92.31%] G-Loss: 33.9250 D-Loss: 2.1266 Loss-g-fm: 11.4917 Loss-g-mel: 16.9941 Loss-g-dur: 1.1679 Loss-g-kl: 1.7835 lr: 0.00014669 grad_norm_g: 839.7505 grad_norm_d: 70.1205
======> Epoch: 2327
Train Epoch: 2328 [88.46%] G-Loss: 31.3550 D-Loss: 2.1951 Loss-g-fm: 9.6293 Loss-g-mel: 15.9890 Loss-g-dur: 1.1851 Loss-g-kl: 1.3633 lr: 0.00014667 grad_norm_g: 416.2415 grad_norm_d: 67.3076
======> Epoch: 2328
Train Epoch: 2329 [84.62%] G-Loss: 32.5441 D-Loss: 2.3524 Loss-g-fm: 10.3694 Loss-g-mel: 17.2106 Loss-g-dur: 1.1536 Loss-g-kl: 1.6245 lr: 0.00014665 grad_norm_g: 645.6321 grad_norm_d: 63.6586
======> Epoch: 2329
Train Epoch: 2330 [80.77%] G-Loss: 36.1402 D-Loss: 2.1572 Loss-g-fm: 12.3286 Loss-g-mel: 18.2790 Loss-g-dur: 1.1759 Loss-g-kl: 1.8062 lr: 0.00014663 grad_norm_g: 786.9056 grad_norm_d: 25.9980
======> Epoch: 2330
Train Epoch: 2331 [76.92%] G-Loss: 30.7671 D-Loss: 2.0323 Loss-g-fm: 11.2797 Loss-g-mel: 14.2150 Loss-g-dur: 0.9695 Loss-g-kl: 1.6385 lr: 0.00014661 grad_norm_g: 855.4134 grad_norm_d: 58.0286
======> Epoch: 2331
Train Epoch: 2332 [73.08%] G-Loss: 35.1486 D-Loss: 2.1586 Loss-g-fm: 11.6902 Loss-g-mel: 17.8598 Loss-g-dur: 1.1752 Loss-g-kl: 1.7824 lr: 0.00014659 grad_norm_g: 509.1434 grad_norm_d: 17.5293
======> Epoch: 2332
Train Epoch: 2333 [69.23%] G-Loss: 28.7209 D-Loss: 2.3744 Loss-g-fm: 9.6859 Loss-g-mel: 13.4615 Loss-g-dur: 0.9479 Loss-g-kl: 1.7087 lr: 0.00014658 grad_norm_g: 1032.2349 grad_norm_d: 84.8848
======> Epoch: 2333
Train Epoch: 2334 [65.38%] G-Loss: 34.4268 D-Loss: 2.2329 Loss-g-fm: 11.6042 Loss-g-mel: 17.7078 Loss-g-dur: 1.1935 Loss-g-kl: 1.6142 lr: 0.00014656 grad_norm_g: 365.0097 grad_norm_d: 51.6509
======> Epoch: 2334
Train Epoch: 2335 [61.54%] G-Loss: 34.6637 D-Loss: 2.1034 Loss-g-fm: 12.2366 Loss-g-mel: 17.1095 Loss-g-dur: 1.2074 Loss-g-kl: 1.5931 lr: 0.00014654 grad_norm_g: 406.9243 grad_norm_d: 34.0023
======> Epoch: 2335
Train Epoch: 2336 [57.69%] G-Loss: 31.1720 D-Loss: 1.9425 Loss-g-fm: 11.7651 Loss-g-mel: 13.9826 Loss-g-dur: 0.9443 Loss-g-kl: 1.5080 lr: 0.00014652 grad_norm_g: 963.1669 grad_norm_d: 58.5164
======> Epoch: 2336
Train Epoch: 2337 [53.85%] G-Loss: 34.3154 D-Loss: 2.1249 Loss-g-fm: 11.4304 Loss-g-mel: 17.4839 Loss-g-dur: 1.1037 Loss-g-kl: 1.5373 lr: 0.00014650 grad_norm_g: 448.9313 grad_norm_d: 10.5322
======> Epoch: 2337
Train Epoch: 2338 [50.00%] G-Loss: 18.3768 D-Loss: 2.2773 Loss-g-fm: 6.0118 Loss-g-mel: 7.5827 Loss-g-dur: 0.5909 Loss-g-kl: 1.5010 lr: 0.00014648 grad_norm_g: 1314.7427 grad_norm_d: 76.4635
======> Epoch: 2338
Train Epoch: 2339 [46.15%] G-Loss: 33.1900 D-Loss: 2.2127 Loss-g-fm: 11.4290 Loss-g-mel: 16.6747 Loss-g-dur: 1.0379 Loss-g-kl: 1.5667 lr: 0.00014647 grad_norm_g: 470.9458 grad_norm_d: 36.6960
======> Epoch: 2339
Train Epoch: 2340 [42.31%] G-Loss: 35.1923 D-Loss: 2.1558 Loss-g-fm: 11.9084 Loss-g-mel: 17.9410 Loss-g-dur: 1.1520 Loss-g-kl: 1.6485 lr: 0.00014645 grad_norm_g: 251.7039 grad_norm_d: 23.3974
======> Epoch: 2340
Train Epoch: 2341 [38.46%] G-Loss: 33.5786 D-Loss: 2.2043 Loss-g-fm: 10.8813 Loss-g-mel: 16.8852 Loss-g-dur: 1.2304 Loss-g-kl: 1.4361 lr: 0.00014643 grad_norm_g: 690.5631 grad_norm_d: 24.1180
======> Epoch: 2341
Train Epoch: 2342 [34.62%] G-Loss: 32.1905 D-Loss: 2.1655 Loss-g-fm: 10.5626 Loss-g-mel: 16.1387 Loss-g-dur: 1.2231 Loss-g-kl: 1.6119 lr: 0.00014641 grad_norm_g: 370.8653 grad_norm_d: 10.7727
======> Epoch: 2342
Train Epoch: 2343 [30.77%] G-Loss: 31.9998 D-Loss: 2.1321 Loss-g-fm: 10.3839 Loss-g-mel: 16.5030 Loss-g-dur: 1.1452 Loss-g-kl: 1.3993 lr: 0.00014639 grad_norm_g: 788.3539 grad_norm_d: 61.6491
======> Epoch: 2343
Train Epoch: 2344 [26.92%] G-Loss: 32.8154 D-Loss: 2.1220 Loss-g-fm: 10.6927 Loss-g-mel: 16.7826 Loss-g-dur: 1.1805 Loss-g-kl: 1.6638 lr: 0.00014638 grad_norm_g: 391.0624 grad_norm_d: 16.3736
======> Epoch: 2344
Train Epoch: 2345 [23.08%] G-Loss: 33.8820 D-Loss: 2.0113 Loss-g-fm: 11.7406 Loss-g-mel: 16.8874 Loss-g-dur: 1.1064 Loss-g-kl: 1.2654 lr: 0.00014636 grad_norm_g: 843.4325 grad_norm_d: 66.6830
======> Epoch: 2345
Train Epoch: 2346 [19.23%] G-Loss: 34.8713 D-Loss: 2.1145 Loss-g-fm: 11.7656 Loss-g-mel: 17.6107 Loss-g-dur: 1.1479 Loss-g-kl: 1.6600 lr: 0.00014634 grad_norm_g: 636.7340 grad_norm_d: 52.4580
======> Epoch: 2346
Train Epoch: 2347 [15.38%] G-Loss: 36.1456 D-Loss: 2.1330 Loss-g-fm: 11.8671 Loss-g-mel: 17.9334 Loss-g-dur: 1.4252 Loss-g-kl: 1.6990 lr: 0.00014632 grad_norm_g: 40.8834 grad_norm_d: 31.8398
======> Epoch: 2347
Train Epoch: 2348 [11.54%] G-Loss: 34.3002 D-Loss: 2.0879 Loss-g-fm: 11.6868 Loss-g-mel: 17.2356 Loss-g-dur: 1.0670 Loss-g-kl: 1.5993 lr: 0.00014630 grad_norm_g: 416.5342 grad_norm_d: 30.3881
======> Epoch: 2348
Train Epoch: 2349 [7.69%] G-Loss: 35.8028 D-Loss: 2.1451 Loss-g-fm: 12.4080 Loss-g-mel: 17.6896 Loss-g-dur: 1.1768 Loss-g-kl: 1.7093 lr: 0.00014628 grad_norm_g: 427.5528 grad_norm_d: 8.4333
======> Epoch: 2349
Train Epoch: 2350 [3.85%] G-Loss: 35.1022 D-Loss: 2.0611 Loss-g-fm: 11.9701 Loss-g-mel: 17.5211 Loss-g-dur: 1.1151 Loss-g-kl: 1.5159 lr: 0.00014627 grad_norm_g: 635.7950 grad_norm_d: 69.8616
======> Epoch: 2350
Train Epoch: 2351 [0.00%] G-Loss: 32.2626 D-Loss: 2.0984 Loss-g-fm: 10.4953 Loss-g-mel: 16.2963 Loss-g-dur: 1.1010 Loss-g-kl: 1.5956 lr: 0.00014625 grad_norm_g: 652.0852 grad_norm_d: 70.9867
Train Epoch: 2351 [96.15%] G-Loss: 34.9262 D-Loss: 2.1669 Loss-g-fm: 11.7786 Loss-g-mel: 17.3373 Loss-g-dur: 1.2516 Loss-g-kl: 1.8753 lr: 0.00014625 grad_norm_g: 606.3379 grad_norm_d: 31.3639
======> Epoch: 2351
Train Epoch: 2352 [92.31%] G-Loss: 36.5711 D-Loss: 2.0647 Loss-g-fm: 13.4006 Loss-g-mel: 17.5547 Loss-g-dur: 1.1794 Loss-g-kl: 1.6456 lr: 0.00014623 grad_norm_g: 253.6437 grad_norm_d: 10.1670
======> Epoch: 2352
Train Epoch: 2353 [88.46%] G-Loss: 34.8416 D-Loss: 2.1815 Loss-g-fm: 11.7752 Loss-g-mel: 17.5486 Loss-g-dur: 1.1290 Loss-g-kl: 1.6108 lr: 0.00014621 grad_norm_g: 616.1983 grad_norm_d: 59.9726
======> Epoch: 2353
Train Epoch: 2354 [84.62%] G-Loss: 33.0314 D-Loss: 2.0774 Loss-g-fm: 11.1885 Loss-g-mel: 16.2335 Loss-g-dur: 1.1147 Loss-g-kl: 1.4763 lr: 0.00014619 grad_norm_g: 1034.6681 grad_norm_d: 48.3933
======> Epoch: 2354
Train Epoch: 2355 [80.77%] G-Loss: 33.3991 D-Loss: 2.2391 Loss-g-fm: 10.0458 Loss-g-mel: 17.7078 Loss-g-dur: 1.2044 Loss-g-kl: 1.6832 lr: 0.00014617 grad_norm_g: 894.4473 grad_norm_d: 66.0039
======> Epoch: 2355
Train Epoch: 2356 [76.92%] G-Loss: 35.9589 D-Loss: 2.2015 Loss-g-fm: 12.1907 Loss-g-mel: 17.8281 Loss-g-dur: 1.2852 Loss-g-kl: 1.7370 lr: 0.00014616 grad_norm_g: 534.2482 grad_norm_d: 96.8022
======> Epoch: 2356
Train Epoch: 2357 [73.08%] G-Loss: 34.6683 D-Loss: 2.1887 Loss-g-fm: 11.9328 Loss-g-mel: 17.3211 Loss-g-dur: 1.1843 Loss-g-kl: 1.5102 lr: 0.00014614 grad_norm_g: 586.9969 grad_norm_d: 31.1500
======> Epoch: 2357
Train Epoch: 2358 [69.23%] G-Loss: 31.4962 D-Loss: 2.2322 Loss-g-fm: 9.6550 Loss-g-mel: 16.3306 Loss-g-dur: 1.1130 Loss-g-kl: 1.7270 lr: 0.00014612 grad_norm_g: 771.9644 grad_norm_d: 59.6668
======> Epoch: 2358
Train Epoch: 2359 [65.38%] G-Loss: 33.6314 D-Loss: 2.1399 Loss-g-fm: 11.8713 Loss-g-mel: 16.4562 Loss-g-dur: 1.0694 Loss-g-kl: 1.6182 lr: 0.00014610 grad_norm_g: 175.4423 grad_norm_d: 19.0009
======> Epoch: 2359
Train Epoch: 2360 [61.54%] G-Loss: 35.6797 D-Loss: 2.0460 Loss-g-fm: 12.4652 Loss-g-mel: 17.4971 Loss-g-dur: 1.1197 Loss-g-kl: 1.6328 lr: 0.00014608 grad_norm_g: 186.4350 grad_norm_d: 32.4779
======> Epoch: 2360
Train Epoch: 2361 [57.69%] G-Loss: 32.4729 D-Loss: 2.2029 Loss-g-fm: 11.0162 Loss-g-mel: 16.1156 Loss-g-dur: 1.0519 Loss-g-kl: 1.5996 lr: 0.00014606 grad_norm_g: 446.6535 grad_norm_d: 79.0197
======> Epoch: 2361
Train Epoch: 2362 [53.85%] G-Loss: 31.6311 D-Loss: 2.2215 Loss-g-fm: 9.5319 Loss-g-mel: 16.6563 Loss-g-dur: 1.1666 Loss-g-kl: 1.1761 lr: 0.00014605 grad_norm_g: 916.2351 grad_norm_d: 70.9096
======> Epoch: 2362
Train Epoch: 2363 [50.00%] G-Loss: 18.0149 D-Loss: 2.2904 Loss-g-fm: 5.9999 Loss-g-mel: 7.3925 Loss-g-dur: 0.5815 Loss-g-kl: 1.4336 lr: 0.00014603 grad_norm_g: 332.2125 grad_norm_d: 9.6648
======> Epoch: 2363
Train Epoch: 2364 [46.15%] G-Loss: 33.0951 D-Loss: 2.2001 Loss-g-fm: 11.2148 Loss-g-mel: 16.6382 Loss-g-dur: 1.1359 Loss-g-kl: 1.5279 lr: 0.00014601 grad_norm_g: 782.2647 grad_norm_d: 63.4921
======> Epoch: 2364
Train Epoch: 2365 [42.31%] G-Loss: 31.9485 D-Loss: 2.1532 Loss-g-fm: 10.5686 Loss-g-mel: 16.3036 Loss-g-dur: 1.1196 Loss-g-kl: 1.1816 lr: 0.00014599 grad_norm_g: 841.7271 grad_norm_d: 59.7305
======> Epoch: 2365
Train Epoch: 2366 [38.46%] G-Loss: 32.9175 D-Loss: 2.1079 Loss-g-fm: 11.3266 Loss-g-mel: 16.7409 Loss-g-dur: 1.1536 Loss-g-kl: 1.3350 lr: 0.00014597 grad_norm_g: 943.1964 grad_norm_d: 70.2548
======> Epoch: 2366
Train Epoch: 2367 [34.62%] G-Loss: 36.3622 D-Loss: 2.0685 Loss-g-fm: 12.5509 Loss-g-mel: 17.8907 Loss-g-dur: 1.1724 Loss-g-kl: 1.6567 lr: 0.00014595 grad_norm_g: 977.6904 grad_norm_d: 48.1939
======> Epoch: 2367
Train Epoch: 2368 [30.77%] G-Loss: 34.9408 D-Loss: 2.2090 Loss-g-fm: 11.8686 Loss-g-mel: 17.2246 Loss-g-dur: 1.2094 Loss-g-kl: 1.9572 lr: 0.00014594 grad_norm_g: 746.8777 grad_norm_d: 40.3508
======> Epoch: 2368
Train Epoch: 2369 [26.92%] G-Loss: 33.3808 D-Loss: 2.2613 Loss-g-fm: 10.7686 Loss-g-mel: 17.2257 Loss-g-dur: 1.1538 Loss-g-kl: 1.6910 lr: 0.00014592 grad_norm_g: 146.3968 grad_norm_d: 50.0478
======> Epoch: 2369
Train Epoch: 2370 [23.08%] G-Loss: 33.7532 D-Loss: 2.0636 Loss-g-fm: 11.5608 Loss-g-mel: 16.7027 Loss-g-dur: 1.1032 Loss-g-kl: 1.4702 lr: 0.00014590 grad_norm_g: 548.2046 grad_norm_d: 41.8240
======> Epoch: 2370
Train Epoch: 2371 [19.23%] G-Loss: 32.4163 D-Loss: 2.1881 Loss-g-fm: 10.6824 Loss-g-mel: 16.5621 Loss-g-dur: 1.1218 Loss-g-kl: 1.3613 lr: 0.00014588 grad_norm_g: 401.4696 grad_norm_d: 30.9466
======> Epoch: 2371
Train Epoch: 2372 [15.38%] G-Loss: 36.4170 D-Loss: 2.1295 Loss-g-fm: 13.2949 Loss-g-mel: 17.4522 Loss-g-dur: 1.1853 Loss-g-kl: 1.6597 lr: 0.00014586 grad_norm_g: 463.1357 grad_norm_d: 20.2546
======> Epoch: 2372
Train Epoch: 2373 [11.54%] G-Loss: 33.7931 D-Loss: 2.2290 Loss-g-fm: 11.6545 Loss-g-mel: 16.5431 Loss-g-dur: 1.1932 Loss-g-kl: 1.2536 lr: 0.00014585 grad_norm_g: 521.3979 grad_norm_d: 57.1328
======> Epoch: 2373
Train Epoch: 2374 [7.69%] G-Loss: 34.0902 D-Loss: 2.1316 Loss-g-fm: 11.9635 Loss-g-mel: 16.6800 Loss-g-dur: 1.0854 Loss-g-kl: 1.5144 lr: 0.00014583 grad_norm_g: 728.8082 grad_norm_d: 69.5103
======> Epoch: 2374
Train Epoch: 2375 [3.85%] G-Loss: 33.6654 D-Loss: 2.1332 Loss-g-fm: 12.1024 Loss-g-mel: 16.5525 Loss-g-dur: 1.1125 Loss-g-kl: 1.2387 lr: 0.00014581 grad_norm_g: 737.9131 grad_norm_d: 49.9250
======> Epoch: 2375
Train Epoch: 2376 [0.00%] G-Loss: 33.4352 D-Loss: 2.1341 Loss-g-fm: 11.4742 Loss-g-mel: 16.7187 Loss-g-dur: 1.0680 Loss-g-kl: 1.3506 lr: 0.00014579 grad_norm_g: 1078.7098 grad_norm_d: 61.8000
Train Epoch: 2376 [96.15%] G-Loss: 35.1245 D-Loss: 2.0897 Loss-g-fm: 11.9525 Loss-g-mel: 17.4526 Loss-g-dur: 1.1687 Loss-g-kl: 1.5955 lr: 0.00014579 grad_norm_g: 532.5694 grad_norm_d: 93.0367
======> Epoch: 2376
Train Epoch: 2377 [92.31%] G-Loss: 32.3194 D-Loss: 2.2005 Loss-g-fm: 10.2975 Loss-g-mel: 16.4590 Loss-g-dur: 1.1064 Loss-g-kl: 1.2938 lr: 0.00014577 grad_norm_g: 708.0768 grad_norm_d: 49.5837
======> Epoch: 2377
Train Epoch: 2378 [88.46%] G-Loss: 34.7583 D-Loss: 2.2554 Loss-g-fm: 12.1098 Loss-g-mel: 16.9226 Loss-g-dur: 1.1373 Loss-g-kl: 1.6381 lr: 0.00014575 grad_norm_g: 845.5439 grad_norm_d: 36.0240
======> Epoch: 2378
Train Epoch: 2379 [84.62%] G-Loss: 33.1063 D-Loss: 2.0386 Loss-g-fm: 11.5035 Loss-g-mel: 16.3860 Loss-g-dur: 1.0759 Loss-g-kl: 1.4681 lr: 0.00014574 grad_norm_g: 691.3269 grad_norm_d: 52.0430
======> Epoch: 2379
Train Epoch: 2380 [80.77%] G-Loss: 34.0358 D-Loss: 2.2196 Loss-g-fm: 11.2228 Loss-g-mel: 17.0593 Loss-g-dur: 1.1342 Loss-g-kl: 1.5283 lr: 0.00014572 grad_norm_g: 678.4758 grad_norm_d: 59.8845
======> Epoch: 2380
Train Epoch: 2381 [76.92%] G-Loss: 34.7456 D-Loss: 2.1423 Loss-g-fm: 12.0727 Loss-g-mel: 17.1921 Loss-g-dur: 1.1838 Loss-g-kl: 1.6889 lr: 0.00014570 grad_norm_g: 928.8086 grad_norm_d: 82.2111
======> Epoch: 2381
Train Epoch: 2382 [73.08%] G-Loss: 35.4348 D-Loss: 2.2473 Loss-g-fm: 12.2827 Loss-g-mel: 17.8311 Loss-g-dur: 1.1826 Loss-g-kl: 1.6395 lr: 0.00014568 grad_norm_g: 657.7533 grad_norm_d: 73.9530
======> Epoch: 2382
Train Epoch: 2383 [69.23%] G-Loss: 17.1069 D-Loss: 2.4474 Loss-g-fm: 5.2169 Loss-g-mel: 7.5779 Loss-g-dur: 0.6206 Loss-g-kl: 1.4174 lr: 0.00014566 grad_norm_g: 1201.5171 grad_norm_d: 59.5670
======> Epoch: 2383
Train Epoch: 2384 [65.38%] G-Loss: 38.6716 D-Loss: 2.0574 Loss-g-fm: 14.1565 Loss-g-mel: 17.9286 Loss-g-dur: 1.1595 Loss-g-kl: 1.7881 lr: 0.00014565 grad_norm_g: 69.9977 grad_norm_d: 9.5013
======> Epoch: 2384
Train Epoch: 2385 [61.54%] G-Loss: 36.6779 D-Loss: 2.0863 Loss-g-fm: 12.6793 Loss-g-mel: 18.1117 Loss-g-dur: 1.1818 Loss-g-kl: 1.6409 lr: 0.00014563 grad_norm_g: 855.4514 grad_norm_d: 10.3685
Saving model and optimizer state at iteration 2385 to /ZFS4T/tts/data/VITS/model_saved/G_62000.pth
Saving model and optimizer state at iteration 2385 to /ZFS4T/tts/data/VITS/model_saved/D_62000.pth
======> Epoch: 2385
Train Epoch: 2386 [57.69%] G-Loss: 31.9924 D-Loss: 2.1202 Loss-g-fm: 10.3551 Loss-g-mel: 16.2765 Loss-g-dur: 1.1132 Loss-g-kl: 1.4451 lr: 0.00014561 grad_norm_g: 880.1356 grad_norm_d: 53.5130
======> Epoch: 2386
Train Epoch: 2387 [53.85%] G-Loss: 34.7655 D-Loss: 2.1273 Loss-g-fm: 11.8970 Loss-g-mel: 16.9460 Loss-g-dur: 1.1895 Loss-g-kl: 1.7380 lr: 0.00014559 grad_norm_g: 819.8663 grad_norm_d: 60.0226
======> Epoch: 2387
Train Epoch: 2388 [50.00%] G-Loss: 34.2605 D-Loss: 2.3501 Loss-g-fm: 11.5345 Loss-g-mel: 17.1406 Loss-g-dur: 1.1048 Loss-g-kl: 1.5875 lr: 0.00014557 grad_norm_g: 813.3399 grad_norm_d: 102.3801
======> Epoch: 2388
Train Epoch: 2389 [46.15%] G-Loss: 34.4436 D-Loss: 2.1782 Loss-g-fm: 11.8692 Loss-g-mel: 17.0739 Loss-g-dur: 1.1594 Loss-g-kl: 1.6097 lr: 0.00014555 grad_norm_g: 700.4878 grad_norm_d: 67.3002
======> Epoch: 2389
Train Epoch: 2390 [42.31%] G-Loss: 33.9274 D-Loss: 2.2604 Loss-g-fm: 11.1568 Loss-g-mel: 17.5121 Loss-g-dur: 1.2372 Loss-g-kl: 1.5668 lr: 0.00014554 grad_norm_g: 261.1381 grad_norm_d: 20.3498
======> Epoch: 2390
Train Epoch: 2391 [38.46%] G-Loss: 35.3290 D-Loss: 2.0482 Loss-g-fm: 11.5520 Loss-g-mel: 18.0176 Loss-g-dur: 1.2708 Loss-g-kl: 1.6795 lr: 0.00014552 grad_norm_g: 43.8644 grad_norm_d: 12.0367
======> Epoch: 2391
Train Epoch: 2392 [34.62%] G-Loss: 29.7175 D-Loss: 2.0362 Loss-g-fm: 11.2874 Loss-g-mel: 13.2714 Loss-g-dur: 0.9433 Loss-g-kl: 1.6169 lr: 0.00014550 grad_norm_g: 221.8249 grad_norm_d: 13.0249
======> Epoch: 2392
Train Epoch: 2393 [30.77%] G-Loss: 32.5365 D-Loss: 2.1242 Loss-g-fm: 10.9281 Loss-g-mel: 16.4035 Loss-g-dur: 1.1767 Loss-g-kl: 1.6716 lr: 0.00014548 grad_norm_g: 683.9942 grad_norm_d: 51.2234
======> Epoch: 2393
Train Epoch: 2394 [26.92%] G-Loss: 38.0549 D-Loss: 2.0105 Loss-g-fm: 13.6917 Loss-g-mel: 18.4718 Loss-g-dur: 1.1723 Loss-g-kl: 1.7889 lr: 0.00014546 grad_norm_g: 983.9650 grad_norm_d: 48.0055
======> Epoch: 2394
Train Epoch: 2395 [23.08%] G-Loss: 35.5002 D-Loss: 2.1608 Loss-g-fm: 12.0241 Loss-g-mel: 18.0157 Loss-g-dur: 1.1587 Loss-g-kl: 1.5512 lr: 0.00014544 grad_norm_g: 502.1243 grad_norm_d: 34.4937
======> Epoch: 2395
Train Epoch: 2396 [19.23%] G-Loss: 31.4157 D-Loss: 2.1969 Loss-g-fm: 9.9523 Loss-g-mel: 15.9088 Loss-g-dur: 1.0763 Loss-g-kl: 1.3926 lr: 0.00014543 grad_norm_g: 203.2330 grad_norm_d: 34.7314
======> Epoch: 2396
Train Epoch: 2397 [15.38%] G-Loss: 35.9861 D-Loss: 2.1768 Loss-g-fm: 12.3555 Loss-g-mel: 17.8905 Loss-g-dur: 1.3105 Loss-g-kl: 1.8689 lr: 0.00014541 grad_norm_g: 114.8244 grad_norm_d: 8.7141
======> Epoch: 2397
Train Epoch: 2398 [11.54%] G-Loss: 33.0304 D-Loss: 2.2092 Loss-g-fm: 10.6363 Loss-g-mel: 17.0319 Loss-g-dur: 1.0707 Loss-g-kl: 1.3418 lr: 0.00014539 grad_norm_g: 635.9941 grad_norm_d: 18.1421
======> Epoch: 2398
Train Epoch: 2399 [7.69%] G-Loss: 36.1858 D-Loss: 2.0870 Loss-g-fm: 12.7303 Loss-g-mel: 17.5539 Loss-g-dur: 1.1943 Loss-g-kl: 1.6258 lr: 0.00014537 grad_norm_g: 863.8044 grad_norm_d: 57.1001
======> Epoch: 2399
Train Epoch: 2400 [3.85%] G-Loss: 34.7984 D-Loss: 2.1034 Loss-g-fm: 12.2207 Loss-g-mel: 17.3565 Loss-g-dur: 1.1759 Loss-g-kl: 1.5118 lr: 0.00014535 grad_norm_g: 696.5432 grad_norm_d: 56.9186
======> Epoch: 2400
Train Epoch: 2401 [0.00%] G-Loss: 34.2832 D-Loss: 2.1156 Loss-g-fm: 11.0218 Loss-g-mel: 17.8758 Loss-g-dur: 1.2133 Loss-g-kl: 1.6192 lr: 0.00014534 grad_norm_g: 413.1497 grad_norm_d: 11.6055
Train Epoch: 2401 [96.15%] G-Loss: 37.7501 D-Loss: 2.0191 Loss-g-fm: 13.0756 Loss-g-mel: 18.6567 Loss-g-dur: 1.3375 Loss-g-kl: 1.8236 lr: 0.00014534 grad_norm_g: 771.3646 grad_norm_d: 63.1969
======> Epoch: 2401
Train Epoch: 2402 [92.31%] G-Loss: 33.3993 D-Loss: 2.1774 Loss-g-fm: 11.0059 Loss-g-mel: 16.8483 Loss-g-dur: 1.1021 Loss-g-kl: 1.4060 lr: 0.00014532 grad_norm_g: 598.9329 grad_norm_d: 26.0940
======> Epoch: 2402
Train Epoch: 2403 [88.46%] G-Loss: 34.7442 D-Loss: 2.1042 Loss-g-fm: 11.9443 Loss-g-mel: 17.3667 Loss-g-dur: 1.2258 Loss-g-kl: 1.6089 lr: 0.00014530 grad_norm_g: 80.9972 grad_norm_d: 34.3704
======> Epoch: 2403
Train Epoch: 2404 [84.62%] G-Loss: 36.4267 D-Loss: 2.1203 Loss-g-fm: 12.1908 Loss-g-mel: 18.1575 Loss-g-dur: 1.1948 Loss-g-kl: 1.7732 lr: 0.00014528 grad_norm_g: 904.9943 grad_norm_d: 55.1030
======> Epoch: 2404
Train Epoch: 2405 [80.77%] G-Loss: 34.6559 D-Loss: 2.1884 Loss-g-fm: 12.0082 Loss-g-mel: 16.9582 Loss-g-dur: 1.1272 Loss-g-kl: 1.6506 lr: 0.00014526 grad_norm_g: 656.9003 grad_norm_d: 4.8259
======> Epoch: 2405
Train Epoch: 2406 [76.92%] G-Loss: 33.6717 D-Loss: 2.0745 Loss-g-fm: 11.3588 Loss-g-mel: 16.8877 Loss-g-dur: 1.0886 Loss-g-kl: 1.3437 lr: 0.00014525 grad_norm_g: 457.5290 grad_norm_d: 83.5960
======> Epoch: 2406
Train Epoch: 2407 [73.08%] G-Loss: 34.2271 D-Loss: 2.2101 Loss-g-fm: 11.7439 Loss-g-mel: 17.1886 Loss-g-dur: 1.1724 Loss-g-kl: 1.5098 lr: 0.00014523 grad_norm_g: 261.9684 grad_norm_d: 26.1847
======> Epoch: 2407
Train Epoch: 2408 [69.23%] G-Loss: 31.5778 D-Loss: 2.1032 Loss-g-fm: 10.0422 Loss-g-mel: 16.2291 Loss-g-dur: 1.1019 Loss-g-kl: 1.3562 lr: 0.00014521 grad_norm_g: 668.5716 grad_norm_d: 59.5585
======> Epoch: 2408
Train Epoch: 2409 [65.38%] G-Loss: 32.2540 D-Loss: 2.1503 Loss-g-fm: 10.6720 Loss-g-mel: 16.2645 Loss-g-dur: 1.1000 Loss-g-kl: 1.6707 lr: 0.00014519 grad_norm_g: 793.6074 grad_norm_d: 43.6255
======> Epoch: 2409
Train Epoch: 2410 [61.54%] G-Loss: 33.5409 D-Loss: 2.1394 Loss-g-fm: 10.9668 Loss-g-mel: 16.9012 Loss-g-dur: 1.1917 Loss-g-kl: 1.6312 lr: 0.00014517 grad_norm_g: 742.4280 grad_norm_d: 71.6519
======> Epoch: 2410
Train Epoch: 2411 [57.69%] G-Loss: 36.3078 D-Loss: 2.0597 Loss-g-fm: 12.7007 Loss-g-mel: 18.0416 Loss-g-dur: 1.1615 Loss-g-kl: 1.6278 lr: 0.00014515 grad_norm_g: 631.6801 grad_norm_d: 41.3956
======> Epoch: 2411
Train Epoch: 2412 [53.85%] G-Loss: 33.3714 D-Loss: 2.2051 Loss-g-fm: 11.2490 Loss-g-mel: 16.7970 Loss-g-dur: 1.1422 Loss-g-kl: 1.5736 lr: 0.00014514 grad_norm_g: 60.3038 grad_norm_d: 24.3331
======> Epoch: 2412
Train Epoch: 2413 [50.00%] G-Loss: 32.6947 D-Loss: 2.1314 Loss-g-fm: 10.7394 Loss-g-mel: 16.3101 Loss-g-dur: 1.1233 Loss-g-kl: 1.6260 lr: 0.00014512 grad_norm_g: 1095.5576 grad_norm_d: 50.8961
======> Epoch: 2413
Train Epoch: 2414 [46.15%] G-Loss: 36.6937 D-Loss: 2.1213 Loss-g-fm: 13.3742 Loss-g-mel: 17.3579 Loss-g-dur: 1.2484 Loss-g-kl: 1.7454 lr: 0.00014510 grad_norm_g: 848.7393 grad_norm_d: 48.4389
======> Epoch: 2414
Train Epoch: 2415 [42.31%] G-Loss: 31.3607 D-Loss: 2.1771 Loss-g-fm: 10.5686 Loss-g-mel: 15.7399 Loss-g-dur: 1.0879 Loss-g-kl: 1.3581 lr: 0.00014508 grad_norm_g: 899.0562 grad_norm_d: 97.5936
======> Epoch: 2415
Train Epoch: 2416 [38.46%] G-Loss: 36.1132 D-Loss: 1.9703 Loss-g-fm: 12.5525 Loss-g-mel: 17.5996 Loss-g-dur: 1.1473 Loss-g-kl: 1.8540 lr: 0.00014506 grad_norm_g: 891.1488 grad_norm_d: 44.2630
======> Epoch: 2416
Train Epoch: 2417 [34.62%] G-Loss: 39.1995 D-Loss: 2.1740 Loss-g-fm: 14.8890 Loss-g-mel: 18.2786 Loss-g-dur: 1.2691 Loss-g-kl: 1.8202 lr: 0.00014505 grad_norm_g: 1017.4547 grad_norm_d: 26.4558
======> Epoch: 2417
Train Epoch: 2418 [30.77%] G-Loss: 33.0496 D-Loss: 2.1322 Loss-g-fm: 10.8165 Loss-g-mel: 17.0641 Loss-g-dur: 1.0909 Loss-g-kl: 1.3719 lr: 0.00014503 grad_norm_g: 189.9922 grad_norm_d: 15.3267
======> Epoch: 2418
Train Epoch: 2419 [26.92%] G-Loss: 19.2302 D-Loss: 2.4434 Loss-g-fm: 6.8334 Loss-g-mel: 7.7070 Loss-g-dur: 0.6325 Loss-g-kl: 1.4901 lr: 0.00014501 grad_norm_g: 1231.4178 grad_norm_d: 62.1620
======> Epoch: 2419
Train Epoch: 2420 [23.08%] G-Loss: 33.7453 D-Loss: 2.2418 Loss-g-fm: 11.2500 Loss-g-mel: 16.7233 Loss-g-dur: 1.0915 Loss-g-kl: 1.6530 lr: 0.00014499 grad_norm_g: 596.1553 grad_norm_d: 67.4637
======> Epoch: 2420
Train Epoch: 2421 [19.23%] G-Loss: 29.7012 D-Loss: 2.2452 Loss-g-fm: 9.3992 Loss-g-mel: 15.6193 Loss-g-dur: 1.1009 Loss-g-kl: 1.1351 lr: 0.00014497 grad_norm_g: 739.6638 grad_norm_d: 58.0846
======> Epoch: 2421
Train Epoch: 2422 [15.38%] G-Loss: 35.9656 D-Loss: 1.9704 Loss-g-fm: 13.0061 Loss-g-mel: 17.2183 Loss-g-dur: 1.1157 Loss-g-kl: 1.7659 lr: 0.00014495 grad_norm_g: 307.0903 grad_norm_d: 6.8111
======> Epoch: 2422
Train Epoch: 2423 [11.54%] G-Loss: 35.8732 D-Loss: 2.1824 Loss-g-fm: 12.0080 Loss-g-mel: 18.0858 Loss-g-dur: 1.1789 Loss-g-kl: 1.6405 lr: 0.00014494 grad_norm_g: 888.2853 grad_norm_d: 33.0400
======> Epoch: 2423
Train Epoch: 2424 [7.69%] G-Loss: 36.4438 D-Loss: 2.0970 Loss-g-fm: 12.6643 Loss-g-mel: 17.8894 Loss-g-dur: 1.3597 Loss-g-kl: 1.5150 lr: 0.00014492 grad_norm_g: 974.6010 grad_norm_d: 13.5423
======> Epoch: 2424
Train Epoch: 2425 [3.85%] G-Loss: 34.5891 D-Loss: 2.1432 Loss-g-fm: 12.4141 Loss-g-mel: 16.6963 Loss-g-dur: 1.1203 Loss-g-kl: 1.2589 lr: 0.00014490 grad_norm_g: 969.0567 grad_norm_d: 99.4853
======> Epoch: 2425
Traceback (most recent call last):
  File "/mnt/disk3/huangyao/tts/vits/train_ms.py", line 218, in <module>
    main()        
  File "/mnt/disk3/huangyao/tts/vits/train_ms.py", line 34, in main
    mp.spawn(run, nprocs = n_gpus, args = (n_gpus, hps,))
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/home/rex/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with signal SIGSEGV
ERROR: Unexpected segmentation fault encountered in worker.
 ERROR: Unexpected segmentation fault encountered in worker.
 ERROR: Unexpected segmentation fault encountered in worker.
 ERROR: Unexpected segmentation fault encountered in worker.
 ERROR: Unexpected segmentation fault encountered in worker.
 ERROR: Unexpected segmentation fault encountered in worker.
 ERROR: Unexpected segmentation fault encountered in worker.
 ERROR: Unexpected segmentation fault encountered in worker.
 ERROR: Unexpected segmentation fault encountered in worker.
 /home/rex/anaconda3/envs/torch2/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 40 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
